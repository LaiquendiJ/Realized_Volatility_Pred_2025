{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09c33b4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T19:38:22.839845Z",
     "iopub.status.busy": "2025-09-06T19:38:22.839396Z",
     "iopub.status.idle": "2025-09-06T19:38:29.560092Z",
     "shell.execute_reply": "2025-09-06T19:38:29.559039Z"
    },
    "papermill": {
     "duration": 6.731075,
     "end_time": "2025-09-06T19:38:29.561889",
     "exception": false,
     "start_time": "2025-09-06T19:38:22.830814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Optional\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler, QuantileTransformer\n",
    "import plotly.express as px\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import glob\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5e27a7",
   "metadata": {
    "papermill": {
     "duration": 0.00771,
     "end_time": "2025-09-06T19:38:29.577446",
     "exception": false,
     "start_time": "2025-09-06T19:38:29.569736",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1 Feature Generation\n",
    "## 1.1 Book Features\n",
    "This section defines utility functions for processing high-frequency trading data. These functions handle price calculations, return computations, and data aggregation across different time intervals. The functions are optimized for Polars DataFrames to efficiently process large orderbook datasets.\n",
    "Key components include:\n",
    "\n",
    "1. Price calculations: Weighted average price (WAP) and weighted mid price (WMP) formulas\n",
    "\n",
    "2. Statistical measures: Log returns and realized volatility computations\n",
    "\n",
    "3. Aggregation: Customizable statistical aggregations for any column\n",
    "\n",
    "4. Time interval processing: Support for both full-period and sub-interval analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fabcf9",
   "metadata": {
    "papermill": {
     "duration": 0.007472,
     "end_time": "2025-09-06T19:38:29.594174",
     "exception": false,
     "start_time": "2025-09-06T19:38:29.586702",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "1.  **`wap1/2`**  =$ \\frac{\\text{ask\\_price}_{1/2} \\cdot \\text{bid\\_size}_{1/2} + \\text{bid\\_price}_{1/2} \\cdot \\text{ask\\_size}_{1/2}}{\\text{ask\\_size}_{1/2} + \\text{bid\\_size}_{1/2}}$ \n",
    "\n",
    "Weighted Average Price at depth 1/2. This smooths out the impact of individual order prices using size as weight. It approximates the \"fair\" market price better than simple mid-price. Including wap2 captures more of the limit order bookâ€™s shape. \n",
    "\n",
    "2. **`bid_size_diff`** = $\\text{bid\\_size}_1 - \\text{bid\\_size}_2$                                                                                      \n",
    "\n",
    "Captures the change in demand depth between level 1 and level 2. Large differences may imply lower resilience in the order book.   \n",
    "\n",
    "3. **`ask_size_diff`** =$\\text{ask\\_size}_1-\\text{ask\\_size}_2$                                                                               \n",
    "\n",
    "Measures ask-side change. Can signal a lack of liquidity on the sell side or increased selling pressure.                              \n",
    "\n",
    "4. **`price_spread`** = $ \\frac{\\text{ask\\_price}_1}{\\text{bid\\_price}_1} - 1$                                                                                \n",
    "\n",
    "A measure of transaction cost and short-term illiquidity.       \n",
    "\n",
    "5. **`order_imbalance_1/2`** =$\\frac{\\text{bid\\_size1/2} - \\text{ask\\_size1/2}}{\\text{bid\\_size1/2} + \\text{ask\\_size1/2}}$                                                     \n",
    "\n",
    "Measures demand-supply pressure at level 1/2. Positive values imply buying pressure, negative values suggest selling pressure.              \n",
    "\n",
    "6. **`depth_ratio`** = $\\frac{\\text{bid\\_size}_1 + \\text{bid\\_size}_2}{\\text{ask\\_size}_1 + \\text{ask\\_size}_2}$                                             \n",
    "\n",
    "Compares total buy-side depth to sell-side depth. Values > 1 suggest buy-side dominance.             \n",
    "                                    \n",
    "7. **`total_volume`** =$ \\text{bid\\_size}_1 + \\text{bid\\_size}_2 + \\text{ask\\_size}_1 + \\text{ask\\_size}_2$                                                  \n",
    "\n",
    "Captures overall liquidity in the limit order book. High volume typically corresponds to tighter spreads and lower volatility.            \n",
    "\n",
    "8. **`wap_diff`**  =$ \\text{wap1} - \\text{wap2}$                                                                                                           \n",
    "Measures how much prices diverge between the top and next levels of the book. Large values may indicate price pressure or volatility. Useful for detecting price trends or pressure. \n",
    "\n",
    "9. **`log_return1/2`** = $ \\log\\left( \\frac{\\text{wap1/2}_t}{\\text{wap1/2}_{t-1}} \\right)$                                                                        \n",
    "Log return of using wap1/2. Used in volatility estimation and price dynamics modeling.                       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c78033bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T19:38:29.612075Z",
     "iopub.status.busy": "2025-09-06T19:38:29.611447Z",
     "iopub.status.idle": "2025-09-06T19:38:29.627702Z",
     "shell.execute_reply": "2025-09-06T19:38:29.626686Z"
    },
    "papermill": {
     "duration": 0.027215,
     "end_time": "2025-09-06T19:38:29.629379",
     "exception": false,
     "start_time": "2025-09-06T19:38:29.602164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Weighted Mid Price - weighted by volume\n",
    "def calc_wmp(bid_p, ask_p, bid_s, ask_s):\n",
    "    return (pl.col(bid_p) * pl.col(bid_s) + pl.col(ask_p) * pl.col(ask_s)) / (pl.col(ask_s) + pl.col(bid_s))\n",
    "\n",
    "# Weighted Average Price - weighted by opposite side volume\n",
    "def calc_wap(bid_p, ask_p, bid_s, ask_s):\n",
    "    return (pl.col(bid_p) * pl.col(ask_s) + pl.col(ask_p) * pl.col(bid_s)) / (\n",
    "        pl.col(bid_s) + pl.col(ask_s)\n",
    "    )\n",
    "\n",
    "# Log return calculation\n",
    "def log_return(col):\n",
    "    return pl.col(col).log().diff()\n",
    "\n",
    "# Realized volatility calculation\n",
    "def realized_vol(col):\n",
    "    return (pl.col(col).pow(2).sum()).sqrt()\n",
    "\n",
    "# Aggregate column with specified functions\n",
    "def agg_all(col: str, functions: list):\n",
    "    dict = {\n",
    "        'sum': pl.col(col).sum().alias(f\"{col}_sum\"),\n",
    "        'mean': pl.col(col).mean().alias(f\"{col}_mean\"),\n",
    "        'std': pl.col(col).std().alias(f\"{col}_std\"),\n",
    "        'max': pl.col(col).max().alias(f\"{col}_max\"),\n",
    "        'min': pl.col(col).min().alias(f\"{col}_min\"),\n",
    "        'realized_vol': (pl.col(col).pow(2).sum()).sqrt().alias(f\"{col}_realized_vol\"),\n",
    "        'unique': pl.col(col).n_unique().alias(f\"{col}_unique\")\n",
    "    }\n",
    "    return [dict[i] for i in functions]\n",
    "\n",
    "# Aggregate with realized volatility (deprecated - use agg_all instead)\n",
    "def agg_with_rvol(col: str):\n",
    "    return agg_all(col) + [\n",
    "        (pl.col(col).pow(2).sum()).sqrt().alias(f\"{col}_realized_vol\")\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efbc055",
   "metadata": {},
   "source": [
    "### 1.1.1 Book Data Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50cbbfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate column with specified functions\n",
    "def agg_all(col: str, functions: list):\n",
    "    dict = {\n",
    "        'sum': pl.col(col).sum().alias(f\"{col}_sum\"),\n",
    "        'mean': pl.col(col).mean().alias(f\"{col}_mean\"),\n",
    "        'std': pl.col(col).std().alias(f\"{col}_std\"),\n",
    "        'max': pl.col(col).max().alias(f\"{col}_max\"),\n",
    "        'min': pl.col(col).min().alias(f\"{col}_min\"),\n",
    "        'realized_vol': (pl.col(col).pow(2).sum()).sqrt().alias(f\"{col}_realized_vol\"),\n",
    "        'unique': pl.col(col).n_unique().alias(f\"{col}_unique\")\n",
    "    }\n",
    "    return [dict[i] for i in functions]\n",
    "\n",
    "# Aggregate with realized volatility (deprecated - use agg_all instead)\n",
    "def agg_with_rvol(col: str):\n",
    "    return agg_all(col) + [\n",
    "        (pl.col(col).pow(2).sum()).sqrt().alias(f\"{col}_realized_vol\")\n",
    "    ]\n",
    "\n",
    "# Aggregate orderbook data by time intervals\n",
    "def aggregate_interval_book(df: pl.DataFrame, interval_length: Optional[list] = None) -> pl.DataFrame:\n",
    "    # Filter to specific interval if provided (counting back from 600 seconds)\n",
    "    if interval_length is not None:\n",
    "        start = 600 - interval_length\n",
    "        df = df.filter(pl.col(\"seconds_in_bucket\") >= start)\n",
    "    \n",
    "    extra_exprs = {}\n",
    "    # Include additional features for full interval aggregation\n",
    "    if interval_length is None:\n",
    "        extra_exprs = {\n",
    "            \"wap1\": agg_all(\"wap1\", ['sum', 'std']),\n",
    "            \"wap2\": agg_all(\"wap2\", ['sum', 'std']),\n",
    "            \"wmp1\": agg_all(\"wmp1\", ['sum', 'std']),\n",
    "            \"wmp2\": agg_all(\"wmp2\", ['sum', 'std']),\n",
    "            \"wap_balance\": agg_all(\"wap_balance\", ['sum', 'max']),\n",
    "            \"price_spread\": agg_all(\"price_spread\", ['sum', 'max']),\n",
    "            \"price_spread2\": agg_all(\"price_spread2\", ['sum', 'max']),\n",
    "            \"bid_spread\": agg_all(\"bid_spread\", ['sum', 'max']),\n",
    "            \"ask_spread\": agg_all(\"ask_spread\", ['sum', 'max']),\n",
    "            \"total_volume\": agg_all(\"total_volume\", ['sum', 'max']),\n",
    "            \"volume_imbalance\": agg_all(\"volume_imbalance\", ['sum', 'max']),\n",
    "        }\n",
    "    \n",
    "    # Core aggregations - realized volatility for log returns\n",
    "    agg_dict = {\n",
    "        \"log_return1\": agg_all(\"log_return1\", ['realized_vol']),\n",
    "        \"log_return2\": agg_all(\"log_return2\", ['realized_vol']),\n",
    "        \"log_return1_wmp\": agg_all(\"log_return1_wmp\", ['realized_vol']),\n",
    "        \"log_return2_wmp\": agg_all(\"log_return2_wmp\", ['realized_vol']),\n",
    "    }\n",
    "    \n",
    "    # Merge extra expressions\n",
    "    agg_dict = agg_dict | extra_exprs\n",
    "    \n",
    "    # Group by time_id and apply aggregations\n",
    "    df_agg = df.group_by(\"time_id\").agg(\n",
    "        [expr for exprs in agg_dict.values() for expr in exprs]\n",
    "    )\n",
    "    \n",
    "    # Add interval suffix to column names if specified\n",
    "    if interval_length is not None:\n",
    "        df_agg = df_agg.rename({\n",
    "            col: f\"{col}_{interval_length}\" if col != \"time_id\" else \"time_id\"\n",
    "            for col in df_agg.columns\n",
    "        })\n",
    "    \n",
    "    return df_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65870baf",
   "metadata": {},
   "source": [
    "### 1.1.2 Orderbook Data Process\n",
    "\n",
    "Then, we processes raw orderbook snapshot data and transforms it into a comprehensive feature set suitable for machine learning models. It handles the complete feature engineering pipeline from raw bid/ask data to aggregated market microstructure indicators.\n",
    "The preprocessing workflow includes:\n",
    "\n",
    "Price feature engineering: Computing weighted prices, spreads, and price imbalances\n",
    "\n",
    "Volume analysis: Calculating total volume, imbalances, and size ratios between orderbook levels\n",
    "\n",
    "Multi-timeframe aggregation: Creating features for different time intervals (e.g., 200s, 300s, 400s windows)\n",
    "\n",
    "Unique identification: Creating composite row IDs for data merging\n",
    "\n",
    "The output is a feature-rich dataset where each row represents aggregated market conditions for a specific stock-time combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa9fb70e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T19:38:29.645902Z",
     "iopub.status.busy": "2025-09-06T19:38:29.645545Z",
     "iopub.status.idle": "2025-09-06T19:38:29.658284Z",
     "shell.execute_reply": "2025-09-06T19:38:29.657424Z"
    },
    "papermill": {
     "duration": 0.022962,
     "end_time": "2025-09-06T19:38:29.660086",
     "exception": false,
     "start_time": "2025-09-06T19:38:29.637124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Process orderbook data file and extract features for multiple time intervals\n",
    "def preprocessor_book(file_path, time_length_list):\n",
    "    # Extract stock ID from file path (format: stock_id=123.parquet)\n",
    "    stock_id = int(file_path.split('=')[1].split('.')[0])\n",
    "    \n",
    "    # Load orderbook data\n",
    "    df = pl.read_parquet(file_path)\n",
    "    \n",
    "    # Step 1: Calculate price and volume features\n",
    "    df = (\n",
    "        df.with_columns([\n",
    "            # Weighted average prices for level 1 and 2\n",
    "            calc_wap(\"bid_price1\", \"ask_price1\", \"bid_size1\", \"ask_size1\").alias(\"wap1\"),\n",
    "            calc_wap(\"bid_price2\", \"ask_price2\", \"bid_size2\", \"ask_size2\").alias(\"wap2\"),\n",
    "            # Weighted mid prices for level 1 and 2\n",
    "            calc_wmp(\"bid_price1\", \"ask_price1\", \"bid_size1\", \"ask_size1\").alias(\"wmp1\"),\n",
    "            calc_wmp(\"bid_price2\", \"ask_price2\", \"bid_size2\", \"ask_size2\").alias(\"wmp2\"),\n",
    "        ])\n",
    "        .with_columns([\n",
    "            # Log returns for price series\n",
    "            log_return(\"wap1\").alias(\"log_return1\"),\n",
    "            log_return(\"wap2\").alias(\"log_return2\"),\n",
    "            log_return(\"wmp1\").alias(\"log_return1_wmp\"),\n",
    "            log_return(\"wmp2\").alias(\"log_return2_wmp\"),\n",
    "            \n",
    "            # Price balance and spread features\n",
    "            (pl.col(\"wap1\") - pl.col(\"wap2\")).abs().alias(\"wap_balance\"),\n",
    "            ((pl.col(\"ask_price1\") - pl.col(\"bid_price1\")) /\n",
    "             ((pl.col(\"ask_price1\") + pl.col(\"bid_price1\")) / 2)).alias(\"price_spread\"),\n",
    "            ((pl.col(\"ask_price2\") - pl.col(\"bid_price2\")) /\n",
    "             ((pl.col(\"ask_price2\") + pl.col(\"bid_price2\")) / 2)).alias(\"price_spread2\"),\n",
    "            (pl.col(\"bid_price1\") - pl.col(\"bid_price2\")).alias(\"bid_spread\"),\n",
    "            (pl.col(\"ask_price1\") - pl.col(\"ask_price2\")).alias(\"ask_spread\"),\n",
    "            \n",
    "            # Volume features\n",
    "            (pl.col(\"ask_size1\") + pl.col(\"ask_size2\") + \n",
    "             pl.col(\"bid_size1\") + pl.col(\"bid_size2\")).alias(\"total_volume\"),\n",
    "            ((pl.col(\"ask_size1\") + pl.col(\"ask_size2\")) - \n",
    "             (pl.col(\"bid_size1\") + pl.col(\"bid_size2\"))).abs().alias(\"volume_imbalance\"),\n",
    "            \n",
    "            # Size difference ratios between levels\n",
    "            (pl.col(\"bid_size1\") / pl.col(\"bid_size2\") - 1).alias(\"bid_size_diff\"),\n",
    "            (pl.col(\"ask_size1\") / pl.col(\"ask_size2\") - 1).alias(\"ask_size_diff\"),\n",
    "            \n",
    "            # Order imbalance features\n",
    "            ((pl.col(\"bid_size1\") - pl.col(\"ask_size1\")) /\n",
    "             (pl.col(\"bid_size1\") + pl.col(\"ask_size1\"))).alias(\"order_imbalance_1\"),\n",
    "            ((pl.col(\"bid_size1\") + pl.col(\"bid_size2\") - pl.col(\"ask_size1\") - pl.col(\"ask_size2\")) /\n",
    "             (pl.col(\"bid_size1\") + pl.col(\"bid_size2\") + pl.col(\"ask_size1\") + pl.col(\"ask_size2\"))).alias(\"order_imbalance_total\"),\n",
    "        ])\n",
    "    )\n",
    "    \n",
    "    # Step 2: Sort by time for proper sequence processing\n",
    "    df = df.sort([\"time_id\", \"seconds_in_bucket\"])\n",
    "    \n",
    "    # Step 3: Drop raw orderbook columns to reduce memory usage\n",
    "    df = df.drop([\n",
    "        \"ask_price1\", \"ask_price2\", \"ask_size1\", \"ask_size2\",\n",
    "        \"bid_price1\", \"bid_price2\", \"bid_size1\", \"bid_size2\"\n",
    "    ])\n",
    "    \n",
    "    # Step 4: Aggregate features for different time intervals\n",
    "    merged_df = None\n",
    "    for length in time_length_list:\n",
    "        df_agg = aggregate_interval_book(df, length)\n",
    "        merged_df = df_agg if merged_df is None else merged_df.join(\n",
    "            df_agg, on=\"time_id\", how=\"left\")\n",
    "    \n",
    "    # Step 5: Get full interval features and merge with interval-specific features\n",
    "    df_feature = aggregate_interval_book(df)  # Full 600-second interval\n",
    "    df_feature = df_feature.join(merged_df, on=\"time_id\", how=\"left\")\n",
    "    \n",
    "    # Step 6: Create unique row identifier\n",
    "    df_feature = df_feature.with_columns(\n",
    "        (pl.lit(stock_id).cast(pl.Utf8) + \"-\" +\n",
    "         pl.col(\"time_id\").cast(pl.Utf8)).alias(\"row_id\")\n",
    "    )\n",
    "    \n",
    "    return df_feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b395a0f9",
   "metadata": {
    "papermill": {
     "duration": 0.008381,
     "end_time": "2025-09-06T19:38:30.525171",
     "exception": false,
     "start_time": "2025-09-06T19:38:30.516790",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1.2 Trade Features \n",
    "This code processes trade data for stocks, computing additional features such as average trade size per order and the logarithmic return of trade prices. The logarithmic return is calculated as the log of the ratio between the current price and the previous price. Rows with null values resulting from this calculation (due to missing previous prices) are removed to ensure data quality. The cleaned and enriched DataFrame is then saved as Parquet files, organized by stock ID.\n",
    "\n",
    "### 1.2.1 Trade Data Aggregation\n",
    "This function aggregates executed trade data across different time intervals to create transaction-based features. Unlike orderbook data which captures market depth at snapshots, this processes actual trade executions to understand market activity and price dynamics.\n",
    "The aggregation focuses on:\n",
    "\n",
    "1. Trade activity metrics: Order counts, trade sizes, and execution amounts\n",
    "\n",
    "2. Price movement analysis: Realized volatility from trade-to-trade price changes\n",
    "\n",
    "3. Time coverage: Measuring the temporal distribution of trading activity\n",
    "\n",
    "4. Flexible intervals: Supporting both full-period and sub-interval analysis with appropriate feature naming\n",
    "\n",
    "This complements orderbook features by providing insights into actual market transactions rather than just quoted prices and sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "04b243d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T19:38:30.543208Z",
     "iopub.status.busy": "2025-09-06T19:38:30.542879Z",
     "iopub.status.idle": "2025-09-06T19:38:30.551009Z",
     "shell.execute_reply": "2025-09-06T19:38:30.549928Z"
    },
    "papermill": {
     "duration": 0.020133,
     "end_time": "2025-09-06T19:38:30.553518",
     "exception": false,
     "start_time": "2025-09-06T19:38:30.533385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Aggregate trade data by time intervals\n",
    "def aggregate_interval_trade(df: pl.DataFrame, interval_length: Optional[list] = None) -> pl.DataFrame:\n",
    "    # Filter to specific interval if provided (counting back from 600 seconds)\n",
    "    if interval_length is not None:\n",
    "        start = 600 - interval_length\n",
    "        df = df.filter(pl.col(\"seconds_in_bucket\") >= start)\n",
    "    \n",
    "    extra_exprs = {}\n",
    "    # Include additional trade features for full interval aggregation\n",
    "    if interval_length is None:\n",
    "        extra_exprs = {\n",
    "            \"size_per_order\": agg_all(\"size_per_order\", ['mean']),\n",
    "            \"size\": agg_all(\"size\", ['sum', 'min', 'max']),\n",
    "            \"order_count\": agg_all(\"order_count\", ['max']),\n",
    "            \"amount\": agg_all(\"amount\", ['sum', 'max', 'min'])\n",
    "        }\n",
    "    \n",
    "    # Core trade aggregations\n",
    "    agg_dict = {\n",
    "        \"trade_log_return\": agg_all(\"trade_log_return\", ['realized_vol']),  # Volatility from trade returns\n",
    "        \"order_count\": agg_all(\"order_count\", ['sum']),                      # Total number of orders\n",
    "        \"seconds_in_bucket\": agg_all(\"seconds_in_bucket\", ['unique'])        # Time coverage metric\n",
    "    }\n",
    "    \n",
    "    # Combine core and extra expressions\n",
    "    vals = list(agg_dict.values()) + list(extra_exprs.values())\n",
    "    \n",
    "    # Group by time_id and apply aggregations\n",
    "    df_agg = df.group_by(\"time_id\").agg(\n",
    "        [expr for exprs in vals for expr in exprs]\n",
    "    )\n",
    "    \n",
    "    # Add interval suffix to column names if specified\n",
    "    if interval_length is not None:\n",
    "        df_agg = df_agg.rename({\n",
    "            col: f\"{col}_{interval_length}\" if col != \"time_id\" else \"time_id\"\n",
    "            for col in df_agg.columns\n",
    "        })\n",
    "    \n",
    "    return df_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38860ec8",
   "metadata": {},
   "source": [
    "### 1.2.2 Trade Data Process\n",
    "\n",
    "This function transforms raw trade execution data into engineered features across multiple time horizons. It processes individual trade records to extract meaningful patterns in trading behavior and price movements.\n",
    "The feature engineering pipeline includes:\n",
    "\n",
    "1. Trade efficiency metrics: Size per order ratios and average trade characteristics\n",
    "2. Price dynamics: Trade-to-trade log returns calculated within each time bucket\n",
    "3. Transaction value: Dollar amounts combining price and volume information\n",
    "4. Multi-timeframe analysis: Aggregating trade features across different interval lengths\n",
    "\n",
    "This creates a comprehensive view of actual trading activity to complement the quoted market data from orderbook snapshots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d695542e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T19:38:30.574804Z",
     "iopub.status.busy": "2025-09-06T19:38:30.574430Z",
     "iopub.status.idle": "2025-09-06T19:38:30.582197Z",
     "shell.execute_reply": "2025-09-06T19:38:30.581134Z"
    },
    "papermill": {
     "duration": 0.018858,
     "end_time": "2025-09-06T19:38:30.583971",
     "exception": false,
     "start_time": "2025-09-06T19:38:30.565113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Process trade data file and extract features for multiple time intervals\n",
    "def preprocessor_trade(file_path, time_length_list):\n",
    "    # Extract stock ID from file path (format: stock_id=123.parquet)\n",
    "    stock_id = int(file_path.split('=')[1].split('.')[0])\n",
    "    \n",
    "    # Load trade data\n",
    "    df = pl.read_parquet(file_path)\n",
    "    \n",
    "    # Sort by time for proper sequence processing\n",
    "    df = df.sort([\"time_id\", \"seconds_in_bucket\"])\n",
    "    \n",
    "    # Calculate trade-specific features\n",
    "    df = df.with_columns([\n",
    "        # Average trade size per order\n",
    "        (pl.col(\"size\") / pl.col(\"order_count\")).alias(\"size_per_order\"),\n",
    "        # Price change (log return) within each time bucket\n",
    "        (pl.col(\"price\").diff().over(\"time_id\")).alias(\"trade_log_return\"),\n",
    "        # Mean trade size (aggregated feature)\n",
    "        pl.col(\"size\").mean().alias(\"size_mean\"),\n",
    "        # Dollar amount per trade (price * volume)\n",
    "        (pl.col(\"price\") * pl.col(\"size\")).alias(\"amount\")\n",
    "    ])\n",
    "    \n",
    "    # Aggregate features for different time intervals\n",
    "    merged_df = None\n",
    "    for length in time_length_list:\n",
    "        df_agg = aggregate_interval_trade(df, length)\n",
    "        merged_df = df_agg if merged_df is None else merged_df.join(\n",
    "            df_agg, on=\"time_id\", how=\"left\")\n",
    "    \n",
    "    # Get full interval features and merge with interval-specific features\n",
    "    df_feature = aggregate_interval_trade(df)  # Full 600-second interval\n",
    "    df_feature = df_feature.join(merged_df, on=\"time_id\", how=\"left\")\n",
    "    \n",
    "    # Create unique row identifier\n",
    "    df_feature = df_feature.with_columns(\n",
    "        (pl.lit(stock_id).cast(pl.Utf8) + \"-\" + \n",
    "         pl.col(\"time_id\").cast(pl.Utf8)).alias(\"row_id\")\n",
    "    )\n",
    "    \n",
    "    return df_feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ce72c7",
   "metadata": {},
   "source": [
    "## 1.3 Combine Orderbook and Trade Data\n",
    "\n",
    "This is the orchestrating function that combines all preprocessing steps into a unified workflow. It processes multiple stocks simultaneously, merging orderbook and trade data to create a comprehensive feature matrix for machine learning models.\n",
    "The pipeline workflow:\n",
    "\n",
    "1. Parallel processing: Handles multiple stock files simultaneously with progress tracking\n",
    "2. Data fusion: Combines orderbook snapshots with trade executions using unique identifiers\n",
    "3. Target integration: Merges features with target variables for training or prediction\n",
    "4. Data cleaning: Handles numerical issues like infinite values before final output\n",
    "\n",
    "This serves as the main entry point for the entire feature engineering pipeline, producing model-ready datasets from raw market data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "67d0ad5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T19:38:30.714080Z",
     "iopub.status.busy": "2025-09-06T19:38:30.713650Z",
     "iopub.status.idle": "2025-09-06T19:38:30.722020Z",
     "shell.execute_reply": "2025-09-06T19:38:30.720568Z"
    },
    "papermill": {
     "duration": 0.020005,
     "end_time": "2025-09-06T19:38:30.723696",
     "exception": false,
     "start_time": "2025-09-06T19:38:30.703691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Main preprocessing pipeline to combine orderbook and trade data\n",
    "def preprocessor(list_book_files, list_trade_files, target_data, time_length_list, train=True) -> pd.DataFrame:\n",
    "    dfs = []\n",
    "    \n",
    "    # Process each stock's orderbook and trade data\n",
    "    for book_file, trade_file in tqdm(zip(list_book_files, list_trade_files)):\n",
    "        # Extract features from orderbook data\n",
    "        df_book = preprocessor_book(book_file, time_length_list)\n",
    "        \n",
    "        # Extract features from trade data  \n",
    "        df_trade = preprocessor_trade(trade_file, time_length_list)\n",
    "        \n",
    "        # Combine orderbook and trade features on row_id\n",
    "        df_tmp = df_book.join(df_trade, on=\"row_id\", how=\"left\")\n",
    "        dfs.append(df_tmp)\n",
    "    \n",
    "    # Concatenate all stocks into single dataframe\n",
    "    df = pl.concat(dfs)\n",
    "    \n",
    "    # Prepare target data with matching row_id format\n",
    "    target_data = target_data.with_columns(\n",
    "        (pl.col(\"stock_id\").cast(pl.Utf8) + \"-\" +\n",
    "         pl.col(\"time_id\").cast(pl.Utf8)).alias(\"row_id\")\n",
    "    )\n",
    "    \n",
    "    # Select appropriate columns based on train/test mode\n",
    "    if train:\n",
    "        target_data = target_data.select([\n",
    "            \"row_id\",\n",
    "            \"target\"\n",
    "        ])\n",
    "    else:\n",
    "        target_data = target_data.select([\n",
    "            \"row_id\",\n",
    "        ])\n",
    "    \n",
    "    # Extract stock_id back from row_id for features dataframe\n",
    "    df = df.with_columns(\n",
    "        pl.col(\"row_id\").str.split(\"-\").list.get(0).cast(pl.Int64).alias(\"stock_id\")\n",
    "    )\n",
    "    \n",
    "    # Join target data with features\n",
    "    df = target_data.join(df, on=\"row_id\", how=\"left\")\n",
    "    \n",
    "    # Convert to pandas and handle infinite values\n",
    "    df = df.to_pandas()\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d9b581f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T19:38:30.774414Z",
     "iopub.status.busy": "2025-09-06T19:38:30.774082Z",
     "iopub.status.idle": "2025-09-06T19:38:30.966230Z",
     "shell.execute_reply": "2025-09-06T19:38:30.965139Z"
    },
    "papermill": {
     "duration": 0.202716,
     "end_time": "2025-09-06T19:38:30.967862",
     "exception": false,
     "start_time": "2025-09-06T19:38:30.765146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_book_test= glob.glob(\n",
    "    'data/optiver-realized-volatility-prediction/book_test.parquet/*')\n",
    "list_trade_test = glob.glob(\n",
    "    'data/optiver-realized-volatility-prediction/trade_test.parquet/*')\n",
    "test_data = pl.read_csv(\n",
    "    'data/optiver-realized-volatility-prediction/test.csv')\n",
    "train = pd.read_csv(\"data/optiver-realized-volatility-prediction/train.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd20d53",
   "metadata": {
    "papermill": {
     "duration": 0.007979,
     "end_time": "2025-09-06T19:38:30.984502",
     "exception": false,
     "start_time": "2025-09-06T19:38:30.976523",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1.4 K-fold Feature Extraction based on K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5d3d7d",
   "metadata": {},
   "source": [
    "This section implements a clustering approach to group stocks with similar return patterns, enabling the creation of cross-sectional market features that capture sector or style effects.\n",
    "The clustering methodology:\n",
    "\n",
    "1. Correlation matrix construction: Pivots target returns to create stock-vs-time matrix for correlation analysis\n",
    "2. K-means clustering: Groups stocks into 7 clusters based on their return correlation patterns\n",
    "3. Cluster feature generation: Creates aggregate market indicators by averaging key features within each cluster\n",
    "4. Feature filtering: Only includes clusters with sufficient stocks (â‰¥10) for statistical reliability\n",
    "5. Cross-stock signals: Generates features that capture broader market dynamics beyond individual stock characteristics\n",
    "\n",
    "This approach creates features that reflect market regimes, sector rotations, and systematic risk factors that affect groups of similar stocks simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c5073e12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T19:38:31.003393Z",
     "iopub.status.busy": "2025-09-06T19:38:31.003089Z",
     "iopub.status.idle": "2025-09-06T19:38:31.865838Z",
     "shell.execute_reply": "2025-09-06T19:38:31.865093Z"
    },
    "papermill": {
     "duration": 0.874806,
     "end_time": "2025-09-06T19:38:31.867693",
     "exception": false,
     "start_time": "2025-09-06T19:38:30.992887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "train_p = train.pivot(\n",
    "    index='time_id',\n",
    "    columns='stock_id',\n",
    "    values='target'\n",
    ")\n",
    "\n",
    "corr = train_p.corr()\n",
    "ids = corr.index\n",
    "cluster_num = 7\n",
    "kmeans = KMeans(n_clusters=cluster_num, random_state=10,n_init=10).fit(corr.values)\n",
    "labels = kmeans.labels_\n",
    "cluster_stocks = {}\n",
    "for i in range(cluster_num):\n",
    "    cluster_stocks[i] = [ids[j] for j in range(len(ids)) if labels[j] == i]\n",
    "\n",
    "\n",
    "def generate_cluster_features(df: pd.DataFrame, cluster_stocks: dict) -> pd.DataFrame:\n",
    "    cluster_features = {}\n",
    "\n",
    "    for c, stocks in cluster_stocks.items():\n",
    "        #print(f\"Cluster {c} has {len(stocks)} stocks: {stocks}\")\n",
    "        if len(stocks)<10:\n",
    "            continue\n",
    "        \n",
    "        cluster_df = df[df['stock_id'].isin(stocks)]\n",
    "        \n",
    "        \n",
    "        numeric_cols = ['log_return1_realized_vol', 'total_volume_sum', 'size_sum', 'order_count_sum',\n",
    "                        'price_spread_sum', 'bid_spread_sum', 'ask_spread_sum', 'volume_imbalance_sum','size_tau2']\n",
    "        cluster_avg = (\n",
    "            cluster_df.groupby(\"time_id\")[numeric_cols]\n",
    "            .mean()\n",
    "        )\n",
    "        \n",
    "        cluster_avg = cluster_avg.rename(\n",
    "            columns={col: f\"cluster_{c}_{col}\" for col in numeric_cols}\n",
    "        )\n",
    "        cluster_features[c] = cluster_avg\n",
    "        \n",
    "\n",
    "    cluster_list = list(cluster_features.values())\n",
    "    \n",
    "    result = cluster_list[0]\n",
    "    \n",
    "    for cl in cluster_list[1:]:\n",
    "        result = result.merge(cl, on='time_id', how='outer')\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c9094fb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T19:38:31.959104Z",
     "iopub.status.busy": "2025-09-06T19:38:31.958778Z",
     "iopub.status.idle": "2025-09-06T19:38:32.543815Z",
     "shell.execute_reply": "2025-09-06T19:38:32.542871Z"
    },
    "papermill": {
     "duration": 0.596688,
     "end_time": "2025-09-06T19:38:32.546171",
     "exception": false,
     "start_time": "2025-09-06T19:38:31.949483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize stock clusters using PCA dimensionality reduction\n",
    "\n",
    "# Reduce correlation matrix to 2D for visualization\n",
    "pca = PCA(n_components=2)\n",
    "corr_2d = pca.fit_transform(corr.values)\n",
    "\n",
    "# Create scatter plot of stocks in 2D PCA space\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(corr_2d[:, 0], corr_2d[:, 1], c=kmeans.labels_, cmap='tab10', s=100)\n",
    "\n",
    "# Add stock ID labels to each point\n",
    "for i, stock_id in enumerate(corr.index):\n",
    "    plt.text(corr_2d[i, 0], corr_2d[i, 1], str(stock_id), fontsize=9)\n",
    "\n",
    "plt.title(\"KMeans Clustering Visualization (PCA 2D)\")\n",
    "plt.xlabel(\"PC1\")  \n",
    "plt.ylabel(\"PC2\") \n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cd8d08d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T19:38:32.568386Z",
     "iopub.status.busy": "2025-09-06T19:38:32.568077Z",
     "iopub.status.idle": "2025-09-06T19:38:32.614700Z",
     "shell.execute_reply": "2025-09-06T19:38:32.613620Z"
    },
    "papermill": {
     "duration": 0.059043,
     "end_time": "2025-09-06T19:38:32.616377",
     "exception": false,
     "start_time": "2025-09-06T19:38:32.557334",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_book_train = glob.glob(\n",
    "    'data/optiver-realized-volatility-prediction/book_train.parquet/*')\n",
    "list_trade_train = glob.glob(\n",
    "    'data/optiver-realized-volatility-prediction/trade_train.parquet/*')\n",
    "train_data = pl.read_csv(\n",
    "    'data/optiver-realized-volatility-prediction/train.csv')\n",
    "\n",
    "time_length_list = [500,400,300,200,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ed2d7ff8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T19:38:32.638450Z",
     "iopub.status.busy": "2025-09-06T19:38:32.638126Z",
     "iopub.status.idle": "2025-09-06T19:40:22.611015Z",
     "shell.execute_reply": "2025-09-06T19:40:22.609922Z"
    },
    "papermill": {
     "duration": 109.986276,
     "end_time": "2025-09-06T19:40:22.612656",
     "exception": false,
     "start_time": "2025-09-06T19:38:32.626380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112it [00:38,  2.92it/s]\n",
      "1it [00:00, 90.60it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train = preprocessor(list_book_train, list_trade_train,train_data,time_length_list)\n",
    "df_test = preprocessor(list_book_test,list_trade_test,test_data,time_length_list,train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4465b8bf",
   "metadata": {},
   "source": [
    "## 1.5 Cross-sectional Volatility Features\n",
    "\n",
    "This function creates historical context features by aggregating volatility statistics across two dimensions: individual stock histories and market-wide time periods. These features help capture both stock-specific volatility patterns and broader market regime characteristics.\n",
    "The feature engineering approach:\n",
    "\n",
    "1. Stock-level aggregation: Computes historical volatility statistics for each stock across all time periods\n",
    "2. Time-level aggregation: Calculates cross-sectional volatility statistics across all stocks for each time period\n",
    "3. Multi-timeframe analysis: Includes volatility measures from different interval lengths (200s, 300s, 400s, full period)\n",
    "4. Statistical summaries: Captures mean, standard deviation, maximum, and minimum values for robust characterization\n",
    "5. Contextual enrichment: Adds these aggregated features back to the original dataset for enhanced predictive power\n",
    "\n",
    "This creates features that distinguish between stock-specific volatility patterns and market-wide volatility regimes occurring at specific time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3d870e42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T19:40:22.645497Z",
     "iopub.status.busy": "2025-09-06T19:40:22.645157Z",
     "iopub.status.idle": "2025-09-06T19:40:22.652828Z",
     "shell.execute_reply": "2025-09-06T19:40:22.651974Z"
    },
    "papermill": {
     "duration": 0.025958,
     "end_time": "2025-09-06T19:40:22.654369",
     "exception": false,
     "start_time": "2025-09-06T19:40:22.628411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add stock and time-level volatility statistics as features\n",
    "def get_time_stock(df):\n",
    "    # Volatility columns across different time windows\n",
    "    vol_cols = [\n",
    "        'log_return1_realized_vol', 'log_return2_realized_vol',\n",
    "        'log_return1_realized_vol_400', 'log_return2_realized_vol_400',\n",
    "        'log_return1_realized_vol_300', 'log_return2_realized_vol_300',\n",
    "        'log_return1_realized_vol_200', 'log_return2_realized_vol_200',\n",
    "        'trade_log_return_realized_vol', 'trade_log_return_realized_vol_400',\n",
    "        'trade_log_return_realized_vol_300', 'trade_log_return_realized_vol_200'\n",
    "    ]\n",
    "    \n",
    "    # Stock-level volatility statistics\n",
    "    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min']).reset_index()\n",
    "    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
    "    df_stock_id = df_stock_id.add_suffix('_stock')\n",
    "    \n",
    "    # Time-level volatility statistics\n",
    "    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min']).reset_index()\n",
    "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
    "    df_time_id = df_time_id.add_suffix('_time')\n",
    "    \n",
    "    # Merge back to original dataframe\n",
    "    df = df.merge(df_stock_id, how='left', left_on=['stock_id'], right_on=['stock_id__stock'])\n",
    "    df = df.merge(df_time_id, how='left', left_on=['time_id'], right_on=['time_id__time'])\n",
    "    df.drop(['stock_id__stock', 'time_id__time'], axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f19dfc92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T19:40:22.685958Z",
     "iopub.status.busy": "2025-09-06T19:40:22.685528Z",
     "iopub.status.idle": "2025-09-06T19:40:24.648909Z",
     "shell.execute_reply": "2025-09-06T19:40:24.647736Z"
    },
    "papermill": {
     "duration": 1.983391,
     "end_time": "2025-09-06T19:40:24.652720",
     "exception": false,
     "start_time": "2025-09-06T19:40:22.669329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train=get_time_stock(df_train)\n",
    "df_test=get_time_stock(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedb037b",
   "metadata": {},
   "source": [
    "## 1.6 Activity-based Feature\n",
    "\n",
    "This section creates tau features that serve as activity-based scaling factors, capturing the intensity and distribution of trading activity across different timeframes. These features help normalize other metrics based on the underlying activity levels.\n",
    "The tau feature construction includes:\n",
    "\n",
    "1. Time-based tau: Inverse square root scaling based on temporal coverage, indicating how concentrated trading activity is within time buckets\n",
    "2. Order-based tau: Scaling factors based on order count intensity with interval-specific weights (0.33, 0.5, 0.66)\n",
    "3. Delta tau: Captures changes in activity intensity between different time intervals\n",
    "\n",
    "These tau features provide important context about market activity patterns that can help models better interpret other technical indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "90824e2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T19:40:24.689136Z",
     "iopub.status.busy": "2025-09-06T19:40:24.688776Z",
     "iopub.status.idle": "2025-09-06T19:40:26.467509Z",
     "shell.execute_reply": "2025-09-06T19:40:26.466351Z"
    },
    "papermill": {
     "duration": 1.799228,
     "end_time": "2025-09-06T19:40:26.470717",
     "exception": false,
     "start_time": "2025-09-06T19:40:24.671489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create tau features - activity-based scaling factors\n",
    "\n",
    "# Time-based tau: inverse square root of unique time coverage\n",
    "df_train['size_tau'] = np.sqrt(1 / df_train['seconds_in_bucket_unique'])\n",
    "df_test['size_tau'] = np.sqrt(1 / df_test['seconds_in_bucket_unique'])\n",
    "\n",
    "df_train['size_tau_400'] = np.sqrt(1 / df_train['seconds_in_bucket_unique_400'])\n",
    "df_test['size_tau_400'] = np.sqrt(1 / df_test['seconds_in_bucket_unique_400'])\n",
    "\n",
    "df_train['size_tau_300'] = np.sqrt(1 / df_train['seconds_in_bucket_unique_300'])\n",
    "df_test['size_tau_300'] = np.sqrt(1 / df_test['seconds_in_bucket_unique_300'])\n",
    "\n",
    "df_train['size_tau_200'] = np.sqrt(1 / df_train['seconds_in_bucket_unique_200'])\n",
    "df_test['size_tau_200'] = np.sqrt(1 / df_test['seconds_in_bucket_unique_200'])\n",
    "\n",
    "# Order-based tau: inverse square root of order count with scaling\n",
    "df_train['size_tau2'] = np.sqrt(1 / df_train['order_count_sum'])\n",
    "df_test['size_tau2'] = np.sqrt(1 / df_test['order_count_sum'])\n",
    "\n",
    "df_train['size_tau2_400'] = np.sqrt(0.33 / df_train['order_count_sum'])\n",
    "df_test['size_tau2_400'] = np.sqrt(0.33 / df_test['order_count_sum'])\n",
    "\n",
    "df_train['size_tau2_300'] = np.sqrt(0.5 / df_train['order_count_sum'])\n",
    "df_test['size_tau2_300'] = np.sqrt(0.5 / df_test['order_count_sum'])\n",
    "\n",
    "df_train['size_tau2_200'] = np.sqrt(0.66 / df_train['order_count_sum'])\n",
    "df_test['size_tau2_200'] = np.sqrt(0.66 / df_test['order_count_sum'])\n",
    "\n",
    "# Delta tau - difference between interval and full period\n",
    "df_train['size_tau2_d'] = df_train['size_tau2_400'] - df_train['size_tau2']\n",
    "df_test['size_tau2_d'] = df_test['size_tau2_400'] - df_test['size_tau2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e744340e",
   "metadata": {},
   "source": [
    "## 1.7 Final Dataset Assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f6ed8f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and merge cluster features\n",
    "train_cluster = generate_cluster_features(df_train, cluster_stocks).reset_index()\n",
    "train = pd.merge(df_train, train_cluster, how='left', on='time_id')\n",
    "\n",
    "test_cluster = generate_cluster_features(df_test, cluster_stocks).reset_index()\n",
    "\n",
    "# Extract time_id from row_id for test data\n",
    "df_test['time_id'] = df_test['row_id'].str.split(\"-\").str[1]\n",
    "df_test['time_id'] = df_test['time_id'].astype(int)\n",
    "test_cluster['time_id'] = test_cluster['time_id'].astype(int)\n",
    "\n",
    "test = pd.merge(df_test, test_cluster, how='left', on='time_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726379f9",
   "metadata": {},
   "source": [
    "# 2. Model Training\n",
    "## 2.0 Cross-validation Setup\n",
    "\n",
    "This section implements a cross-validation strategy specifically designed for time series financial data. Unlike standard random splitting, this approach ensures that validation folds are representative of the feature space while maintaining temporal structure.\n",
    "The algorithm methodology:\n",
    "\n",
    "1. Distance-based selection: Uses Euclidean distance in normalized feature space to create diverse yet representative folds\n",
    "2. Iterative fold construction: Builds each fold sequentially by selecting points based on distance probabilities from existing fold centers\n",
    "3. Feature space normalization: Applies MinMax scaling to prevent any single feature from dominating distance calculations\n",
    "4. Probabilistic sampling: Converts distances to selection probabilities, ensuring closer points to fold centers have higher selection likelihood\n",
    "5. Balanced representation: Ensures each fold captures different regions of the feature space for robust cross-validation\n",
    "\n",
    "This creates more statistically sound validation splits compared to simple random partitioning, particularly important for financial time series where temporal dependencies and regime changes can significantly impact model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a8aa8069",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T19:40:26.515323Z",
     "iopub.status.busy": "2025-09-06T19:40:26.514990Z",
     "iopub.status.idle": "2025-09-06T19:40:43.209816Z",
     "shell.execute_reply": "2025-09-06T19:40:43.207876Z"
    },
    "papermill": {
     "duration": 16.725722,
     "end_time": "2025-09-06T19:40:43.214949",
     "exception": false,
     "start_time": "2025-09-06T19:40:26.489227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nfolds = 5\n",
    "n_time = train_p.shape[0]\n",
    "distance_sum=[]\n",
    "distance_sum = [np.zeros(n_time-nfolds) for _ in range(nfolds)]\n",
    "selected_mat=[]\n",
    "train_p = train_p.fillna(train_p.mean())\n",
    "target_mat = train_p.values\n",
    "target_mat = MinMaxScaler(feature_range=(-1, 1)).fit_transform(target_mat)\n",
    "target_mat = np.c_[target_mat, range(n_time)]\n",
    "num_per_fold = int(n_time/nfolds)\n",
    "\n",
    "# Random select points to be the initial centers\n",
    "centers = np.random.choice(np.array(n_time), size=nfolds, replace=False)\n",
    "centers = np.sort(centers)[::-1]\n",
    "# The index of rows that have been considered\n",
    "ind_values = [[centers[i]] for i in range(nfolds)]\n",
    "\n",
    "for i in range(nfolds):\n",
    "    # The row that have been considered\n",
    "    selected_mat.append(target_mat[ind_values[i], :])\n",
    "    target_mat = np.delete(target_mat, obj=ind_values[i], axis=0)\n",
    "    \n",
    "for i in range(num_per_fold):\n",
    "    for j in range(nfolds):\n",
    "        threshold = np.random.uniform(0, 1, 1)\n",
    "        # Match the size foe future matrix calculation\n",
    "        selected_mat[j] = np.tile(selected_mat[j], (target_mat.shape[0], 1))\n",
    "        distance_sum[j] += np.sum((target_mat[:, :-1] -\n",
    "                               selected_mat[j][:, :-1])**2, axis=1)\n",
    "        prob_vec = distance_sum[j]/np.sum(distance_sum[j])\n",
    "        cum_prob = 0\n",
    "        line_idx = 0\n",
    "        for val in prob_vec:\n",
    "            cum_prob += val\n",
    "            if (cum_prob > threshold):  # the column was selected\n",
    "                break\n",
    "            line_idx += 1\n",
    "        cum_prob = 0\n",
    "\n",
    "        # Update parameters\n",
    "        \n",
    "        for n in range(nfolds):\n",
    "            distance_sum[n] = np.delete(distance_sum[n].copy(), line_idx)\n",
    "        centers[j] = line_idx\n",
    "        selected_mat[j] = target_mat[line_idx, :]\n",
    "        ind_values[j].append(target_mat[line_idx, -1])\n",
    "        target_mat = np.delete(target_mat, obj=line_idx, axis=0)\n",
    "  \n",
    "    if target_mat.shape[0]==0:\n",
    "        break\n",
    "    \n",
    "for ind in range(nfolds):\n",
    "    ind_values[ind] = train_p.index[[int(term) for term in ind_values[ind]]]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d065118",
   "metadata": {
    "papermill": {
     "duration": 0.020341,
     "end_time": "2025-09-06T19:40:43.265632",
     "exception": false,
     "start_time": "2025-09-06T19:40:43.245291",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.1 LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b3824c1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T19:40:43.306683Z",
     "iopub.status.busy": "2025-09-06T19:40:43.306169Z",
     "iopub.status.idle": "2025-09-06T19:40:58.026609Z",
     "shell.execute_reply": "2025-09-06T19:40:58.025564Z"
    },
    "papermill": {
     "duration": 14.743773,
     "end_time": "2025-09-06T19:40:58.028432",
     "exception": false,
     "start_time": "2025-09-06T19:40:43.284659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "import lightgbm as lgb\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160e2910",
   "metadata": {},
   "source": [
    "### 2.1.1 Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d582f492",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T19:40:58.061945Z",
     "iopub.status.busy": "2025-09-06T19:40:58.060540Z",
     "iopub.status.idle": "2025-09-06T19:40:58.068110Z",
     "shell.execute_reply": "2025-09-06T19:40:58.066522Z"
    },
    "papermill": {
     "duration": 0.026174,
     "end_time": "2025-09-06T19:40:58.070494",
     "exception": false,
     "start_time": "2025-09-06T19:40:58.044320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Root Mean Squared Percentage Error (RMSPE) implementations\n",
    "\n",
    "# NumPy version for evaluation\n",
    "def rmspe(y_true, y_pred):\n",
    "    \"\"\"Root mean squared percentage error - measures relative prediction accuracy\"\"\"\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "# PyTorch version for neural network training\n",
    "def RMSPELoss(y_pred, y_true):\n",
    "    \"\"\"RMSPE loss for PyTorch models\"\"\"\n",
    "    return torch.sqrt(torch.mean(((y_true - y_pred) / y_true) ** 2)).clone()\n",
    "\n",
    "# LightGBM evaluation function\n",
    "def rmspe_feval(y_pred, dataset):\n",
    "    \"\"\"RMSPE evaluation function for LightGBM\"\"\"\n",
    "    y_true = dataset.get_label()\n",
    "    return 'RMSPE', round(rmspe(y_true=y_true, y_pred=y_pred), 5), False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcf33f1",
   "metadata": {},
   "source": [
    "### 2.1.2 LightGBM Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a15dce37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T19:40:58.148429Z",
     "iopub.status.busy": "2025-09-06T19:40:58.147388Z",
     "iopub.status.idle": "2025-09-06T20:04:20.967322Z",
     "shell.execute_reply": "2025-09-06T20:04:20.965936Z"
    },
    "papermill": {
     "duration": 1402.861092,
     "end_time": "2025-09-06T20:04:20.991124",
     "exception": false,
     "start_time": "2025-09-06T19:40:58.130032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010743 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 47230\n",
      "[LightGBM] [Info] Number of data points in the train set: 343145, number of used features: 215\n",
      "[LightGBM] [Info] Start training from score 0.001797\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's rmse: 0.000514226\ttraining's RMSPE: 0.23818\tvalid_1's rmse: 0.000536354\tvalid_1's RMSPE: 0.24744\n",
      "[200]\ttraining's rmse: 0.000464107\ttraining's RMSPE: 0.21497\tvalid_1's rmse: 0.000492755\tvalid_1's RMSPE: 0.22733\n",
      "[300]\ttraining's rmse: 0.000450792\ttraining's RMSPE: 0.2088\tvalid_1's rmse: 0.00048056\tvalid_1's RMSPE: 0.2217\n",
      "[400]\ttraining's rmse: 0.000442063\ttraining's RMSPE: 0.20475\tvalid_1's rmse: 0.000473382\tvalid_1's RMSPE: 0.21839\n",
      "[500]\ttraining's rmse: 0.000435211\ttraining's RMSPE: 0.20158\tvalid_1's rmse: 0.000467943\tvalid_1's RMSPE: 0.21588\n",
      "[600]\ttraining's rmse: 0.000429138\ttraining's RMSPE: 0.19877\tvalid_1's rmse: 0.000463554\tvalid_1's RMSPE: 0.21385\n",
      "[700]\ttraining's rmse: 0.000424311\ttraining's RMSPE: 0.19653\tvalid_1's rmse: 0.000460553\tvalid_1's RMSPE: 0.21247\n",
      "[800]\ttraining's rmse: 0.000420079\ttraining's RMSPE: 0.19457\tvalid_1's rmse: 0.000457982\tvalid_1's RMSPE: 0.21128\n",
      "[900]\ttraining's rmse: 0.000416063\ttraining's RMSPE: 0.19271\tvalid_1's rmse: 0.000455068\tvalid_1's RMSPE: 0.20994\n",
      "[1000]\ttraining's rmse: 0.000412627\ttraining's RMSPE: 0.19112\tvalid_1's rmse: 0.000452818\tvalid_1's RMSPE: 0.2089\n",
      "[1100]\ttraining's rmse: 0.000409278\ttraining's RMSPE: 0.18957\tvalid_1's rmse: 0.000450761\tvalid_1's RMSPE: 0.20795\n",
      "[1200]\ttraining's rmse: 0.000406108\ttraining's RMSPE: 0.1881\tvalid_1's rmse: 0.00044879\tvalid_1's RMSPE: 0.20704\n",
      "[1300]\ttraining's rmse: 0.000403402\ttraining's RMSPE: 0.18685\tvalid_1's rmse: 0.000447489\tvalid_1's RMSPE: 0.20644\n",
      "[1400]\ttraining's rmse: 0.000400758\ttraining's RMSPE: 0.18562\tvalid_1's rmse: 0.000445902\tvalid_1's RMSPE: 0.20571\n",
      "[1500]\ttraining's rmse: 0.000398421\ttraining's RMSPE: 0.18454\tvalid_1's rmse: 0.000444733\tvalid_1's RMSPE: 0.20517\n",
      "[1600]\ttraining's rmse: 0.000396017\ttraining's RMSPE: 0.18343\tvalid_1's rmse: 0.00044307\tvalid_1's RMSPE: 0.2044\n",
      "[1700]\ttraining's rmse: 0.000393791\ttraining's RMSPE: 0.1824\tvalid_1's rmse: 0.000441813\tvalid_1's RMSPE: 0.20382\n",
      "[1800]\ttraining's rmse: 0.000391773\ttraining's RMSPE: 0.18146\tvalid_1's rmse: 0.000440781\tvalid_1's RMSPE: 0.20335\n",
      "[1900]\ttraining's rmse: 0.000389844\ttraining's RMSPE: 0.18057\tvalid_1's rmse: 0.000439791\tvalid_1's RMSPE: 0.20289\n",
      "[2000]\ttraining's rmse: 0.000388082\ttraining's RMSPE: 0.17975\tvalid_1's rmse: 0.000439129\tvalid_1's RMSPE: 0.20259\n",
      "[2100]\ttraining's rmse: 0.00038628\ttraining's RMSPE: 0.17892\tvalid_1's rmse: 0.000438479\tvalid_1's RMSPE: 0.20229\n",
      "[2200]\ttraining's rmse: 0.000384599\ttraining's RMSPE: 0.17814\tvalid_1's rmse: 0.000437664\tvalid_1's RMSPE: 0.20191\n",
      "[2300]\ttraining's rmse: 0.000382994\ttraining's RMSPE: 0.1774\tvalid_1's rmse: 0.00043697\tvalid_1's RMSPE: 0.20159\n",
      "[2400]\ttraining's rmse: 0.000381406\ttraining's RMSPE: 0.17666\tvalid_1's rmse: 0.000436341\tvalid_1's RMSPE: 0.2013\n",
      "[2500]\ttraining's rmse: 0.000379842\ttraining's RMSPE: 0.17594\tvalid_1's rmse: 0.000435514\tvalid_1's RMSPE: 0.20092\n",
      "[2600]\ttraining's rmse: 0.000378384\ttraining's RMSPE: 0.17526\tvalid_1's rmse: 0.000434632\tvalid_1's RMSPE: 0.20051\n",
      "[2700]\ttraining's rmse: 0.00037694\ttraining's RMSPE: 0.17459\tvalid_1's rmse: 0.000434049\tvalid_1's RMSPE: 0.20024\n",
      "[2800]\ttraining's rmse: 0.00037562\ttraining's RMSPE: 0.17398\tvalid_1's rmse: 0.000433366\tvalid_1's RMSPE: 0.19993\n",
      "Early stopping, best iteration is:\n",
      "[2815]\ttraining's rmse: 0.000375415\ttraining's RMSPE: 0.17389\tvalid_1's rmse: 0.000433268\tvalid_1's RMSPE: 0.19988\n",
      "Fold 2\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012344 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 47294\n",
      "[LightGBM] [Info] Number of data points in the train set: 343145, number of used features: 215\n",
      "[LightGBM] [Info] Start training from score 0.001802\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's rmse: 0.000513329\ttraining's RMSPE: 0.23747\tvalid_1's rmse: 0.000522384\tvalid_1's RMSPE: 0.24219\n",
      "[200]\ttraining's rmse: 0.000463231\ttraining's RMSPE: 0.21429\tvalid_1's rmse: 0.00047349\tvalid_1's RMSPE: 0.21953\n",
      "[300]\ttraining's rmse: 0.000449993\ttraining's RMSPE: 0.20817\tvalid_1's rmse: 0.000462168\tvalid_1's RMSPE: 0.21428\n",
      "[400]\ttraining's rmse: 0.000441404\ttraining's RMSPE: 0.2042\tvalid_1's rmse: 0.000455348\tvalid_1's RMSPE: 0.21111\n",
      "[500]\ttraining's rmse: 0.000434226\ttraining's RMSPE: 0.20088\tvalid_1's rmse: 0.000449964\tvalid_1's RMSPE: 0.20862\n",
      "[600]\ttraining's rmse: 0.000428794\ttraining's RMSPE: 0.19836\tvalid_1's rmse: 0.000446625\tvalid_1's RMSPE: 0.20707\n",
      "[700]\ttraining's rmse: 0.000423883\ttraining's RMSPE: 0.19609\tvalid_1's rmse: 0.000443371\tvalid_1's RMSPE: 0.20556\n",
      "[800]\ttraining's rmse: 0.00041948\ttraining's RMSPE: 0.19405\tvalid_1's rmse: 0.00044036\tvalid_1's RMSPE: 0.20417\n",
      "[900]\ttraining's rmse: 0.000415269\ttraining's RMSPE: 0.19211\tvalid_1's rmse: 0.000437325\tvalid_1's RMSPE: 0.20276\n",
      "[1000]\ttraining's rmse: 0.000411595\ttraining's RMSPE: 0.19041\tvalid_1's rmse: 0.000434974\tvalid_1's RMSPE: 0.20167\n",
      "[1100]\ttraining's rmse: 0.000408344\ttraining's RMSPE: 0.1889\tvalid_1's rmse: 0.000433083\tvalid_1's RMSPE: 0.20079\n",
      "[1200]\ttraining's rmse: 0.000405393\ttraining's RMSPE: 0.18754\tvalid_1's rmse: 0.00043148\tvalid_1's RMSPE: 0.20005\n",
      "[1300]\ttraining's rmse: 0.000402644\ttraining's RMSPE: 0.18627\tvalid_1's rmse: 0.000430454\tvalid_1's RMSPE: 0.19957\n",
      "[1400]\ttraining's rmse: 0.000400026\ttraining's RMSPE: 0.18505\tvalid_1's rmse: 0.000429105\tvalid_1's RMSPE: 0.19895\n",
      "[1500]\ttraining's rmse: 0.000397605\ttraining's RMSPE: 0.18394\tvalid_1's rmse: 0.000427882\tvalid_1's RMSPE: 0.19838\n",
      "[1600]\ttraining's rmse: 0.000395406\ttraining's RMSPE: 0.18292\tvalid_1's rmse: 0.000426745\tvalid_1's RMSPE: 0.19785\n",
      "[1700]\ttraining's rmse: 0.000393194\ttraining's RMSPE: 0.18189\tvalid_1's rmse: 0.000425669\tvalid_1's RMSPE: 0.19735\n",
      "[1800]\ttraining's rmse: 0.000391287\ttraining's RMSPE: 0.18101\tvalid_1's rmse: 0.000425122\tvalid_1's RMSPE: 0.1971\n",
      "Early stopping, best iteration is:\n",
      "[1790]\ttraining's rmse: 0.000391463\ttraining's RMSPE: 0.18109\tvalid_1's rmse: 0.000424956\tvalid_1's RMSPE: 0.19702\n",
      "Fold 3\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013732 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 47296\n",
      "[LightGBM] [Info] Number of data points in the train set: 343146, number of used features: 215\n",
      "[LightGBM] [Info] Start training from score 0.001804\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's rmse: 0.000513675\ttraining's RMSPE: 0.23736\tvalid_1's rmse: 0.000525105\tvalid_1's RMSPE: 0.24457\n",
      "[200]\ttraining's rmse: 0.00046388\ttraining's RMSPE: 0.21435\tvalid_1's rmse: 0.000475015\tvalid_1's RMSPE: 0.22124\n",
      "[300]\ttraining's rmse: 0.000450639\ttraining's RMSPE: 0.20823\tvalid_1's rmse: 0.000462643\tvalid_1's RMSPE: 0.21548\n",
      "[400]\ttraining's rmse: 0.000442012\ttraining's RMSPE: 0.20424\tvalid_1's rmse: 0.000455574\tvalid_1's RMSPE: 0.21219\n",
      "[500]\ttraining's rmse: 0.000435442\ttraining's RMSPE: 0.20121\tvalid_1's rmse: 0.000450707\tvalid_1's RMSPE: 0.20992\n",
      "[600]\ttraining's rmse: 0.000429664\ttraining's RMSPE: 0.19854\tvalid_1's rmse: 0.000445945\tvalid_1's RMSPE: 0.2077\n",
      "[700]\ttraining's rmse: 0.000424568\ttraining's RMSPE: 0.19618\tvalid_1's rmse: 0.000442558\tvalid_1's RMSPE: 0.20612\n",
      "[800]\ttraining's rmse: 0.000420071\ttraining's RMSPE: 0.1941\tvalid_1's rmse: 0.000439405\tvalid_1's RMSPE: 0.20465\n",
      "[900]\ttraining's rmse: 0.000416059\ttraining's RMSPE: 0.19225\tvalid_1's rmse: 0.000437266\tvalid_1's RMSPE: 0.20366\n",
      "[1000]\ttraining's rmse: 0.000412354\ttraining's RMSPE: 0.19054\tvalid_1's rmse: 0.000434663\tvalid_1's RMSPE: 0.20245\n",
      "[1100]\ttraining's rmse: 0.000409097\ttraining's RMSPE: 0.18903\tvalid_1's rmse: 0.000432664\tvalid_1's RMSPE: 0.20152\n",
      "[1200]\ttraining's rmse: 0.000406133\ttraining's RMSPE: 0.18766\tvalid_1's rmse: 0.000430899\tvalid_1's RMSPE: 0.20069\n",
      "[1300]\ttraining's rmse: 0.000403344\ttraining's RMSPE: 0.18637\tvalid_1's rmse: 0.000429287\tvalid_1's RMSPE: 0.19994\n",
      "[1400]\ttraining's rmse: 0.000400811\ttraining's RMSPE: 0.1852\tvalid_1's rmse: 0.000428043\tvalid_1's RMSPE: 0.19936\n",
      "[1500]\ttraining's rmse: 0.000398469\ttraining's RMSPE: 0.18412\tvalid_1's rmse: 0.000426829\tvalid_1's RMSPE: 0.1988\n",
      "[1600]\ttraining's rmse: 0.000396184\ttraining's RMSPE: 0.18307\tvalid_1's rmse: 0.000425699\tvalid_1's RMSPE: 0.19827\n",
      "[1700]\ttraining's rmse: 0.000394071\ttraining's RMSPE: 0.18209\tvalid_1's rmse: 0.000424805\tvalid_1's RMSPE: 0.19785\n",
      "[1800]\ttraining's rmse: 0.000392129\ttraining's RMSPE: 0.18119\tvalid_1's rmse: 0.000424292\tvalid_1's RMSPE: 0.19762\n",
      "[1900]\ttraining's rmse: 0.000390254\ttraining's RMSPE: 0.18033\tvalid_1's rmse: 0.000423583\tvalid_1's RMSPE: 0.19729\n",
      "[2000]\ttraining's rmse: 0.000388384\ttraining's RMSPE: 0.17946\tvalid_1's rmse: 0.000422761\tvalid_1's RMSPE: 0.1969\n",
      "[2100]\ttraining's rmse: 0.000386666\ttraining's RMSPE: 0.17867\tvalid_1's rmse: 0.000422016\tvalid_1's RMSPE: 0.19656\n",
      "[2200]\ttraining's rmse: 0.000384896\ttraining's RMSPE: 0.17785\tvalid_1's rmse: 0.000421415\tvalid_1's RMSPE: 0.19628\n",
      "[2300]\ttraining's rmse: 0.000383174\ttraining's RMSPE: 0.17705\tvalid_1's rmse: 0.000420564\tvalid_1's RMSPE: 0.19588\n",
      "[2400]\ttraining's rmse: 0.000381618\ttraining's RMSPE: 0.17634\tvalid_1's rmse: 0.00042024\tvalid_1's RMSPE: 0.19573\n",
      "[2500]\ttraining's rmse: 0.000380177\ttraining's RMSPE: 0.17567\tvalid_1's rmse: 0.000420019\tvalid_1's RMSPE: 0.19563\n",
      "Early stopping, best iteration is:\n",
      "[2455]\ttraining's rmse: 0.000380817\ttraining's RMSPE: 0.17597\tvalid_1's rmse: 0.000420003\tvalid_1's RMSPE: 0.19562\n",
      "Fold 4\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018224 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 47293\n",
      "[LightGBM] [Info] Number of data points in the train set: 343146, number of used features: 215\n",
      "[LightGBM] [Info] Start training from score 0.001798\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's rmse: 0.000513008\ttraining's RMSPE: 0.23762\tvalid_1's rmse: 0.000519907\tvalid_1's RMSPE: 0.23982\n",
      "[200]\ttraining's rmse: 0.000462632\ttraining's RMSPE: 0.21429\tvalid_1's rmse: 0.000472471\tvalid_1's RMSPE: 0.21794\n",
      "[300]\ttraining's rmse: 0.00044944\ttraining's RMSPE: 0.20818\tvalid_1's rmse: 0.000461021\tvalid_1's RMSPE: 0.21265\n",
      "[400]\ttraining's rmse: 0.000440913\ttraining's RMSPE: 0.20423\tvalid_1's rmse: 0.00045384\tvalid_1's RMSPE: 0.20934\n",
      "[500]\ttraining's rmse: 0.000434155\ttraining's RMSPE: 0.2011\tvalid_1's rmse: 0.000448326\tvalid_1's RMSPE: 0.2068\n",
      "[600]\ttraining's rmse: 0.000428413\ttraining's RMSPE: 0.19844\tvalid_1's rmse: 0.000444036\tvalid_1's RMSPE: 0.20482\n",
      "[700]\ttraining's rmse: 0.000423334\ttraining's RMSPE: 0.19609\tvalid_1's rmse: 0.000440158\tvalid_1's RMSPE: 0.20303\n",
      "[800]\ttraining's rmse: 0.000418808\ttraining's RMSPE: 0.19399\tvalid_1's rmse: 0.000436913\tvalid_1's RMSPE: 0.20153\n",
      "[900]\ttraining's rmse: 0.000414964\ttraining's RMSPE: 0.19221\tvalid_1's rmse: 0.000434304\tvalid_1's RMSPE: 0.20033\n",
      "[1000]\ttraining's rmse: 0.000411226\ttraining's RMSPE: 0.19048\tvalid_1's rmse: 0.000431606\tvalid_1's RMSPE: 0.19909\n",
      "[1100]\ttraining's rmse: 0.000408055\ttraining's RMSPE: 0.18901\tvalid_1's rmse: 0.000429499\tvalid_1's RMSPE: 0.19811\n",
      "[1200]\ttraining's rmse: 0.000405095\ttraining's RMSPE: 0.18764\tvalid_1's rmse: 0.0004276\tvalid_1's RMSPE: 0.19724\n",
      "[1300]\ttraining's rmse: 0.000402379\ttraining's RMSPE: 0.18638\tvalid_1's rmse: 0.000425969\tvalid_1's RMSPE: 0.19649\n",
      "[1400]\ttraining's rmse: 0.000399736\ttraining's RMSPE: 0.18516\tvalid_1's rmse: 0.00042434\tvalid_1's RMSPE: 0.19573\n",
      "[1500]\ttraining's rmse: 0.000397354\ttraining's RMSPE: 0.18405\tvalid_1's rmse: 0.000423039\tvalid_1's RMSPE: 0.19513\n",
      "[1600]\ttraining's rmse: 0.000395013\ttraining's RMSPE: 0.18297\tvalid_1's rmse: 0.000421622\tvalid_1's RMSPE: 0.19448\n",
      "[1700]\ttraining's rmse: 0.00039289\ttraining's RMSPE: 0.18199\tvalid_1's rmse: 0.000420492\tvalid_1's RMSPE: 0.19396\n",
      "[1800]\ttraining's rmse: 0.000390896\ttraining's RMSPE: 0.18106\tvalid_1's rmse: 0.000419578\tvalid_1's RMSPE: 0.19354\n",
      "[1900]\ttraining's rmse: 0.000389058\ttraining's RMSPE: 0.18021\tvalid_1's rmse: 0.000418849\tvalid_1's RMSPE: 0.1932\n",
      "[2000]\ttraining's rmse: 0.000387206\ttraining's RMSPE: 0.17935\tvalid_1's rmse: 0.000417954\tvalid_1's RMSPE: 0.19279\n",
      "[2100]\ttraining's rmse: 0.000385459\ttraining's RMSPE: 0.17854\tvalid_1's rmse: 0.0004171\tvalid_1's RMSPE: 0.19239\n",
      "[2200]\ttraining's rmse: 0.000383764\ttraining's RMSPE: 0.17776\tvalid_1's rmse: 0.000416339\tvalid_1's RMSPE: 0.19204\n",
      "[2300]\ttraining's rmse: 0.000382075\ttraining's RMSPE: 0.17698\tvalid_1's rmse: 0.00041561\tvalid_1's RMSPE: 0.19171\n",
      "[2400]\ttraining's rmse: 0.000380513\ttraining's RMSPE: 0.17625\tvalid_1's rmse: 0.000414959\tvalid_1's RMSPE: 0.19141\n",
      "[2500]\ttraining's rmse: 0.000379029\ttraining's RMSPE: 0.17557\tvalid_1's rmse: 0.000414582\tvalid_1's RMSPE: 0.19123\n",
      "[2600]\ttraining's rmse: 0.000377565\ttraining's RMSPE: 0.17489\tvalid_1's rmse: 0.000413994\tvalid_1's RMSPE: 0.19096\n",
      "[2700]\ttraining's rmse: 0.000376129\ttraining's RMSPE: 0.17422\tvalid_1's rmse: 0.000413557\tvalid_1's RMSPE: 0.19076\n",
      "[2800]\ttraining's rmse: 0.000374795\ttraining's RMSPE: 0.1736\tvalid_1's rmse: 0.000413142\tvalid_1's RMSPE: 0.19057\n",
      "[2900]\ttraining's rmse: 0.00037344\ttraining's RMSPE: 0.17298\tvalid_1's rmse: 0.000412659\tvalid_1's RMSPE: 0.19035\n",
      "[3000]\ttraining's rmse: 0.00037215\ttraining's RMSPE: 0.17238\tvalid_1's rmse: 0.000412272\tvalid_1's RMSPE: 0.19017\n",
      "[3100]\ttraining's rmse: 0.000370858\ttraining's RMSPE: 0.17178\tvalid_1's rmse: 0.000411825\tvalid_1's RMSPE: 0.18996\n",
      "[3200]\ttraining's rmse: 0.000369654\ttraining's RMSPE: 0.17122\tvalid_1's rmse: 0.000411607\tvalid_1's RMSPE: 0.18986\n",
      "[3300]\ttraining's rmse: 0.000368401\ttraining's RMSPE: 0.17064\tvalid_1's rmse: 0.00041118\tvalid_1's RMSPE: 0.18966\n",
      "[3400]\ttraining's rmse: 0.000367244\ttraining's RMSPE: 0.17011\tvalid_1's rmse: 0.000411035\tvalid_1's RMSPE: 0.1896\n",
      "[3500]\ttraining's rmse: 0.000366129\ttraining's RMSPE: 0.16959\tvalid_1's rmse: 0.000410907\tvalid_1's RMSPE: 0.18954\n",
      "[3600]\ttraining's rmse: 0.000364987\ttraining's RMSPE: 0.16906\tvalid_1's rmse: 0.000410582\tvalid_1's RMSPE: 0.18939\n",
      "[3700]\ttraining's rmse: 0.000363867\ttraining's RMSPE: 0.16854\tvalid_1's rmse: 0.000410267\tvalid_1's RMSPE: 0.18924\n",
      "[3800]\ttraining's rmse: 0.000362768\ttraining's RMSPE: 0.16803\tvalid_1's rmse: 0.000409979\tvalid_1's RMSPE: 0.18911\n",
      "[3900]\ttraining's rmse: 0.000361729\ttraining's RMSPE: 0.16755\tvalid_1's rmse: 0.000409788\tvalid_1's RMSPE: 0.18902\n",
      "[4000]\ttraining's rmse: 0.000360681\ttraining's RMSPE: 0.16707\tvalid_1's rmse: 0.000409557\tvalid_1's RMSPE: 0.18891\n",
      "Early stopping, best iteration is:\n",
      "[3987]\ttraining's rmse: 0.000360806\ttraining's RMSPE: 0.16712\tvalid_1's rmse: 0.000409556\tvalid_1's RMSPE: 0.18891\n",
      "Fold 5\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015653 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 47298\n",
      "[LightGBM] [Info] Number of data points in the train set: 343146, number of used features: 215\n",
      "[LightGBM] [Info] Start training from score 0.001800\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's rmse: 0.000513636\ttraining's RMSPE: 0.23781\tvalid_1's rmse: 0.000518189\tvalid_1's RMSPE: 0.23943\n",
      "[200]\ttraining's rmse: 0.000463252\ttraining's RMSPE: 0.21449\tvalid_1's rmse: 0.000470954\tvalid_1's RMSPE: 0.21761\n",
      "[300]\ttraining's rmse: 0.000450068\ttraining's RMSPE: 0.20838\tvalid_1's rmse: 0.000460729\tvalid_1's RMSPE: 0.21288\n",
      "[400]\ttraining's rmse: 0.000441518\ttraining's RMSPE: 0.20442\tvalid_1's rmse: 0.000454948\tvalid_1's RMSPE: 0.21021\n",
      "[500]\ttraining's rmse: 0.000434745\ttraining's RMSPE: 0.20129\tvalid_1's rmse: 0.000450622\tvalid_1's RMSPE: 0.20821\n",
      "[600]\ttraining's rmse: 0.000428948\ttraining's RMSPE: 0.1986\tvalid_1's rmse: 0.000446443\tvalid_1's RMSPE: 0.20628\n",
      "[700]\ttraining's rmse: 0.000423628\ttraining's RMSPE: 0.19614\tvalid_1's rmse: 0.000443142\tvalid_1's RMSPE: 0.20476\n",
      "[800]\ttraining's rmse: 0.000419259\ttraining's RMSPE: 0.19412\tvalid_1's rmse: 0.000440546\tvalid_1's RMSPE: 0.20356\n",
      "[900]\ttraining's rmse: 0.000415276\ttraining's RMSPE: 0.19227\tvalid_1's rmse: 0.000438129\tvalid_1's RMSPE: 0.20244\n",
      "[1000]\ttraining's rmse: 0.000411724\ttraining's RMSPE: 0.19063\tvalid_1's rmse: 0.000436221\tvalid_1's RMSPE: 0.20156\n",
      "[1100]\ttraining's rmse: 0.00040846\ttraining's RMSPE: 0.18912\tvalid_1's rmse: 0.000434383\tvalid_1's RMSPE: 0.20071\n",
      "[1200]\ttraining's rmse: 0.000405187\ttraining's RMSPE: 0.1876\tvalid_1's rmse: 0.000432541\tvalid_1's RMSPE: 0.19986\n",
      "[1300]\ttraining's rmse: 0.000402605\ttraining's RMSPE: 0.18641\tvalid_1's rmse: 0.000431409\tvalid_1's RMSPE: 0.19934\n",
      "[1400]\ttraining's rmse: 0.00039991\ttraining's RMSPE: 0.18516\tvalid_1's rmse: 0.000430151\tvalid_1's RMSPE: 0.19875\n",
      "[1500]\ttraining's rmse: 0.000397531\ttraining's RMSPE: 0.18406\tvalid_1's rmse: 0.000429324\tvalid_1's RMSPE: 0.19837\n",
      "[1600]\ttraining's rmse: 0.00039521\ttraining's RMSPE: 0.18298\tvalid_1's rmse: 0.000428042\tvalid_1's RMSPE: 0.19778\n",
      "[1700]\ttraining's rmse: 0.000393052\ttraining's RMSPE: 0.18198\tvalid_1's rmse: 0.000427233\tvalid_1's RMSPE: 0.19741\n",
      "[1800]\ttraining's rmse: 0.000390885\ttraining's RMSPE: 0.18098\tvalid_1's rmse: 0.000426149\tvalid_1's RMSPE: 0.19691\n",
      "[1900]\ttraining's rmse: 0.000388909\ttraining's RMSPE: 0.18007\tvalid_1's rmse: 0.000425492\tvalid_1's RMSPE: 0.1966\n",
      "[2000]\ttraining's rmse: 0.00038721\ttraining's RMSPE: 0.17928\tvalid_1's rmse: 0.000425267\tvalid_1's RMSPE: 0.1965\n",
      "[2100]\ttraining's rmse: 0.000385433\ttraining's RMSPE: 0.17846\tvalid_1's rmse: 0.000424634\tvalid_1's RMSPE: 0.1962\n",
      "[2200]\ttraining's rmse: 0.00038365\ttraining's RMSPE: 0.17763\tvalid_1's rmse: 0.000424101\tvalid_1's RMSPE: 0.19596\n",
      "[2300]\ttraining's rmse: 0.000382058\ttraining's RMSPE: 0.17689\tvalid_1's rmse: 0.00042359\tvalid_1's RMSPE: 0.19572\n",
      "[2400]\ttraining's rmse: 0.000380473\ttraining's RMSPE: 0.17616\tvalid_1's rmse: 0.000423193\tvalid_1's RMSPE: 0.19554\n",
      "[2500]\ttraining's rmse: 0.000378961\ttraining's RMSPE: 0.17546\tvalid_1's rmse: 0.000422685\tvalid_1's RMSPE: 0.1953\n",
      "[2600]\ttraining's rmse: 0.000377426\ttraining's RMSPE: 0.17475\tvalid_1's rmse: 0.00042228\tvalid_1's RMSPE: 0.19512\n",
      "[2700]\ttraining's rmse: 0.000376055\ttraining's RMSPE: 0.17411\tvalid_1's rmse: 0.000422\tvalid_1's RMSPE: 0.19499\n",
      "[2800]\ttraining's rmse: 0.000374713\ttraining's RMSPE: 0.17349\tvalid_1's rmse: 0.000421752\tvalid_1's RMSPE: 0.19487\n",
      "Early stopping, best iteration is:\n",
      "[2760]\ttraining's rmse: 0.000375205\ttraining's RMSPE: 0.17372\tvalid_1's rmse: 0.000421666\tvalid_1's RMSPE: 0.19483\n",
      "Average RMSE: 0.19525\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"objective\": \"rmse\",\n",
    "    \"metric\": \"rmse\",\n",
    "   \n",
    "    \"boosting_type\": \"gbdt\",\n",
    "  #   'min_data_in_leaf':500,\n",
    "    'max_depth': -1,\n",
    "    'subsample_freq': 4,\n",
    " \n",
    "    'feature_fraction': 0.3,\n",
    "    'bagging_fraction': 0.75,\n",
    "    'learning_rate': 0.02,\n",
    "\n",
    "    'lambda_l1': 0.5,\n",
    "    'lambda_l2': 1.0,\n",
    "\n",
    "}\n",
    "def train_and_pred(train,test,params,n_splits = 5):\n",
    "    feature_importances = pd.DataFrame()\n",
    "    fold_scores=[]\n",
    "    \n",
    "    X = train.drop(['target'],axis=1)\n",
    "    y = train['target']\n",
    "    \n",
    "    kfold = KFold(n_splits , random_state = 42, shuffle = True)\n",
    "    test_pred= np.zeros(test.shape[0])\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X, y)):\n",
    "        print(f\"Fold {fold+1}\")\n",
    "        \n",
    "        X_train, y_train = X.loc[train_idx], y[train_idx]\n",
    "        X_val, y_val = X.loc[val_idx], y[val_idx]\n",
    "\n",
    "        weights_train = 1 / np.square(y_train.values)\n",
    "        weights_val = 1 / np.square(y_val.values)\n",
    "\n",
    "        X_train = X_train.drop(['row_id','time_id'],axis=1)\n",
    "        X_val = X_val.drop(['row_id','time_id'],axis=1)\n",
    "        features = X_train.columns.tolist()\n",
    "        lgb_train = lgb.Dataset(X_train, label=y_train, weight=weights_train,\n",
    "                                categorical_feature=['stock_id'])\n",
    "        lgb_val = lgb.Dataset(X_val, label=y_val, weight=weights_val,\n",
    "                            reference=lgb_train)\n",
    "        model = lgb.train(\n",
    "            params=params,\n",
    "            train_set=lgb_train,\n",
    "            num_boost_round=5000,\n",
    "            feval=rmspe_feval,\n",
    "            valid_sets=[lgb_train, lgb_val],\n",
    "            callbacks=[lgb.early_stopping(\n",
    "                stopping_rounds=50), lgb.log_evaluation(100)],\n",
    "\n",
    "        )  \n",
    "\n",
    "        y_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "   \n",
    "        test_pred += model.predict(test[features]) / 5\n",
    "        rmse = rmspe(y_true=y_val, y_pred=y_pred)\n",
    "        \n",
    "\n",
    "        fold_scores.append(round(rmse,5))\n",
    "\n",
    "        fold_importance = pd.DataFrame()\n",
    "        fold_importance[\"feature\"] = X_train.columns\n",
    "        fold_importance[\"importance\"] = model.feature_importance(\n",
    "            importance_type='split')\n",
    "        fold_importance[\"fold\"] = fold + 1\n",
    "\n",
    "        feature_importances = pd.concat(\n",
    "            [feature_importances, fold_importance], axis=0)\n",
    "\n",
    "    print(f\"Average RMSE: {np.mean(fold_scores):.5f}\")\n",
    "    return feature_importances, test_pred\n",
    "\n",
    "feature_importances,test_pred =  train_and_pred(train,test,params,n_splits = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d259c0",
   "metadata": {
    "papermill": {
     "duration": 0.024331,
     "end_time": "2025-09-06T20:04:21.038903",
     "exception": false,
     "start_time": "2025-09-06T20:04:21.014572",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.2 Neural Network\n",
    "### 2.2.0 Setup for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5f0a4fb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T20:04:21.205742Z",
     "iopub.status.busy": "2025-09-06T20:04:21.204518Z",
     "iopub.status.idle": "2025-09-06T20:04:40.469893Z",
     "shell.execute_reply": "2025-09-06T20:04:40.468834Z"
    },
    "papermill": {
     "duration": 19.294234,
     "end_time": "2025-09-06T20:04:40.471742",
     "exception": false,
     "start_time": "2025-09-06T20:04:21.177508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Keras version of RMSPE loss function\n",
    "def rmse_keras(y_true, y_pred):\n",
    "    \"\"\"RMSPE loss function for Keras models\"\"\"\n",
    "    return K.sqrt(K.mean(K.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "# Early stopping callback - prevent overfitting\n",
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=20, verbose=0,\n",
    "    mode='min', restore_best_weights=True)\n",
    "\n",
    "# Learning rate reduction callback - improve convergence\n",
    "plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.2, patience=7, verbose=0,\n",
    "    mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aca580cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T20:04:40.525952Z",
     "iopub.status.busy": "2025-09-06T20:04:40.525048Z",
     "iopub.status.idle": "2025-09-06T20:04:40.530884Z",
     "shell.execute_reply": "2025-09-06T20:04:40.529912Z"
    },
    "papermill": {
     "duration": 0.034602,
     "end_time": "2025-09-06T20:04:40.532411",
     "exception": false,
     "start_time": "2025-09-06T20:04:40.497809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "colNames = df_train.columns.tolist()\n",
    "colNames.remove('row_id')\n",
    "colNames.remove('target')\n",
    "colNames.remove('time_id')\n",
    "colNames.remove('stock_id')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4590ad23",
   "metadata": {},
   "source": [
    "### 2.2.1 Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "acca9c7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T20:04:40.584291Z",
     "iopub.status.busy": "2025-09-06T20:04:40.583954Z",
     "iopub.status.idle": "2025-09-06T20:05:03.393581Z",
     "shell.execute_reply": "2025-09-06T20:05:03.392515Z"
    },
    "papermill": {
     "duration": 22.838911,
     "end_time": "2025-09-06T20:05:03.395655",
     "exception": false,
     "start_time": "2025-09-06T20:04:40.556744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transform features to normal distribution for neural networks\n",
    "qt_train = []\n",
    "train_nn = train[colNames].copy()\n",
    "test_nn = test[colNames].copy()\n",
    "\n",
    "for col in colNames:\n",
    "    qt = QuantileTransformer(random_state=42, n_quantiles=2000, output_distribution='normal')\n",
    "    train_nn[col] = qt.fit_transform(train_nn[[col]])\n",
    "    test_nn[col] = qt.transform(test_nn[[col]])\n",
    "    qt_train.append(qt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "da7d9f0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T20:05:03.448033Z",
     "iopub.status.busy": "2025-09-06T20:05:03.447081Z",
     "iopub.status.idle": "2025-09-06T20:05:04.818603Z",
     "shell.execute_reply": "2025-09-06T20:05:04.817574Z"
    },
    "papermill": {
     "duration": 1.399362,
     "end_time": "2025-09-06T20:05:04.820377",
     "exception": false,
     "start_time": "2025-09-06T20:05:03.421015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5q/z83yd5r96290nk8203y3gnkr0000gn/T/ipykernel_95708/476974002.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_nn[['stock_id', 'time_id', 'target']] = df_train[['stock_id', 'time_id', 'target']]\n",
      "/var/folders/5q/z83yd5r96290nk8203y3gnkr0000gn/T/ipykernel_95708/476974002.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_nn[['stock_id', 'time_id', 'target']] = df_train[['stock_id', 'time_id', 'target']]\n",
      "/var/folders/5q/z83yd5r96290nk8203y3gnkr0000gn/T/ipykernel_95708/476974002.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_nn[['stock_id', 'time_id', 'target']] = df_train[['stock_id', 'time_id', 'target']]\n",
      "/var/folders/5q/z83yd5r96290nk8203y3gnkr0000gn/T/ipykernel_95708/476974002.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_nn[['stock_id', 'time_id']] = df_test[['stock_id', 'time_id']]\n",
      "/var/folders/5q/z83yd5r96290nk8203y3gnkr0000gn/T/ipykernel_95708/476974002.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_nn[['stock_id', 'time_id']] = df_test[['stock_id', 'time_id']]\n"
     ]
    }
   ],
   "source": [
    "# Add identifiers and targets to transformed data\n",
    "train_nn[['stock_id', 'time_id', 'target']] = df_train[['stock_id', 'time_id', 'target']]\n",
    "\n",
    "# Generate cluster features\n",
    "train_cluster = generate_cluster_features(df_train, cluster_stocks).reset_index()\n",
    "test_cluster = generate_cluster_features(df_test, cluster_stocks).reset_index()\n",
    "test_cluster['time_id'] = test_cluster['time_id'].astype(int)\n",
    "\n",
    "test_nn[['stock_id', 'time_id']] = df_test[['stock_id', 'time_id']]\n",
    "\n",
    "# Merge cluster features with transformed data\n",
    "train_nn = pd.merge(train_nn, train_cluster, how='left', on='time_id')\n",
    "test_nn = pd.merge(test_nn, test_cluster, how='left', on='time_id')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5267f064",
   "metadata": {},
   "source": [
    "### 2.2.2 Model Defining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "83d7acc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T20:05:04.952659Z",
     "iopub.status.busy": "2025-09-06T20:05:04.952305Z",
     "iopub.status.idle": "2025-09-06T20:05:04.967898Z",
     "shell.execute_reply": "2025-09-06T20:05:04.966900Z"
    },
    "papermill": {
     "duration": 0.043457,
     "end_time": "2025-09-06T20:05:04.969815",
     "exception": false,
     "start_time": "2025-09-06T20:05:04.926358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def swish(x, beta = 1):\n",
    "    return (x * K.sigmoid(beta * x))\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "\n",
    "get_custom_objects().update({'swish': Activation(swish)})\n",
    "\n",
    "hidden_units = (128,64,32)\n",
    "stock_embedding_size = 24\n",
    "cat_data = train_nn['stock_id']\n",
    "\n",
    "def base_model():\n",
    "    \n",
    "    # Each instance will consist of two inputs: a single user id, and a single movie id\n",
    "    stock_id_input = keras.Input(shape=(1,), name='stock_id')\n",
    "    num_input = keras.Input(shape=(215,), name='num_data')\n",
    "\n",
    "\n",
    "    #embedding, flatenning and concatenating\n",
    "    stock_embedded = keras.layers.Embedding(max(cat_data)+1, stock_embedding_size, \n",
    "                                           input_length=1, name='stock_embedding')(stock_id_input)\n",
    "    stock_flattened = keras.layers.Flatten()(stock_embedded)\n",
    "    out = keras.layers.Concatenate()([stock_flattened, num_input])\n",
    "    \n",
    "    # Add one or more hidden layers\n",
    "    for n_hidden in hidden_units:\n",
    "\n",
    "        out = keras.layers.Dense(n_hidden, activation='swish')(out)\n",
    "        \n",
    "    # A single output: our predicted rating\n",
    "    out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n",
    "    \n",
    "    model = keras.Model(\n",
    "    inputs = [stock_id_input, num_input],\n",
    "    outputs = out,\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186bfb17",
   "metadata": {},
   "source": [
    "### 2.2.3 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0431153b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T20:05:05.311955Z",
     "iopub.status.busy": "2025-09-06T20:05:05.311613Z",
     "iopub.status.idle": "2025-09-06T20:31:12.422188Z",
     "shell.execute_reply": "2025-09-06T20:31:12.420568Z"
    },
    "papermill": {
     "duration": 1567.140971,
     "end_time": "2025-09-06T20:31:12.424192",
     "exception": false,
     "start_time": "2025-09-06T20:05:05.283221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5q/z83yd5r96290nk8203y3gnkr0000gn/T/ipykernel_95708/3275547514.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_nn[target_name] = 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 13.5426 - val_loss: 2.9000 - lr: 0.0060\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 4.3709 - val_loss: 2.7244 - lr: 0.0060\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 3.3323 - val_loss: 3.9513 - lr: 0.0060\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 1.7401 - val_loss: 1.2074 - lr: 0.0060\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.9316 - val_loss: 0.8067 - lr: 0.0060\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6695 - val_loss: 0.4640 - lr: 0.0060\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6091 - val_loss: 0.6370 - lr: 0.0060\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5885 - val_loss: 0.7519 - lr: 0.0060\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6097 - val_loss: 0.7964 - lr: 0.0060\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.5401 - val_loss: 0.4282 - lr: 0.0060\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 1.0363 - val_loss: 0.5477 - lr: 0.0060\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5125 - val_loss: 0.7307 - lr: 0.0060\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5528 - val_loss: 0.5737 - lr: 0.0060\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.5333 - val_loss: 0.3551 - lr: 0.0060\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5233 - val_loss: 0.5952 - lr: 0.0060\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.8398 - val_loss: 0.6239 - lr: 0.0060\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.4870 - val_loss: 0.5806 - lr: 0.0060\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.4769 - val_loss: 0.5178 - lr: 0.0060\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.4672 - val_loss: 0.5598 - lr: 0.0060\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.4507 - val_loss: 0.4951 - lr: 0.0060\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 1.4368 - val_loss: 1.7731 - lr: 0.0060\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3008 - val_loss: 0.2340 - lr: 0.0012\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2275 - val_loss: 0.2417 - lr: 0.0012\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2257 - val_loss: 0.2376 - lr: 0.0012\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2234 - val_loss: 0.2296 - lr: 0.0012\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2223 - val_loss: 0.2316 - lr: 0.0012\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2197 - val_loss: 0.2252 - lr: 0.0012\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2199 - val_loss: 0.2260 - lr: 0.0012\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2205 - val_loss: 0.2293 - lr: 0.0012\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2186 - val_loss: 0.2292 - lr: 0.0012\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2198 - val_loss: 0.2249 - lr: 0.0012\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2177 - val_loss: 0.2216 - lr: 0.0012\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2174 - val_loss: 0.2222 - lr: 0.0012\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2163 - val_loss: 0.2229 - lr: 0.0012\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2179 - val_loss: 0.2221 - lr: 0.0012\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2166 - val_loss: 0.2229 - lr: 0.0012\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2169 - val_loss: 0.2321 - lr: 0.0012\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2192 - val_loss: 0.2286 - lr: 0.0012\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2195 - val_loss: 0.2387 - lr: 0.0012\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2103 - val_loss: 0.2177 - lr: 2.4000e-04\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2097 - val_loss: 0.2176 - lr: 2.4000e-04\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2094 - val_loss: 0.2168 - lr: 2.4000e-04\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2093 - val_loss: 0.2189 - lr: 2.4000e-04\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2094 - val_loss: 0.2193 - lr: 2.4000e-04\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2095 - val_loss: 0.2168 - lr: 2.4000e-04\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2101 - val_loss: 0.2222 - lr: 2.4000e-04\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2095 - val_loss: 0.2169 - lr: 2.4000e-04\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2093 - val_loss: 0.2177 - lr: 2.4000e-04\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2093 - val_loss: 0.2180 - lr: 2.4000e-04\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2078 - val_loss: 0.2162 - lr: 4.8000e-05\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2077 - val_loss: 0.2167 - lr: 4.8000e-05\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2076 - val_loss: 0.2172 - lr: 4.8000e-05\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2077 - val_loss: 0.2175 - lr: 4.8000e-05\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2075 - val_loss: 0.2166 - lr: 4.8000e-05\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2075 - val_loss: 0.2164 - lr: 4.8000e-05\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2075 - val_loss: 0.2171 - lr: 4.8000e-05\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2075 - val_loss: 0.2170 - lr: 4.8000e-05\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2070 - val_loss: 0.2166 - lr: 9.6000e-06\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2070 - val_loss: 0.2165 - lr: 9.6000e-06\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2070 - val_loss: 0.2166 - lr: 9.6000e-06\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2070 - val_loss: 0.2166 - lr: 9.6000e-06\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2070 - val_loss: 0.2165 - lr: 9.6000e-06\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2069 - val_loss: 0.2164 - lr: 9.6000e-06\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2070 - val_loss: 0.2168 - lr: 9.6000e-06\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2068 - val_loss: 0.2166 - lr: 1.9200e-06\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2068 - val_loss: 0.2166 - lr: 1.9200e-06\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2068 - val_loss: 0.2165 - lr: 1.9200e-06\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2068 - val_loss: 0.2164 - lr: 1.9200e-06\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2068 - val_loss: 0.2166 - lr: 1.9200e-06\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2068 - val_loss: 0.2166 - lr: 1.9200e-06\n",
      "2681/2681 [==============================] - 1s 334us/step\n",
      "Fold 1 NN: 0.21619\n",
      "[0.00162672 0.00535995 0.00272644]\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "[0.00036883 0.00208681 0.00208681]\n",
      "CV 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 15.4434 - val_loss: 5.0384 - lr: 0.0060\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 3.1446 - val_loss: 2.6721 - lr: 0.0060\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 1.5033 - val_loss: 1.9154 - lr: 0.0060\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 1.3069 - val_loss: 1.0587 - lr: 0.0060\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 1.0064 - val_loss: 1.0172 - lr: 0.0060\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.8915 - val_loss: 0.5510 - lr: 0.0060\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 1.1072 - val_loss: 1.0693 - lr: 0.0060\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6910 - val_loss: 0.6166 - lr: 0.0060\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6042 - val_loss: 0.7840 - lr: 0.0060\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.5840 - val_loss: 0.5106 - lr: 0.0060\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5367 - val_loss: 0.3533 - lr: 0.0060\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5312 - val_loss: 0.5158 - lr: 0.0060\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.4155 - val_loss: 0.3300 - lr: 0.0060\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2841 - val_loss: 0.2664 - lr: 0.0060\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.7590 - val_loss: 1.0224 - lr: 0.0060\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.6914 - val_loss: 0.5140 - lr: 0.0060\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5056 - val_loss: 0.5579 - lr: 0.0060\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.4761 - val_loss: 0.5208 - lr: 0.0060\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.4499 - val_loss: 0.4787 - lr: 0.0060\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.4408 - val_loss: 0.5800 - lr: 0.0060\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.4282 - val_loss: 0.5688 - lr: 0.0060\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2312 - val_loss: 0.2308 - lr: 0.0012\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2139 - val_loss: 0.2295 - lr: 0.0012\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2143 - val_loss: 0.2278 - lr: 0.0012\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2143 - val_loss: 0.2313 - lr: 0.0012\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2140 - val_loss: 0.2336 - lr: 0.0012\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2125 - val_loss: 0.2304 - lr: 0.0012\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2124 - val_loss: 0.2315 - lr: 0.0012\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2113 - val_loss: 0.2272 - lr: 0.0012\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2127 - val_loss: 0.2256 - lr: 0.0012\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2123 - val_loss: 0.2291 - lr: 0.0012\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2128 - val_loss: 0.2274 - lr: 0.0012\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2138 - val_loss: 0.2265 - lr: 0.0012\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2124 - val_loss: 0.2310 - lr: 0.0012\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2115 - val_loss: 0.2294 - lr: 0.0012\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2117 - val_loss: 0.2297 - lr: 0.0012\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2150 - val_loss: 0.2469 - lr: 0.0012\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2082 - val_loss: 0.2256 - lr: 2.4000e-04\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2071 - val_loss: 0.2248 - lr: 2.4000e-04\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2068 - val_loss: 0.2239 - lr: 2.4000e-04\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2066 - val_loss: 0.2235 - lr: 2.4000e-04\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2067 - val_loss: 0.2244 - lr: 2.4000e-04\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2066 - val_loss: 0.2246 - lr: 2.4000e-04\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2066 - val_loss: 0.2243 - lr: 2.4000e-04\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2063 - val_loss: 0.2251 - lr: 2.4000e-04\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2068 - val_loss: 0.2244 - lr: 2.4000e-04\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2069 - val_loss: 0.2248 - lr: 2.4000e-04\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2065 - val_loss: 0.2272 - lr: 2.4000e-04\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2053 - val_loss: 0.2232 - lr: 4.8000e-05\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2051 - val_loss: 0.2237 - lr: 4.8000e-05\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2051 - val_loss: 0.2237 - lr: 4.8000e-05\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2050 - val_loss: 0.2234 - lr: 4.8000e-05\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2050 - val_loss: 0.2230 - lr: 4.8000e-05\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2050 - val_loss: 0.2243 - lr: 4.8000e-05\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2050 - val_loss: 0.2229 - lr: 4.8000e-05\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2050 - val_loss: 0.2240 - lr: 4.8000e-05\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2050 - val_loss: 0.2235 - lr: 4.8000e-05\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2049 - val_loss: 0.2242 - lr: 4.8000e-05\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2049 - val_loss: 0.2230 - lr: 4.8000e-05\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2049 - val_loss: 0.2229 - lr: 4.8000e-05\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2049 - val_loss: 0.2236 - lr: 4.8000e-05\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2048 - val_loss: 0.2243 - lr: 4.8000e-05\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2043 - val_loss: 0.2232 - lr: 9.6000e-06\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2042 - val_loss: 0.2231 - lr: 9.6000e-06\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2042 - val_loss: 0.2232 - lr: 9.6000e-06\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2042 - val_loss: 0.2230 - lr: 9.6000e-06\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2043 - val_loss: 0.2237 - lr: 9.6000e-06\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2042 - val_loss: 0.2230 - lr: 9.6000e-06\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2042 - val_loss: 0.2232 - lr: 9.6000e-06\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2040 - val_loss: 0.2231 - lr: 1.9200e-06\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2040 - val_loss: 0.2232 - lr: 1.9200e-06\n",
      "Epoch 72/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2040 - val_loss: 0.2230 - lr: 1.9200e-06\n",
      "Epoch 73/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2040 - val_loss: 0.2230 - lr: 1.9200e-06\n",
      "Epoch 74/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2040 - val_loss: 0.2230 - lr: 1.9200e-06\n",
      "Epoch 75/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2040 - val_loss: 0.2230 - lr: 1.9200e-06\n",
      "2681/2681 [==============================] - 1s 333us/step\n",
      "Fold 2 NN: 0.22287\n",
      "[0.00225759 0.00290113 0.00288826]\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "[0.00095733 0.00542167 0.00542167]\n",
      "CV 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 10.3680 - val_loss: 1.2481 - lr: 0.0060\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.8309 - val_loss: 0.6856 - lr: 0.0060\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.7817 - val_loss: 0.9971 - lr: 0.0060\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.7261 - val_loss: 0.7996 - lr: 0.0060\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.6452 - val_loss: 0.5964 - lr: 0.0060\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.6975 - val_loss: 0.7453 - lr: 0.0060\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.5531 - val_loss: 0.4715 - lr: 0.0060\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.4863 - val_loss: 0.4334 - lr: 0.0060\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.7969 - val_loss: 0.4465 - lr: 0.0060\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.4309 - val_loss: 0.4105 - lr: 0.0060\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.4129 - val_loss: 0.4121 - lr: 0.0060\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.4536 - val_loss: 0.2441 - lr: 0.0060\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2527 - val_loss: 0.2349 - lr: 0.0060\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2486 - val_loss: 0.2225 - lr: 0.0060\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2526 - val_loss: 0.2409 - lr: 0.0060\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2518 - val_loss: 0.2265 - lr: 0.0060\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2667 - val_loss: 0.2479 - lr: 0.0060\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2690 - val_loss: 0.2200 - lr: 0.0060\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2663 - val_loss: 0.2309 - lr: 0.0060\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2687 - val_loss: 0.2657 - lr: 0.0060\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2729 - val_loss: 0.2538 - lr: 0.0060\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2700 - val_loss: 0.3138 - lr: 0.0060\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2682 - val_loss: 0.2277 - lr: 0.0060\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.5395 - val_loss: 0.6429 - lr: 0.0060\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.9408 - val_loss: 0.2618 - lr: 0.0060\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2295 - val_loss: 0.2203 - lr: 0.0012\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2254 - val_loss: 0.2202 - lr: 0.0012\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2232 - val_loss: 0.2166 - lr: 0.0012\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2230 - val_loss: 0.2206 - lr: 0.0012\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2217 - val_loss: 0.2164 - lr: 0.0012\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2230 - val_loss: 0.2184 - lr: 0.0012\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2205 - val_loss: 0.2171 - lr: 0.0012\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2198 - val_loss: 0.2149 - lr: 0.0012\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2188 - val_loss: 0.2147 - lr: 0.0012\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2189 - val_loss: 0.2169 - lr: 0.0012\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2182 - val_loss: 0.2208 - lr: 0.0012\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2193 - val_loss: 0.2138 - lr: 0.0012\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2165 - val_loss: 0.2193 - lr: 0.0012\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2170 - val_loss: 0.2135 - lr: 0.0012\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2171 - val_loss: 0.2133 - lr: 0.0012\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2163 - val_loss: 0.2153 - lr: 0.0012\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2175 - val_loss: 0.2251 - lr: 0.0012\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2168 - val_loss: 0.2194 - lr: 0.0012\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2143 - val_loss: 0.2222 - lr: 0.0012\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2166 - val_loss: 0.2185 - lr: 0.0012\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2147 - val_loss: 0.2140 - lr: 0.0012\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2154 - val_loss: 0.2151 - lr: 0.0012\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2102 - val_loss: 0.2123 - lr: 2.4000e-04\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2097 - val_loss: 0.2118 - lr: 2.4000e-04\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2097 - val_loss: 0.2123 - lr: 2.4000e-04\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2096 - val_loss: 0.2122 - lr: 2.4000e-04\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2095 - val_loss: 0.2127 - lr: 2.4000e-04\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2100 - val_loss: 0.2123 - lr: 2.4000e-04\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2094 - val_loss: 0.2122 - lr: 2.4000e-04\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2099 - val_loss: 0.2130 - lr: 2.4000e-04\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2097 - val_loss: 0.2116 - lr: 2.4000e-04\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2094 - val_loss: 0.2134 - lr: 2.4000e-04\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2091 - val_loss: 0.2127 - lr: 2.4000e-04\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2090 - val_loss: 0.2125 - lr: 2.4000e-04\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2088 - val_loss: 0.2164 - lr: 2.4000e-04\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2094 - val_loss: 0.2137 - lr: 2.4000e-04\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2093 - val_loss: 0.2143 - lr: 2.4000e-04\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2085 - val_loss: 0.2125 - lr: 2.4000e-04\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2076 - val_loss: 0.2117 - lr: 4.8000e-05\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2074 - val_loss: 0.2117 - lr: 4.8000e-05\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2074 - val_loss: 0.2122 - lr: 4.8000e-05\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2074 - val_loss: 0.2115 - lr: 4.8000e-05\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2073 - val_loss: 0.2117 - lr: 4.8000e-05\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2074 - val_loss: 0.2115 - lr: 4.8000e-05\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2073 - val_loss: 0.2115 - lr: 4.8000e-05\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2073 - val_loss: 0.2116 - lr: 4.8000e-05\n",
      "Epoch 72/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2073 - val_loss: 0.2120 - lr: 4.8000e-05\n",
      "Epoch 73/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2074 - val_loss: 0.2117 - lr: 4.8000e-05\n",
      "Epoch 74/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2071 - val_loss: 0.2121 - lr: 4.8000e-05\n",
      "Epoch 75/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2069 - val_loss: 0.2114 - lr: 9.6000e-06\n",
      "Epoch 76/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2068 - val_loss: 0.2115 - lr: 9.6000e-06\n",
      "Epoch 77/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2068 - val_loss: 0.2116 - lr: 9.6000e-06\n",
      "Epoch 78/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2068 - val_loss: 0.2115 - lr: 9.6000e-06\n",
      "Epoch 79/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2068 - val_loss: 0.2115 - lr: 9.6000e-06\n",
      "Epoch 80/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2068 - val_loss: 0.2115 - lr: 9.6000e-06\n",
      "Epoch 81/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2068 - val_loss: 0.2116 - lr: 9.6000e-06\n",
      "Epoch 82/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2067 - val_loss: 0.2115 - lr: 1.9200e-06\n",
      "Epoch 83/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2067 - val_loss: 0.2114 - lr: 1.9200e-06\n",
      "Epoch 84/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2067 - val_loss: 0.2115 - lr: 1.9200e-06\n",
      "Epoch 85/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2067 - val_loss: 0.2114 - lr: 1.9200e-06\n",
      "Epoch 86/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2067 - val_loss: 0.2114 - lr: 1.9200e-06\n",
      "Epoch 87/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2067 - val_loss: 0.2115 - lr: 1.9200e-06\n",
      "Epoch 88/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2067 - val_loss: 0.2114 - lr: 1.9200e-06\n",
      "Epoch 89/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2066 - val_loss: 0.2114 - lr: 3.8400e-07\n",
      "Epoch 90/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2067 - val_loss: 0.2114 - lr: 3.8400e-07\n",
      "Epoch 91/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2067 - val_loss: 0.2114 - lr: 3.8400e-07\n",
      "Epoch 92/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2067 - val_loss: 0.2114 - lr: 3.8400e-07\n",
      "Epoch 93/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2067 - val_loss: 0.2115 - lr: 3.8400e-07\n",
      "Epoch 94/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2067 - val_loss: 0.2114 - lr: 3.8400e-07\n",
      "Epoch 95/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2066 - val_loss: 0.2115 - lr: 3.8400e-07\n",
      "Epoch 96/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2067 - val_loss: 0.2115 - lr: 7.6800e-08\n",
      "Epoch 97/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2066 - val_loss: 0.2115 - lr: 7.6800e-08\n",
      "Epoch 98/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2067 - val_loss: 0.2115 - lr: 7.6800e-08\n",
      "Epoch 99/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2067 - val_loss: 0.2115 - lr: 7.6800e-08\n",
      "Epoch 100/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2067 - val_loss: 0.2114 - lr: 7.6800e-08\n",
      "Epoch 101/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2067 - val_loss: 0.2114 - lr: 7.6800e-08\n",
      "Epoch 102/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2066 - val_loss: 0.2114 - lr: 7.6800e-08\n",
      "Epoch 103/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2066 - val_loss: 0.2115 - lr: 1.5360e-08\n",
      "Epoch 104/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2067 - val_loss: 0.2115 - lr: 1.5360e-08\n",
      "Epoch 105/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2067 - val_loss: 0.2114 - lr: 1.5360e-08\n",
      "2681/2681 [==============================] - 1s 331us/step\n",
      "Fold 3 NN: 0.21142\n",
      "[0.00155842 0.00389961 0.00291451]\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "[0.00146936 0.00728809 0.00728809]\n",
      "CV 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 10.6331 - val_loss: 3.8666 - lr: 0.0060\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 2.4842 - val_loss: 2.9921 - lr: 0.0060\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.8928 - val_loss: 1.0217 - lr: 0.0060\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.7250 - val_loss: 0.7678 - lr: 0.0060\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.7056 - val_loss: 0.9669 - lr: 0.0060\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6685 - val_loss: 0.6754 - lr: 0.0060\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.6867 - val_loss: 0.7438 - lr: 0.0060\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.6575 - val_loss: 0.5936 - lr: 0.0060\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6687 - val_loss: 0.6025 - lr: 0.0060\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.7359 - val_loss: 0.9945 - lr: 0.0060\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5841 - val_loss: 0.4436 - lr: 0.0060\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5437 - val_loss: 0.5852 - lr: 0.0060\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5568 - val_loss: 0.5979 - lr: 0.0060\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5213 - val_loss: 0.4407 - lr: 0.0060\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5298 - val_loss: 0.4300 - lr: 0.0060\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.4591 - val_loss: 0.4395 - lr: 0.0060\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.4516 - val_loss: 0.3636 - lr: 0.0060\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.4652 - val_loss: 0.3884 - lr: 0.0060\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.4416 - val_loss: 0.3754 - lr: 0.0060\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3703 - val_loss: 0.3626 - lr: 0.0060\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3733 - val_loss: 0.3315 - lr: 0.0060\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3582 - val_loss: 0.2705 - lr: 0.0060\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5896 - val_loss: 0.2276 - lr: 0.0060\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2322 - val_loss: 0.2221 - lr: 0.0060\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2260 - val_loss: 0.2244 - lr: 0.0060\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2259 - val_loss: 0.2233 - lr: 0.0060\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2332 - val_loss: 0.2306 - lr: 0.0060\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2402 - val_loss: 0.2208 - lr: 0.0060\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2273 - val_loss: 0.2382 - lr: 0.0060\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2304 - val_loss: 0.2252 - lr: 0.0060\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2382 - val_loss: 0.2483 - lr: 0.0060\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2514 - val_loss: 0.2267 - lr: 0.0060\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2581 - val_loss: 0.2283 - lr: 0.0060\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3667 - val_loss: 0.2190 - lr: 0.0060\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2399 - val_loss: 0.2198 - lr: 0.0060\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2300 - val_loss: 0.2202 - lr: 0.0060\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2390 - val_loss: 0.2321 - lr: 0.0060\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.4716 - val_loss: 0.4288 - lr: 0.0060\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5307 - val_loss: 0.2380 - lr: 0.0060\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2277 - val_loss: 0.2192 - lr: 0.0060\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2257 - val_loss: 0.2209 - lr: 0.0060\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2139 - val_loss: 0.2156 - lr: 0.0012\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2129 - val_loss: 0.2148 - lr: 0.0012\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2128 - val_loss: 0.2139 - lr: 0.0012\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2128 - val_loss: 0.2154 - lr: 0.0012\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2124 - val_loss: 0.2174 - lr: 0.0012\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2120 - val_loss: 0.2141 - lr: 0.0012\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2113 - val_loss: 0.2130 - lr: 0.0012\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2109 - val_loss: 0.2126 - lr: 0.0012\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2111 - val_loss: 0.2134 - lr: 0.0012\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2112 - val_loss: 0.2120 - lr: 0.0012\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2107 - val_loss: 0.2135 - lr: 0.0012\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2120 - val_loss: 0.2110 - lr: 0.0012\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2109 - val_loss: 0.2176 - lr: 0.0012\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2110 - val_loss: 0.2120 - lr: 0.0012\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2104 - val_loss: 0.2121 - lr: 0.0012\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2094 - val_loss: 0.2131 - lr: 0.0012\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2123 - val_loss: 0.2153 - lr: 0.0012\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2099 - val_loss: 0.2156 - lr: 0.0012\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2109 - val_loss: 0.2106 - lr: 0.0012\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2099 - val_loss: 0.2112 - lr: 0.0012\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2112 - val_loss: 0.2151 - lr: 0.0012\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2091 - val_loss: 0.2106 - lr: 0.0012\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2094 - val_loss: 0.2115 - lr: 0.0012\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2111 - val_loss: 0.2112 - lr: 0.0012\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2083 - val_loss: 0.2132 - lr: 0.0012\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2100 - val_loss: 0.2124 - lr: 0.0012\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2056 - val_loss: 0.2099 - lr: 2.4000e-04\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2053 - val_loss: 0.2095 - lr: 2.4000e-04\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2051 - val_loss: 0.2100 - lr: 2.4000e-04\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2051 - val_loss: 0.2096 - lr: 2.4000e-04\n",
      "Epoch 72/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2051 - val_loss: 0.2104 - lr: 2.4000e-04\n",
      "Epoch 73/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2050 - val_loss: 0.2129 - lr: 2.4000e-04\n",
      "Epoch 74/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2053 - val_loss: 0.2102 - lr: 2.4000e-04\n",
      "Epoch 75/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2048 - val_loss: 0.2097 - lr: 2.4000e-04\n",
      "Epoch 76/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2048 - val_loss: 0.2102 - lr: 2.4000e-04\n",
      "Epoch 77/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2041 - val_loss: 0.2094 - lr: 4.8000e-05\n",
      "Epoch 78/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2041 - val_loss: 0.2094 - lr: 4.8000e-05\n",
      "Epoch 79/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2040 - val_loss: 0.2094 - lr: 4.8000e-05\n",
      "Epoch 80/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2040 - val_loss: 0.2096 - lr: 4.8000e-05\n",
      "Epoch 81/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2040 - val_loss: 0.2096 - lr: 4.8000e-05\n",
      "Epoch 82/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2040 - val_loss: 0.2095 - lr: 4.8000e-05\n",
      "Epoch 83/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2040 - val_loss: 0.2095 - lr: 4.8000e-05\n",
      "Epoch 84/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2039 - val_loss: 0.2094 - lr: 4.8000e-05\n",
      "Epoch 85/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2038 - val_loss: 0.2093 - lr: 9.6000e-06\n",
      "Epoch 86/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2037 - val_loss: 0.2093 - lr: 9.6000e-06\n",
      "Epoch 87/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2037 - val_loss: 0.2095 - lr: 9.6000e-06\n",
      "Epoch 88/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2038 - val_loss: 0.2094 - lr: 9.6000e-06\n",
      "Epoch 89/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2037 - val_loss: 0.2093 - lr: 9.6000e-06\n",
      "Epoch 90/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2037 - val_loss: 0.2094 - lr: 9.6000e-06\n",
      "Epoch 91/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2037 - val_loss: 0.2094 - lr: 9.6000e-06\n",
      "Epoch 92/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2037 - val_loss: 0.2094 - lr: 9.6000e-06\n",
      "Epoch 93/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2037 - val_loss: 0.2094 - lr: 1.9200e-06\n",
      "Epoch 94/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2037 - val_loss: 0.2093 - lr: 1.9200e-06\n",
      "Epoch 95/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2036 - val_loss: 0.2093 - lr: 1.9200e-06\n",
      "Epoch 96/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2036 - val_loss: 0.2094 - lr: 1.9200e-06\n",
      "Epoch 97/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2036 - val_loss: 0.2093 - lr: 1.9200e-06\n",
      "Epoch 98/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2036 - val_loss: 0.2094 - lr: 1.9200e-06\n",
      "Epoch 99/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2036 - val_loss: 0.2094 - lr: 1.9200e-06\n",
      "Epoch 100/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2036 - val_loss: 0.2093 - lr: 3.8400e-07\n",
      "Epoch 101/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2036 - val_loss: 0.2094 - lr: 3.8400e-07\n",
      "Epoch 102/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2036 - val_loss: 0.2093 - lr: 3.8400e-07\n",
      "Epoch 103/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2036 - val_loss: 0.2094 - lr: 3.8400e-07\n",
      "Epoch 104/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2036 - val_loss: 0.2094 - lr: 3.8400e-07\n",
      "Epoch 105/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2036 - val_loss: 0.2094 - lr: 3.8400e-07\n",
      "Epoch 106/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2036 - val_loss: 0.2094 - lr: 3.8400e-07\n",
      "2681/2681 [==============================] - 1s 333us/step\n",
      "Fold 4 NN: 0.20927\n",
      "[0.00216092 0.00787347 0.00241393]\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "[0.00197644 0.008819   0.008819  ]\n",
      "CV 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 12.8093 - val_loss: 0.6910 - lr: 0.0060\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 2.7886 - val_loss: 1.4007 - lr: 0.0060\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 1.3908 - val_loss: 0.7435 - lr: 0.0060\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6549 - val_loss: 0.8475 - lr: 0.0060\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6086 - val_loss: 0.7040 - lr: 0.0060\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.5385 - val_loss: 0.3357 - lr: 0.0060\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3046 - val_loss: 0.3356 - lr: 0.0060\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3152 - val_loss: 0.3360 - lr: 0.0060\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.8883 - val_loss: 0.3995 - lr: 0.0060\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3174 - val_loss: 0.2470 - lr: 0.0060\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3135 - val_loss: 0.2942 - lr: 0.0060\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.4470 - val_loss: 8.0593 - lr: 0.0060\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 1.3468 - val_loss: 0.2794 - lr: 0.0060\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2787 - val_loss: 0.3185 - lr: 0.0060\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2761 - val_loss: 0.2371 - lr: 0.0060\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 1.2240 - val_loss: 0.2697 - lr: 0.0060\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2553 - val_loss: 0.5330 - lr: 0.0060\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.4169 - val_loss: 0.2240 - lr: 0.0060\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3334 - val_loss: 0.3222 - lr: 0.0060\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.4119 - val_loss: 0.4893 - lr: 0.0060\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3898 - val_loss: 0.4039 - lr: 0.0060\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3644 - val_loss: 0.4081 - lr: 0.0060\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3578 - val_loss: 0.3288 - lr: 0.0060\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2449 - val_loss: 0.2311 - lr: 0.0060\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2296 - val_loss: 0.2301 - lr: 0.0060\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2135 - val_loss: 0.2143 - lr: 0.0012\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2125 - val_loss: 0.2138 - lr: 0.0012\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2122 - val_loss: 0.2136 - lr: 0.0012\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2118 - val_loss: 0.2142 - lr: 0.0012\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2119 - val_loss: 0.2141 - lr: 0.0012\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2117 - val_loss: 0.2134 - lr: 0.0012\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2126 - val_loss: 0.2142 - lr: 0.0012\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2133 - val_loss: 0.2146 - lr: 0.0012\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2122 - val_loss: 0.2145 - lr: 0.0012\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2117 - val_loss: 0.2125 - lr: 0.0012\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2120 - val_loss: 0.2133 - lr: 0.0012\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2127 - val_loss: 0.2213 - lr: 0.0012\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2153 - val_loss: 0.2144 - lr: 0.0012\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2127 - val_loss: 0.2189 - lr: 0.0012\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2174 - val_loss: 0.2143 - lr: 0.0012\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2186 - val_loss: 0.2144 - lr: 0.0012\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2125 - val_loss: 0.2179 - lr: 0.0012\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2081 - val_loss: 0.2114 - lr: 2.4000e-04\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2073 - val_loss: 0.2119 - lr: 2.4000e-04\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2071 - val_loss: 0.2122 - lr: 2.4000e-04\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2073 - val_loss: 0.2121 - lr: 2.4000e-04\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2071 - val_loss: 0.2145 - lr: 2.4000e-04\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2072 - val_loss: 0.2134 - lr: 2.4000e-04\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2068 - val_loss: 0.2125 - lr: 2.4000e-04\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2073 - val_loss: 0.2130 - lr: 2.4000e-04\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2059 - val_loss: 0.2127 - lr: 4.8000e-05\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2058 - val_loss: 0.2120 - lr: 4.8000e-05\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2058 - val_loss: 0.2119 - lr: 4.8000e-05\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2057 - val_loss: 0.2119 - lr: 4.8000e-05\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2058 - val_loss: 0.2124 - lr: 4.8000e-05\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2057 - val_loss: 0.2122 - lr: 4.8000e-05\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2057 - val_loss: 0.2120 - lr: 4.8000e-05\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2054 - val_loss: 0.2117 - lr: 9.6000e-06\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2054 - val_loss: 0.2117 - lr: 9.6000e-06\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2054 - val_loss: 0.2119 - lr: 9.6000e-06\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2054 - val_loss: 0.2119 - lr: 9.6000e-06\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2053 - val_loss: 0.2119 - lr: 9.6000e-06\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2053 - val_loss: 0.2119 - lr: 9.6000e-06\n",
      "2681/2681 [==============================] - 1s 350us/step\n",
      "Fold 5 NN: 0.21144\n",
      "[0.00424927 0.00241504 0.00200584]\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "[0.00223619 0.01174973 0.01174973]\n"
     ]
    }
   ],
   "source": [
    "target_name='target'\n",
    "scores_folds = {}\n",
    "model_name = 'NN'\n",
    "pred_name = 'pred_{}'.format(model_name)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kfolds=5\n",
    "kf = KFold(n_splits=kfolds, shuffle=True, random_state=42)\n",
    "scores_folds[model_name] = []\n",
    "counter = 1\n",
    "colNames_0 = df_train.columns.tolist()\n",
    "colNames_0.remove('row_id')\n",
    "try:\n",
    "    colNames_0.remove('pred_NN')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "df_test['stock_id']=df_test['row_id'].str.split(\"-\").str[0]\n",
    "df_test['stock_id'] = df_test['stock_id'].astype(int)\n",
    "test_nn[['stock_id','time_id']]=df_test[['stock_id','time_id']]\n",
    "test_nn[colNames] = test_nn[colNames].fillna(train_nn[colNames].mean())\n",
    "\n",
    "\n",
    "train_nn = train_nn.fillna(0)\n",
    "test_nn = test_nn.fillna(0)\n",
    "test_predictions_nn = np.zeros(test_nn.shape[0])\n",
    "\n",
    "test_nn[target_name] = 0\n",
    "train_nn[pred_name] = 0\n",
    "\n",
    "for n_count in range(kfolds):\n",
    "    print('CV {}/{}'.format(counter, kfolds))\n",
    "    \n",
    "    indexes = np.arange(nfolds).astype(int)    \n",
    "    indexes = np.delete(indexes,obj=n_count, axis=0) \n",
    "    \n",
    "    indexes = np.r_[ind_values[indexes[0]],ind_values[indexes[1]],ind_values[indexes[2]],ind_values[indexes[3]]]\n",
    "    X_train = train_nn.loc[train_nn.time_id.isin(indexes), colNames_0]\n",
    "    df_train_1 = df_train.loc[df_train.time_id.isin(indexes),colNames_0]\n",
    "    train_cluster = generate_cluster_features(df_train_1, cluster_stocks).reset_index()\n",
    "    train_cluster = train_cluster.fillna(0)\n",
    "\n",
    "    X_train = pd.merge(X_train,train_cluster,how='left',on='time_id')\n",
    "    \n",
    "\n",
    "    X_test = train_nn.loc[train_nn.time_id.isin(ind_values[n_count]), colNames_0]\n",
    "    df_train_0 = df_train.loc[df_train.time_id.isin(ind_values[n_count]),colNames_0]\n",
    "    test_cluster = generate_cluster_features(df_train_0, cluster_stocks).reset_index()\n",
    "    test_cluster = test_cluster.fillna(0)\n",
    "    X_test = pd.merge(X_test,test_cluster,how='left',on='time_id')\n",
    "\n",
    "    y_train = train_nn.loc[train_nn.time_id.isin(indexes), target_name]\n",
    "    y_test = train_nn.loc[train_nn.time_id.isin(ind_values[n_count]), target_name]\n",
    "    \n",
    "    model = base_model()\n",
    "    \n",
    "    model.compile(\n",
    "        keras.optimizers.Adam(learning_rate=0.006),\n",
    "        loss=rmse_keras\n",
    "    )\n",
    "    colNames_new = train_nn.columns.tolist()\n",
    " \n",
    "    try:\n",
    "        colNames_new.remove('row_id')\n",
    "    except:\n",
    "        pass\n",
    "    colNames_new.remove('target')\n",
    "    colNames_new.remove('time_id')\n",
    "    '''try:\n",
    "        colNames_new.remove('stock_id')\n",
    "    except:\n",
    "        pass'''\n",
    "    try:\n",
    "        colNames_new.remove('pred_NN')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    X_train = X_train.loc[:, colNames_new]\n",
    "    X_test = X_test.loc[:, colNames_new]\n",
    "    \n",
    "    num_data = X_train[colNames_new]\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))         \n",
    "    num_data = scaler.fit_transform(num_data.values)    \n",
    "    \n",
    "    cat_data = X_train['stock_id']    \n",
    "    target =  y_train\n",
    "    \n",
    "    num_data_test = X_test[colNames_new]\n",
    "    num_data_test = scaler.transform(num_data_test.values)\n",
    "    cat_data_test = X_test['stock_id']\n",
    "\n",
    "    tt =scaler.transform(test_nn[colNames_new].values)\n",
    "\n",
    "    model.fit([cat_data, num_data], \n",
    "              target,               \n",
    "              batch_size=2048,\n",
    "              epochs=1000,\n",
    "              validation_data=([cat_data_test, num_data_test], y_test),\n",
    "              callbacks=[es, plateau],\n",
    "              validation_batch_size=len(y_test),\n",
    "              shuffle=True,\n",
    "             verbose = 1)\n",
    "\n",
    "    preds = model.predict([cat_data_test, num_data_test]).reshape(1,-1)[0]\n",
    "    \n",
    "    score = round(rmspe(y_true = y_test, y_pred = preds),5)\n",
    "    print('Fold {} {}: {}'.format(counter, model_name, score))\n",
    "    scores_folds[model_name].append(score)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(preds[:3])\n",
    "    test_predictions_nn += model.predict([test_nn['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)/kfolds\n",
    "    print(test_predictions_nn[:3])\n",
    "    #test[target_name] += model.predict([test['stock_id'], test[features_to_consider]]).reshape(1,-1)[0].clip(0,1e10)\n",
    "       \n",
    "    counter += 1\n",
    "    colNames_new.append('stock_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1aeeef",
   "metadata": {},
   "source": [
    "# 3. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7ee4b682",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T20:31:15.207926Z",
     "iopub.status.busy": "2025-09-06T20:31:15.207524Z",
     "iopub.status.idle": "2025-09-06T20:31:15.250678Z",
     "shell.execute_reply": "2025-09-06T20:31:15.249552Z"
    },
    "papermill": {
     "duration": 1.44711,
     "end_time": "2025-09-06T20:31:15.252526",
     "exception": false,
     "start_time": "2025-09-06T20:31:13.805416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSPE NN: 1.0 - Folds: [0.21619, 0.22287, 0.21142, 0.20927, 0.21144]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5q/z83yd5r96290nk8203y3gnkr0000gn/T/ipykernel_95708/3919126777.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_nn[\"row_id\"] = test_nn[\"stock_id\"].astype(str) + \"-\" + test_nn[\"time_id\"].astype(str)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_return1_realized_vol</th>\n",
       "      <th>log_return2_realized_vol</th>\n",
       "      <th>log_return1_wmp_realized_vol</th>\n",
       "      <th>log_return2_wmp_realized_vol</th>\n",
       "      <th>wap1_sum</th>\n",
       "      <th>wap1_std</th>\n",
       "      <th>wap2_sum</th>\n",
       "      <th>wap2_std</th>\n",
       "      <th>wmp1_sum</th>\n",
       "      <th>wmp1_std</th>\n",
       "      <th>...</th>\n",
       "      <th>cluster_6_total_volume_sum</th>\n",
       "      <th>cluster_6_size_sum</th>\n",
       "      <th>cluster_6_order_count_sum</th>\n",
       "      <th>cluster_6_price_spread_sum</th>\n",
       "      <th>cluster_6_bid_spread_sum</th>\n",
       "      <th>cluster_6_ask_spread_sum</th>\n",
       "      <th>cluster_6_volume_imbalance_sum</th>\n",
       "      <th>cluster_6_size_tau2</th>\n",
       "      <th>target</th>\n",
       "      <th>row_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.968637</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-2.387690</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-2.682197</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-2.668400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002398</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001254</td>\n",
       "      <td>0.001453</td>\n",
       "      <td>0.001412</td>\n",
       "      <td>0.004062</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>0.004728</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>0.003865</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>0.004748</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007181</td>\n",
       "      <td>0-32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001254</td>\n",
       "      <td>0.001453</td>\n",
       "      <td>0.001412</td>\n",
       "      <td>0.004062</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>0.004728</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>0.003865</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>0.004748</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007181</td>\n",
       "      <td>0-34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 218 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   log_return1_realized_vol  log_return2_realized_vol  \\\n",
       "0                 -3.968637                 -5.199338   \n",
       "1                  0.001254                  0.001453   \n",
       "2                  0.001254                  0.001453   \n",
       "\n",
       "   log_return1_wmp_realized_vol  log_return2_wmp_realized_vol  wap1_sum  \\\n",
       "0                     -5.199338                     -5.199338 -5.199338   \n",
       "1                      0.001412                      0.004062  0.000398   \n",
       "2                      0.001412                      0.004062  0.000398   \n",
       "\n",
       "   wap1_std  wap2_sum  wap2_std  wmp1_sum  wmp1_std  ...  \\\n",
       "0 -2.387690 -5.199338 -2.682197 -5.199338 -2.668400  ...   \n",
       "1  0.004728  0.000390  0.003865  0.000405  0.004748  ...   \n",
       "2  0.004728  0.000390  0.003865  0.000405  0.004748  ...   \n",
       "\n",
       "   cluster_6_total_volume_sum  cluster_6_size_sum  cluster_6_order_count_sum  \\\n",
       "0                         0.0                 0.0                        0.0   \n",
       "1                         0.0                 0.0                        0.0   \n",
       "2                         0.0                 0.0                        0.0   \n",
       "\n",
       "   cluster_6_price_spread_sum  cluster_6_bid_spread_sum  \\\n",
       "0                         0.0                       0.0   \n",
       "1                         0.0                       0.0   \n",
       "2                         0.0                       0.0   \n",
       "\n",
       "   cluster_6_ask_spread_sum  cluster_6_volume_imbalance_sum  \\\n",
       "0                       0.0                             0.0   \n",
       "1                       0.0                             0.0   \n",
       "2                       0.0                             0.0   \n",
       "\n",
       "   cluster_6_size_tau2    target  row_id  \n",
       "0                  0.0  0.002398     0-4  \n",
       "1                  0.0  0.007181    0-32  \n",
       "2                  0.0  0.007181    0-34  \n",
       "\n",
       "[3 rows x 218 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_nn[\"row_id\"] = test_nn[\"stock_id\"].astype(str) + \"-\" + test_nn[\"time_id\"].astype(str) \n",
    "test_nn[target_name] = (test_predictions_nn+test_pred)/2\n",
    "\n",
    "score = round(rmspe(y_true = train_nn[target_name].values, y_pred = train_nn[pred_name].values),5)\n",
    "print('RMSPE {}: {} - Folds: {}'.format(model_name, score, scores_folds[model_name]))\n",
    "\n",
    "display(test_nn)\n",
    "test_nn[['row_id', target_name]].to_csv('submission.csv',index = False)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 2344753,
     "sourceId": 27233,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3189.233975,
   "end_time": "2025-09-06T20:31:25.869635",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-06T19:38:16.635660",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
