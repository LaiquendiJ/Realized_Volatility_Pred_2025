{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5afad30",
   "metadata": {
    "papermill": {
     "duration": 0.00938,
     "end_time": "2025-09-06T19:38:22.822947",
     "exception": false,
     "start_time": "2025-09-06T19:38:22.813567",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Factor Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c33b4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T19:38:22.839845Z",
     "iopub.status.busy": "2025-09-06T19:38:22.839396Z",
     "iopub.status.idle": "2025-09-06T19:38:29.560092Z",
     "shell.execute_reply": "2025-09-06T19:38:29.559039Z"
    },
    "papermill": {
     "duration": 6.731075,
     "end_time": "2025-09-06T19:38:29.561889",
     "exception": false,
     "start_time": "2025-09-06T19:38:22.830814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Optional\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler, QuantileTransformer\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import glob\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5e27a7",
   "metadata": {
    "papermill": {
     "duration": 0.00771,
     "end_time": "2025-09-06T19:38:29.577446",
     "exception": false,
     "start_time": "2025-09-06T19:38:29.569736",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1.1 Feature Generation\n",
    "### 1.1.1 Book Features\n",
    "We first processes raw order book data and extracts meaningful features for modeling. The following table is the summary of the feature we generated from the original datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fabcf9",
   "metadata": {
    "papermill": {
     "duration": 0.007472,
     "end_time": "2025-09-06T19:38:29.594174",
     "exception": false,
     "start_time": "2025-09-06T19:38:29.586702",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "1.  **`wap1/2`**  =$ \\frac{\\text{ask\\_price}_{1/2} \\cdot \\text{bid\\_size}_{1/2} + \\text{bid\\_price}_{1/2} \\cdot \\text{ask\\_size}_{1/2}}{\\text{ask\\_size}_{1/2} + \\text{bid\\_size}_{1/2}}$ \n",
    "\n",
    "Weighted Average Price at depth 1/2. This smooths out the impact of individual order prices using size as weight. It approximates the \"fair\" market price better than simple mid-price. Including wap2 captures more of the limit order book’s shape. \n",
    "\n",
    "2. **`bid_size_diff`** = $\\text{bid\\_size}_1 - \\text{bid\\_size}_2$                                                                                      \n",
    "\n",
    "Captures the change in demand depth between level 1 and level 2. Large differences may imply lower resilience in the order book.   \n",
    "\n",
    "3. **`ask_size_diff`** =$\\text{ask\\_size}_1-\\text{ask\\_size}_2$                                                                               \n",
    "\n",
    "Measures ask-side change. Can signal a lack of liquidity on the sell side or increased selling pressure.                              \n",
    "\n",
    "4. **`price_spread`** = $ \\frac{\\text{ask\\_price}_1}{\\text{bid\\_price}_1} - 1$                                                                                \n",
    "\n",
    "A measure of transaction cost and short-term illiquidity.       \n",
    "\n",
    "5. **`order_imbalance_1/2`** =$\\frac{\\text{bid\\_size1/2} - \\text{ask\\_size1/2}}{\\text{bid\\_size1/2} + \\text{ask\\_size1/2}}$                                                     \n",
    "\n",
    "Measures demand-supply pressure at level 1/2. Positive values imply buying pressure, negative values suggest selling pressure.              \n",
    "\n",
    "6. **`depth_ratio`** = $\\frac{\\text{bid\\_size}_1 + \\text{bid\\_size}_2}{\\text{ask\\_size}_1 + \\text{ask\\_size}_2}$                                             \n",
    "\n",
    "Compares total buy-side depth to sell-side depth. Values > 1 suggest buy-side dominance.             \n",
    "                                    \n",
    "7. **`total_volume`** =$ \\text{bid\\_size}_1 + \\text{bid\\_size}_2 + \\text{ask\\_size}_1 + \\text{ask\\_size}_2$                                                  \n",
    "\n",
    "Captures overall liquidity in the limit order book. High volume typically corresponds to tighter spreads and lower volatility.            \n",
    "\n",
    "8. **`wap_diff`**  =$ \\text{wap1} - \\text{wap2}$                                                                                                           \n",
    "Measures how much prices diverge between the top and next levels of the book. Large values may indicate price pressure or volatility. Useful for detecting price trends or pressure. \n",
    "\n",
    "9. **`log_return1/2`** = $ \\log\\left( \\frac{\\text{wap1/2}_t}{\\text{wap1/2}_{t-1}} \\right)$                                                                        \n",
    "Log return of using wap1/2. Used in volatility estimation and price dynamics modeling.                       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78033bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T19:38:29.612075Z",
     "iopub.status.busy": "2025-09-06T19:38:29.611447Z",
     "iopub.status.idle": "2025-09-06T19:38:29.627702Z",
     "shell.execute_reply": "2025-09-06T19:38:29.626686Z"
    },
    "papermill": {
     "duration": 0.027215,
     "end_time": "2025-09-06T19:38:29.629379",
     "exception": false,
     "start_time": "2025-09-06T19:38:29.602164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_wmp(bid_p, ask_p, bid_s, ask_s):\n",
    "    return (pl.col(bid_p) * pl.col(bid_s) + pl.col(ask_p) * pl.col(ask_s)) /(pl.col(ask_s) + pl.col(bid_s))\n",
    "# WAP         \n",
    "def calc_wap(bid_p, ask_p, bid_s, ask_s):\n",
    "    return (pl.col(bid_p) * pl.col(ask_s) + pl.col(ask_p) * pl.col(bid_s)) / (\n",
    "        pl.col(bid_s) + pl.col(ask_s)\n",
    "    )\n",
    "\n",
    "# log return\n",
    "def log_return(col):\n",
    "    return pl.col(col).log().diff()\n",
    "\n",
    "# realized volatility\n",
    "def realized_vol(col):\n",
    "    return (pl.col(col).pow(2).sum()).sqrt()\n",
    "\n",
    "\n",
    "def agg_all(col: str, functions: list):\n",
    "    dict = {'sum': pl.col(col).sum().alias(f\"{col}_sum\"),\n",
    "            'mean': pl.col(col).mean().alias(f\"{col}_mean\"),\n",
    "            'std': pl.col(col).std().alias(f\"{col}_std\"),\n",
    "            'max': pl.col(col).max().alias(f\"{col}_max\"),\n",
    "            'min': pl.col(col).min().alias(f\"{col}_min\"),\n",
    "            'realized_vol':  (pl.col(col).pow(2).sum()).sqrt().alias(f\"{col}_realized_vol\"),\n",
    "             'unique':pl.col(col).n_unique().alias(f\"{col}_unique\")\n",
    "            }\n",
    "    return [\n",
    "        dict[i] for i in functions\n",
    "    ]\n",
    "\n",
    "\n",
    "def agg_with_rvol(col: str):\n",
    "    return agg_all(col) + [\n",
    "        (pl.col(col).pow(2).sum()).sqrt().alias(f\"{col}_realized_vol\")\n",
    "    ]\n",
    "\n",
    "\n",
    "def aggregate_interval_book(df: pl.DataFrame, interval_length: Optional[list] = None) -> pl.DataFrame:\n",
    "    if interval_length is not None:\n",
    "        start = 600 - interval_length\n",
    "        df = df.filter(pl.col(\"seconds_in_bucket\") >= start)\n",
    "    extra_exprs={}\n",
    "    if interval_length is None:\n",
    "        extra_exprs = {\n",
    "            \"wap1\": agg_all(\"wap1\",['sum','std']),\n",
    "        \"wap2\": agg_all(\"wap2\",['sum','std']),\n",
    "        \"wmp1\": agg_all(\"wmp1\",['sum','std']),\n",
    "        \"wmp2\": agg_all(\"wmp2\",['sum','std']),\n",
    "        \n",
    "        \"wap_balance\": agg_all(\"wap_balance\",['sum','max']),\n",
    "        \"price_spread\": agg_all(\"price_spread\",['sum','max']),\n",
    "        \"price_spread2\": agg_all(\"price_spread2\",['sum','max']),\n",
    "        \"bid_spread\": agg_all(\"bid_spread\",['sum','max']),\n",
    "        \"ask_spread\": agg_all(\"ask_spread\",['sum','max']),\n",
    "        \"total_volume\": agg_all(\"total_volume\",['sum','max']),\n",
    "        \"volume_imbalance\": agg_all(\"volume_imbalance\",['sum','max']),\n",
    "        }\n",
    "    agg_dict = {\n",
    "        \"log_return1\": agg_all(\"log_return1\",['realized_vol']),\n",
    "        \"log_return2\": agg_all(\"log_return2\",['realized_vol']),\n",
    "        \"log_return1_wmp\": agg_all(\"log_return1_wmp\",['realized_vol']),\n",
    "        \"log_return2_wmp\": agg_all(\"log_return2_wmp\",['realized_vol']),\n",
    "    }\n",
    "    agg_dict = agg_dict|extra_exprs\n",
    "    df_agg = df.group_by(\"time_id\").agg(\n",
    "        [expr for exprs in agg_dict.values() for expr in exprs]\n",
    "    )\n",
    "\n",
    "    if interval_length is not None:\n",
    "        df_agg = df_agg.rename({col: f\"{col}_{interval_length}\" if col != \"time_id\" else \"time_id\"\n",
    "                                for col in df_agg.columns})\n",
    "\n",
    "    return df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9fb70e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T19:38:29.645902Z",
     "iopub.status.busy": "2025-09-06T19:38:29.645545Z",
     "iopub.status.idle": "2025-09-06T19:38:29.658284Z",
     "shell.execute_reply": "2025-09-06T19:38:29.657424Z"
    },
    "papermill": {
     "duration": 0.022962,
     "end_time": "2025-09-06T19:38:29.660086",
     "exception": false,
     "start_time": "2025-09-06T19:38:29.637124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocessor_book(file_path, time_length_list):\n",
    "    stock_id = int(file_path.split('=')[1].split('.')[0])\n",
    "    df = pl.read_parquet(file_path)\n",
    "    df = (\n",
    "        df.with_columns([\n",
    "            calc_wap(\"bid_price1\",\"ask_price1\",\"bid_size1\",\"ask_size1\").alias(\"wap1\"),\n",
    "            calc_wap(\"bid_price2\",\"ask_price2\",\"bid_size2\",\"ask_size2\").alias(\"wap2\"),\n",
    "            calc_wmp(\"bid_price1\",\"ask_price1\",\"bid_size1\",\"ask_size1\").alias(\"wmp1\"),\n",
    "            calc_wmp(\"bid_price2\",\"ask_price2\",\"bid_size2\",\"ask_size2\").alias(\"wmp2\"),\n",
    "        ])\n",
    "        .with_columns([\n",
    "            log_return(\"wap1\").alias(\"log_return1\"),\n",
    "            log_return(\"wap2\").alias(\"log_return2\"),\n",
    "            log_return(\"wmp1\").alias(\"log_return1_wmp\"),\n",
    "            log_return(\"wmp2\").alias(\"log_return2_wmp\"),\n",
    "            (pl.col(\"wap1\") - pl.col(\"wap2\")).abs().alias(\"wap_balance\"),\n",
    "            ((pl.col(\"ask_price1\") - pl.col(\"bid_price1\")) \n",
    "             / ((pl.col(\"ask_price1\") + pl.col(\"bid_price1\"))/2)).alias(\"price_spread\"),\n",
    "            ((pl.col(\"ask_price2\") - pl.col(\"bid_price2\")) \n",
    "             / ((pl.col(\"ask_price2\") + pl.col(\"bid_price2\"))/2)).alias(\"price_spread2\"),\n",
    "            (pl.col(\"bid_price1\") - pl.col(\"bid_price2\")).alias(\"bid_spread\"),\n",
    "            (pl.col(\"ask_price1\") - pl.col(\"ask_price2\")).alias(\"ask_spread\"),\n",
    "            (pl.col(\"ask_size1\")+pl.col(\"ask_size2\")+pl.col(\"bid_size1\")+pl.col(\"bid_size2\")).alias(\"total_volume\"),\n",
    "            ((pl.col(\"ask_size1\")+pl.col(\"ask_size2\"))-(pl.col(\"bid_size1\")+pl.col(\"bid_size2\"))).abs().alias(\"volume_imbalance\"),\n",
    "            \n",
    "            # bid_size_diff, ask_size_diff\n",
    "        (pl.col(\"bid_size1\") / pl.col(\"bid_size2\")-1).alias(\"bid_size_diff\"),\n",
    "        (pl.col(\"ask_size1\") / pl.col(\"ask_size2\")-1).alias(\"ask_size_diff\"),\n",
    " \n",
    "       \n",
    "        # order_imbalance_1, order_imbalance_total\n",
    "        ((pl.col(\"bid_size1\") - pl.col(\"ask_size1\")) /\n",
    "         (pl.col(\"bid_size1\") + pl.col(\"ask_size1\"))).alias(\"order_imbalance_1\"),\n",
    "        ((pl.col(\"bid_size1\") + pl.col(\"bid_size2\") - pl.col(\"ask_size1\") - pl.col(\"ask_size2\")) /\n",
    "         (pl.col(\"bid_size1\") + pl.col(\"bid_size2\") + pl.col(\"ask_size1\") + pl.col(\"ask_size2\"))).alias(\"order_imbalance_total\"),\n",
    "        \n",
    "        ])\n",
    "    )\n",
    "  \n",
    "    # Step 2: Add dynamic (time-based) features\n",
    "    df = df.sort([\"time_id\", \"seconds_in_bucket\"])\n",
    "\n",
    "    # Step 3: Drop raw order book columns to reduce storage and redundancy\n",
    "    df = df.drop([\n",
    "        \"ask_price1\", \"ask_price2\", \"ask_size1\", \"ask_size2\",\n",
    "        \"bid_price1\", \"bid_price2\", \"bid_size1\", \"bid_size2\"\n",
    "    ])\n",
    "\n",
    "    # Rows with null values for log_return1/2 are dropped. These nulls are a result of the `.diff()` operation within the calculation.\n",
    "    merged_df = None\n",
    "    for length in time_length_list:\n",
    "        df_agg = aggregate_interval_book(df, length)\n",
    "        merged_df = df_agg if merged_df is None else merged_df.join(\n",
    "        df_agg, on=\"time_id\", how=\"left\")\n",
    "\n",
    "    df_feature = aggregate_interval_book(df)\n",
    "\n",
    "    df_feature = df_feature.join(merged_df, on=\"time_id\", how=\"left\")\n",
    "\n",
    "    df_feature = df_feature.with_columns(\n",
    "        (pl.lit(stock_id).cast(pl.Utf8) + \"-\" +\n",
    "         pl.col(\"time_id\").cast(pl.Utf8)).alias(\"row_id\")\n",
    "    )\n",
    "    \n",
    "\n",
    "\n",
    "    return df_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbfa9a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T19:38:29.676970Z",
     "iopub.status.busy": "2025-09-06T19:38:29.676276Z",
     "iopub.status.idle": "2025-09-06T19:38:30.471011Z",
     "shell.execute_reply": "2025-09-06T19:38:30.469998Z"
    },
    "papermill": {
     "duration": 0.80485,
     "end_time": "2025-09-06T19:38:30.472709",
     "exception": false,
     "start_time": "2025-09-06T19:38:29.667859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 36)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>time_id</th><th>log_return1_realized_vol</th><th>log_return2_realized_vol</th><th>log_return1_wmp_realized_vol</th><th>log_return2_wmp_realized_vol</th><th>wap1_sum</th><th>wap1_std</th><th>wap2_sum</th><th>wap2_std</th><th>wmp1_sum</th><th>wmp1_std</th><th>wmp2_sum</th><th>wmp2_std</th><th>wap_balance_sum</th><th>wap_balance_max</th><th>price_spread_sum</th><th>price_spread_max</th><th>price_spread2_sum</th><th>price_spread2_max</th><th>bid_spread_sum</th><th>bid_spread_max</th><th>ask_spread_sum</th><th>ask_spread_max</th><th>total_volume_sum</th><th>total_volume_max</th><th>volume_imbalance_sum</th><th>volume_imbalance_max</th><th>log_return1_realized_vol_300</th><th>log_return2_realized_vol_300</th><th>log_return1_wmp_realized_vol_300</th><th>log_return2_wmp_realized_vol_300</th><th>log_return1_realized_vol_150</th><th>log_return2_realized_vol_150</th><th>log_return1_wmp_realized_vol_150</th><th>log_return2_wmp_realized_vol_150</th><th>row_id</th></tr><tr><td>i16</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td></tr></thead><tbody><tr><td>5</td><td>0.004499</td><td>0.006999</td><td>0.005466</td><td>0.006119</td><td>303.125061</td><td>0.000693</td><td>303.105539</td><td>0.000781</td><td>303.134863</td><td>0.000637</td><td>303.146936</td><td>0.000652</td><td>0.117051</td><td>0.001414</td><td>0.257255</td><td>0.001393</td><td>0.355451</td><td>0.001699</td><td>0.053006</td><td>0.000672</td><td>-0.045557</td><td>-0.000052</td><td>97696</td><td>762</td><td>40738</td><td>518</td><td>0.002953</td><td>0.004863</td><td>0.003873</td><td>0.004492</td><td>0.001721</td><td>0.004114</td><td>0.002599</td><td>0.003368</td><td>&quot;0-5&quot;</td></tr><tr><td>11</td><td>0.003966</td><td>0.004628</td><td>0.003873</td><td>0.003877</td><td>200.047768</td><td>0.000262</td><td>200.041171</td><td>0.000272</td><td>200.035611</td><td>0.000298</td><td>200.040851</td><td>0.000278</td><td>0.042312</td><td>0.000639</td><td>0.078836</td><td>0.000903</td><td>0.134182</td><td>0.001104</td><td>0.028358</td><td>0.000652</td><td>-0.027001</td><td>-0.00005</td><td>82290</td><td>876</td><td>28410</td><td>481</td><td>0.000981</td><td>0.002009</td><td>0.001308</td><td>0.001637</td><td>0.000918</td><td>0.001883</td><td>0.001169</td><td>0.001438</td><td>&quot;0-11&quot;</td></tr><tr><td>16</td><td>0.002451</td><td>0.00496</td><td>0.002658</td><td>0.004693</td><td>187.913849</td><td>0.000864</td><td>187.939824</td><td>0.000862</td><td>187.923063</td><td>0.00067</td><td>187.897375</td><td>0.000779</td><td>0.062228</td><td>0.001135</td><td>0.13633</td><td>0.001149</td><td>0.210563</td><td>0.001915</td><td>0.036955</td><td>0.00067</td><td>-0.037243</td><td>-0.000048</td><td>78274</td><td>758</td><td>26586</td><td>579</td><td>0.001295</td><td>0.003196</td><td>0.001815</td><td>0.002354</td><td>0.001158</td><td>0.002972</td><td>0.001703</td><td>0.001947</td><td>&quot;0-16&quot;</td></tr><tr><td>31</td><td>0.003742</td><td>0.00409</td><td>0.003071</td><td>0.004029</td><td>119.859781</td><td>0.000757</td><td>119.835941</td><td>0.000656</td><td>119.870163</td><td>0.000606</td><td>119.88424</td><td>0.000733</td><td>0.045611</td><td>0.001082</td><td>0.103252</td><td>0.001622</td><td>0.139066</td><td>0.002039</td><td>0.022764</td><td>0.000694</td><td>-0.013001</td><td>-0.000046</td><td>52232</td><td>912</td><td>17546</td><td>576</td><td>0.001776</td><td>0.002713</td><td>0.001312</td><td>0.001814</td><td>0.000993</td><td>0.001424</td><td>0.000441</td><td>0.000636</td><td>&quot;0-31&quot;</td></tr><tr><td>62</td><td>0.00321</td><td>0.003988</td><td>0.002414</td><td>0.00348</td><td>175.932865</td><td>0.000258</td><td>175.934256</td><td>0.000317</td><td>175.928283</td><td>0.000215</td><td>175.912533</td><td>0.000312</td><td>0.044783</td><td>0.000724</td><td>0.069901</td><td>0.000793</td><td>0.122698</td><td>0.001166</td><td>0.033565</td><td>0.000466</td><td>-0.019206</td><td>-0.000047</td><td>60407</td><td>738</td><td>21797</td><td>424</td><td>0.00152</td><td>0.002188</td><td>0.001625</td><td>0.002443</td><td>0.001378</td><td>0.000966</td><td>0.001197</td><td>0.001488</td><td>&quot;0-62&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 36)\n",
       "┌─────────┬────────────┬────────────┬────────────┬───┬────────────┬───────────┬───────────┬────────┐\n",
       "│ time_id ┆ log_return ┆ log_return ┆ log_return ┆ … ┆ log_return ┆ log_retur ┆ log_retur ┆ row_id │\n",
       "│ ---     ┆ 1_realized ┆ 2_realized ┆ 1_wmp_real ┆   ┆ 2_realized ┆ n1_wmp_re ┆ n2_wmp_re ┆ ---    │\n",
       "│ i16     ┆ _vol       ┆ _vol       ┆ ized_vol   ┆   ┆ _vol_150   ┆ alized_vo ┆ alized_vo ┆ str    │\n",
       "│         ┆ ---        ┆ ---        ┆ ---        ┆   ┆ ---        ┆ l_1…      ┆ l_1…      ┆        │\n",
       "│         ┆ f64        ┆ f64        ┆ f64        ┆   ┆ f64        ┆ ---       ┆ ---       ┆        │\n",
       "│         ┆            ┆            ┆            ┆   ┆            ┆ f64       ┆ f64       ┆        │\n",
       "╞═════════╪════════════╪════════════╪════════════╪═══╪════════════╪═══════════╪═══════════╪════════╡\n",
       "│ 5       ┆ 0.004499   ┆ 0.006999   ┆ 0.005466   ┆ … ┆ 0.004114   ┆ 0.002599  ┆ 0.003368  ┆ 0-5    │\n",
       "│ 11      ┆ 0.003966   ┆ 0.004628   ┆ 0.003873   ┆ … ┆ 0.001883   ┆ 0.001169  ┆ 0.001438  ┆ 0-11   │\n",
       "│ 16      ┆ 0.002451   ┆ 0.00496    ┆ 0.002658   ┆ … ┆ 0.002972   ┆ 0.001703  ┆ 0.001947  ┆ 0-16   │\n",
       "│ 31      ┆ 0.003742   ┆ 0.00409    ┆ 0.003071   ┆ … ┆ 0.001424   ┆ 0.000441  ┆ 0.000636  ┆ 0-31   │\n",
       "│ 62      ┆ 0.00321    ┆ 0.003988   ┆ 0.002414   ┆ … ┆ 0.000966   ┆ 0.001197  ┆ 0.001488  ┆ 0-62   │\n",
       "└─────────┴────────────┴────────────┴────────────┴───┴────────────┴───────────┴───────────┴────────┘"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor_book('data/optiver-realized-volatility-prediction/book_train.parquet/stock_id=0',[300,150]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b235aba2",
   "metadata": {
    "papermill": {
     "duration": 0.007808,
     "end_time": "2025-09-06T19:38:30.488752",
     "exception": false,
     "start_time": "2025-09-06T19:38:30.480944",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1.1.2 10-Minute Time Window Book Features\n",
    "This code performs 10min-time-interval aggregation on order book data. It groups data by time_id within specified time intervals and computes key features such as realized volatility, logarithmic price range, and various mean values related to prices, order imbalances, and volume. The aggregated features are then saved as Parquet files named by stock ID, enabling efficient downstream analysis and modeling. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6579afb5",
   "metadata": {
    "papermill": {
     "duration": 0.007982,
     "end_time": "2025-09-06T19:38:30.506924",
     "exception": false,
     "start_time": "2025-09-06T19:38:30.498942",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1.1.3 150-Second Time Segment Book Features\n",
    "This code processes order book data by dividing each trading period into fixed-length intervals (in this case, 4 intervals of 150 seconds each). For each stock file, it aggregates features separately for each time interval using the aggregate_interval_book function, and then merges these interval-based feature sets into a single DataFrame keyed by time_id. The resulting combined features capture finer-grained temporal dynamics within the trading period. Finally, the aggregated data is saved as Parquet files organized by stock ID."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b395a0f9",
   "metadata": {
    "papermill": {
     "duration": 0.008381,
     "end_time": "2025-09-06T19:38:30.525171",
     "exception": false,
     "start_time": "2025-09-06T19:38:30.516790",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1.1.4 Trade Features \n",
    "This code processes trade data for stocks, computing additional features such as average trade size per order and the logarithmic return of trade prices. The logarithmic return is calculated as the log of the ratio between the current price and the previous price. Rows with null values resulting from this calculation (due to missing previous prices) are removed to ensure data quality. The cleaned and enriched DataFrame is then saved as Parquet files, organized by stock ID.\n",
    "\n",
    "### 1.1.5 10-Minute Time Window Trade Features\n",
    "This code aggregates trade data into 10-minute intervals by grouping on time_id. For each interval, it calculates various statistical features, including realized volatility from squared log returns, price range, mean price, mean order size, total traded size, and standard deviation of trade sizes. The processed features are saved to separate Parquet files for each stock, enabling efficient downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b243d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T19:38:30.543208Z",
     "iopub.status.busy": "2025-09-06T19:38:30.542879Z",
     "iopub.status.idle": "2025-09-06T19:38:30.551009Z",
     "shell.execute_reply": "2025-09-06T19:38:30.549928Z"
    },
    "papermill": {
     "duration": 0.020133,
     "end_time": "2025-09-06T19:38:30.553518",
     "exception": false,
     "start_time": "2025-09-06T19:38:30.533385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def aggregate_interval_trade(df: pl.DataFrame, interval_length: Optional[list] = None) -> pl.DataFrame:\n",
    "    if interval_length is not None:\n",
    "        start = 600 - interval_length\n",
    "        df = df.filter(pl.col(\"seconds_in_bucket\") >= start)\n",
    "\n",
    "    extra_exprs={}\n",
    "    if interval_length is None:\n",
    "        extra_exprs = {\n",
    "            \"size_per_order\": agg_all(\"size_per_order\",['mean']),\n",
    "            \"size\": agg_all(\"size\",['sum','min','max']),\n",
    "            \"order_count\": agg_all(\"order_count\",['max']),\n",
    "            \"amount\":agg_all(\"amount\",['sum','max','min'])\n",
    "        }\n",
    "        \n",
    "    agg_dict = {\n",
    "        \"trade_log_return\": agg_all(\"trade_log_return\",['realized_vol']),\n",
    "        \"order_count\":agg_all(\"order_count\",['sum']),\n",
    "        \"seconds_in_bucket\": agg_all(\"seconds_in_bucket\",['unique'])   \n",
    "    }\n",
    "    vals = list(agg_dict.values()) + list(extra_exprs.values())\n",
    "    df_agg = df.group_by(\"time_id\").agg(\n",
    "        [expr for exprs in vals for expr in exprs]\n",
    "    )\n",
    "    if interval_length is not None:\n",
    "        df_agg = df_agg.rename({col: f\"{col}_{interval_length}\" if col != \"time_id\" else \"time_id\"\n",
    "                                for col in df_agg.columns})\n",
    "\n",
    "    return df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d695542e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T19:38:30.574804Z",
     "iopub.status.busy": "2025-09-06T19:38:30.574430Z",
     "iopub.status.idle": "2025-09-06T19:38:30.582197Z",
     "shell.execute_reply": "2025-09-06T19:38:30.581134Z"
    },
    "papermill": {
     "duration": 0.018858,
     "end_time": "2025-09-06T19:38:30.583971",
     "exception": false,
     "start_time": "2025-09-06T19:38:30.565113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocessor_trade(file_path, time_length_list):\n",
    "    stock_id = int(file_path.split('=')[1].split('.')[0])\n",
    "    df = pl.read_parquet(file_path)\n",
    "    df = df.sort([\"time_id\", \"seconds_in_bucket\"])\n",
    "\n",
    "    df = df.with_columns([\n",
    "        # size_per_order\n",
    "        (pl.col(\"size\")/pl.col(\"order_count\")).alias(\"size_per_order\"),\n",
    "        # trade_log_return\n",
    "         (pl.col(\"price\").diff().over(\"time_id\")).alias(\"trade_log_return\"),\n",
    "         pl.col(\"size\").mean().alias(\"size_mean\"),\n",
    "          (pl.col(\"price\")* pl.col(\"size\")).alias(\"amount\")\n",
    "        \n",
    "    ])\n",
    "\n",
    "    merged_df = None\n",
    "    for length in time_length_list:\n",
    "        \n",
    "        df_agg = aggregate_interval_trade(df, length)\n",
    "        merged_df = df_agg if merged_df is None else merged_df.join(df_agg, on=\"time_id\", how=\"left\")\n",
    "\n",
    "    df_feature = aggregate_interval_trade(df)  \n",
    "    df_feature = df_feature.join(merged_df, on=\"time_id\", how=\"left\")\n",
    "\n",
    "    df_feature = df_feature.with_columns(\n",
    "        (pl.lit(stock_id).cast(pl.Utf8) + \"-\" + pl.col(\"time_id\").cast(pl.Utf8)).alias(\"row_id\")\n",
    "    )\n",
    "    \n",
    "\n",
    "    return df_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1fdb57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T19:38:30.601160Z",
     "iopub.status.busy": "2025-09-06T19:38:30.600446Z",
     "iopub.status.idle": "2025-09-06T19:38:30.693309Z",
     "shell.execute_reply": "2025-09-06T19:38:30.692371Z"
    },
    "papermill": {
     "duration": 0.103332,
     "end_time": "2025-09-06T19:38:30.695291",
     "exception": false,
     "start_time": "2025-09-06T19:38:30.591959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 19)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>time_id</th><th>trade_log_return_realized_vol</th><th>order_count_sum</th><th>seconds_in_bucket_unique</th><th>size_per_order_mean</th><th>size_sum</th><th>size_min</th><th>size_max</th><th>order_count_max</th><th>amount_sum</th><th>amount_max</th><th>amount_min</th><th>trade_log_return_realized_vol_300</th><th>order_count_sum_300</th><th>seconds_in_bucket_unique_300</th><th>trade_log_return_realized_vol_150</th><th>order_count_sum_150</th><th>seconds_in_bucket_unique_150</th><th>row_id</th></tr><tr><td>i16</td><td>f32</td><td>i64</td><td>u32</td><td>f64</td><td>i32</td><td>i32</td><td>i32</td><td>i16</td><td>f64</td><td>f64</td><td>f64</td><td>f32</td><td>i64</td><td>u32</td><td>f32</td><td>i64</td><td>u32</td><td>str</td></tr></thead><tbody><tr><td>5</td><td>0.002013</td><td>110</td><td>40</td><td>23.118036</td><td>3179</td><td>1</td><td>499</td><td>12</td><td>3190.139181</td><td>500.592485</td><td>1.002715</td><td>0.001313</td><td>54</td><td>21</td><td>0.001063</td><td>37</td><td>14</td><td>&quot;0-5&quot;</td></tr><tr><td>11</td><td>0.000901</td><td>57</td><td>30</td><td>20.061111</td><td>1289</td><td>1</td><td>280</td><td>6</td><td>1289.353432</td><td>280.020061</td><td>0.999724</td><td>0.000588</td><td>36</td><td>16</td><td>0.000501</td><td>22</td><td>10</td><td>&quot;0-11&quot;</td></tr><tr><td>16</td><td>0.00196</td><td>68</td><td>25</td><td>25.548476</td><td>2161</td><td>1</td><td>391</td><td>8</td><td>2158.608928</td><td>390.44356</td><td>0.999928</td><td>0.001135</td><td>38</td><td>12</td><td>0.001046</td><td>33</td><td>9</td><td>&quot;0-16&quot;</td></tr><tr><td>31</td><td>0.001559</td><td>59</td><td>15</td><td>33.507407</td><td>1962</td><td>5</td><td>450</td><td>15</td><td>1959.605547</td><td>449.087539</td><td>4.991672</td><td>0.001088</td><td>46</td><td>9</td><td>0.0008</td><td>11</td><td>3</td><td>&quot;0-31&quot;</td></tr><tr><td>62</td><td>0.000871</td><td>89</td><td>22</td><td>16.177243</td><td>1791</td><td>1</td><td>341</td><td>17</td><td>1790.254496</td><td>340.804756</td><td>0.999231</td><td>0.000453</td><td>54</td><td>11</td><td>0.00036</td><td>14</td><td>4</td><td>&quot;0-62&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 19)\n",
       "┌─────────┬────────────┬────────────┬────────────┬───┬────────────┬───────────┬───────────┬────────┐\n",
       "│ time_id ┆ trade_log_ ┆ order_coun ┆ seconds_in ┆ … ┆ trade_log_ ┆ order_cou ┆ seconds_i ┆ row_id │\n",
       "│ ---     ┆ return_rea ┆ t_sum      ┆ _bucket_un ┆   ┆ return_rea ┆ nt_sum_15 ┆ n_bucket_ ┆ ---    │\n",
       "│ i16     ┆ lized_vol  ┆ ---        ┆ ique       ┆   ┆ lized_vol_ ┆ 0         ┆ unique_15 ┆ str    │\n",
       "│         ┆ ---        ┆ i64        ┆ ---        ┆   ┆ …          ┆ ---       ┆ 0         ┆        │\n",
       "│         ┆ f32        ┆            ┆ u32        ┆   ┆ ---        ┆ i64       ┆ ---       ┆        │\n",
       "│         ┆            ┆            ┆            ┆   ┆ f32        ┆           ┆ u32       ┆        │\n",
       "╞═════════╪════════════╪════════════╪════════════╪═══╪════════════╪═══════════╪═══════════╪════════╡\n",
       "│ 5       ┆ 0.002013   ┆ 110        ┆ 40         ┆ … ┆ 0.001063   ┆ 37        ┆ 14        ┆ 0-5    │\n",
       "│ 11      ┆ 0.000901   ┆ 57         ┆ 30         ┆ … ┆ 0.000501   ┆ 22        ┆ 10        ┆ 0-11   │\n",
       "│ 16      ┆ 0.00196    ┆ 68         ┆ 25         ┆ … ┆ 0.001046   ┆ 33        ┆ 9         ┆ 0-16   │\n",
       "│ 31      ┆ 0.001559   ┆ 59         ┆ 15         ┆ … ┆ 0.0008     ┆ 11        ┆ 3         ┆ 0-31   │\n",
       "│ 62      ┆ 0.000871   ┆ 89         ┆ 22         ┆ … ┆ 0.00036    ┆ 14        ┆ 4         ┆ 0-62   │\n",
       "└─────────┴────────────┴────────────┴────────────┴───┴────────────┴───────────┴───────────┴────────┘"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor_trade('data/optiver-realized-volatility-prediction/trade_train.parquet/stock_id=0',[300,150]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d0ad5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T19:38:30.714080Z",
     "iopub.status.busy": "2025-09-06T19:38:30.713650Z",
     "iopub.status.idle": "2025-09-06T19:38:30.722020Z",
     "shell.execute_reply": "2025-09-06T19:38:30.720568Z"
    },
    "papermill": {
     "duration": 0.020005,
     "end_time": "2025-09-06T19:38:30.723696",
     "exception": false,
     "start_time": "2025-09-06T19:38:30.703691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocessor(list_book_files, list_trade_files, target_data, time_length_list, train=True) -> pd.DataFrame:\n",
    "    dfs = []\n",
    "    for book_file, trade_file in tqdm(zip(list_book_files, list_trade_files)):\n",
    "        df_book = preprocessor_book(book_file, time_length_list)\n",
    "        df_trade = preprocessor_trade(trade_file, time_length_list)\n",
    "        df_tmp = df_book.join(df_trade, on=\"row_id\", how=\"left\")\n",
    "\n",
    "        dfs.append(df_tmp)\n",
    "\n",
    "    df = pl.concat(dfs)\n",
    "    target_data = target_data.with_columns(\n",
    "        (pl.col(\"stock_id\").cast(pl.Utf8) + \"-\" +\n",
    "         pl.col(\"time_id\").cast(pl.Utf8)).alias(\"row_id\")\n",
    "    )\n",
    "    if train:\n",
    "        target_data = target_data.select([\n",
    "            \"row_id\",\n",
    "            \"target\"\n",
    "        ])\n",
    "    else:\n",
    "        target_data = target_data.select([\n",
    "            \"row_id\",\n",
    "\n",
    "        ])\n",
    "    df = df.with_columns(\n",
    "        pl.col(\"row_id\").str.split(\n",
    "            \"-\").list.get(0).cast(pl.Int64).alias(\"stock_id\")\n",
    "    )\n",
    "    df = target_data.join(df, on=\"row_id\", how=\"left\")\n",
    "    df = df.to_pandas()\n",
    "    df.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1e7e5b",
   "metadata": {
    "papermill": {
     "duration": 0.008457,
     "end_time": "2025-09-06T19:38:30.740716",
     "exception": false,
     "start_time": "2025-09-06T19:38:30.732259",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Training data preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cbd9de",
   "metadata": {
    "papermill": {
     "duration": 0.008166,
     "end_time": "2025-09-06T19:38:30.757008",
     "exception": false,
     "start_time": "2025-09-06T19:38:30.748842",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Test data preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9b581f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T19:38:30.774414Z",
     "iopub.status.busy": "2025-09-06T19:38:30.774082Z",
     "iopub.status.idle": "2025-09-06T19:38:30.966230Z",
     "shell.execute_reply": "2025-09-06T19:38:30.965139Z"
    },
    "papermill": {
     "duration": 0.202716,
     "end_time": "2025-09-06T19:38:30.967862",
     "exception": false,
     "start_time": "2025-09-06T19:38:30.765146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_book_test= glob.glob(\n",
    "    'data/optiver-realized-volatility-prediction/book_test.parquet/*')\n",
    "list_trade_test = glob.glob(\n",
    "    'data/optiver-realized-volatility-prediction/trade_test.parquet/*')\n",
    "test_data = pl.read_csv(\n",
    "    'data/optiver-realized-volatility-prediction/test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd20d53",
   "metadata": {
    "papermill": {
     "duration": 0.007979,
     "end_time": "2025-09-06T19:38:30.984502",
     "exception": false,
     "start_time": "2025-09-06T19:38:30.976523",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## K-fold based on K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5073e12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T19:38:31.003393Z",
     "iopub.status.busy": "2025-09-06T19:38:31.003089Z",
     "iopub.status.idle": "2025-09-06T19:38:31.865838Z",
     "shell.execute_reply": "2025-09-06T19:38:31.865093Z"
    },
    "papermill": {
     "duration": 0.874806,
     "end_time": "2025-09-06T19:38:31.867693",
     "exception": false,
     "start_time": "2025-09-06T19:38:30.992887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/optiver-realized-volatility-prediction/train.csv\")\n",
    "train_p = train.pivot(\n",
    "    index='time_id',\n",
    "    columns='stock_id',\n",
    "    values='target'\n",
    ")\n",
    "\n",
    "corr = train_p.corr()\n",
    "ids = corr.index\n",
    "cluster_num = 7\n",
    "kmeans = KMeans(n_clusters=cluster_num, random_state=0,\n",
    "                n_init=10).fit(corr.values)\n",
    "labels = kmeans.labels_\n",
    "cluster_stocks = {}\n",
    "for i in range(cluster_num):\n",
    "    cluster_stocks[i] = [ids[j] for j in range(len(ids)) if labels[j] == i]\n",
    "\n",
    "\n",
    "def generate_cluster_features(df: pd.DataFrame, cluster_stocks: dict) -> pd.DataFrame:\n",
    "    cluster_features = {}\n",
    "\n",
    "    for c, stocks in cluster_stocks.items():\n",
    "        #print(f\"Cluster {c} has {len(stocks)} stocks: {stocks}\")\n",
    "        if len(stocks)<10:\n",
    "            cluster_df = df\n",
    "        else:\n",
    "            cluster_df = df[df['stock_id'].isin(stocks)]\n",
    "        numeric_cols = ['log_return1_realized_vol', 'total_volume_sum', 'size_sum', 'order_count_sum',\n",
    "                        'price_spread_sum', 'bid_spread_sum', 'ask_spread_sum', 'volume_imbalance_sum','size_tau2']\n",
    "        cluster_avg = (\n",
    "            cluster_df.groupby(\"time_id\")[numeric_cols]\n",
    "            .mean()\n",
    "        )\n",
    "\n",
    "        cluster_avg = cluster_avg.rename(\n",
    "            columns={col: f\"cluster_{c}_{col}\" for col in numeric_cols}\n",
    "        )\n",
    "        cluster_features[c] = cluster_avg\n",
    "\n",
    "    cluster_list = list(cluster_features.values())\n",
    "    result = cluster_list[0]\n",
    "\n",
    "    for cl in cluster_list[1:]:\n",
    "        result = result.merge(cl, on='time_id', how='left')\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9094fb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T19:38:31.959104Z",
     "iopub.status.busy": "2025-09-06T19:38:31.958778Z",
     "iopub.status.idle": "2025-09-06T19:38:32.543815Z",
     "shell.execute_reply": "2025-09-06T19:38:32.542871Z"
    },
    "papermill": {
     "duration": 0.596688,
     "end_time": "2025-09-06T19:38:32.546171",
     "exception": false,
     "start_time": "2025-09-06T19:38:31.949483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pca = PCA(n_components=2)\n",
    "corr_2d = pca.fit_transform(corr.values)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(corr_2d[:,0], corr_2d[:,1], c=kmeans.labels_, cmap='tab10', s=100)\n",
    "for i, stock_id in enumerate(corr.index):\n",
    "    plt.text(corr_2d[i,0], corr_2d[i,1], str(stock_id), fontsize=9)\n",
    "plt.title(\"KMeans Clustering Visualization (PCA 2D)\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8d08d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T19:38:32.568386Z",
     "iopub.status.busy": "2025-09-06T19:38:32.568077Z",
     "iopub.status.idle": "2025-09-06T19:38:32.614700Z",
     "shell.execute_reply": "2025-09-06T19:38:32.613620Z"
    },
    "papermill": {
     "duration": 0.059043,
     "end_time": "2025-09-06T19:38:32.616377",
     "exception": false,
     "start_time": "2025-09-06T19:38:32.557334",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_book_train = glob.glob(\n",
    "    'data/optiver-realized-volatility-prediction/book_train.parquet/*')\n",
    "list_trade_train = glob.glob(\n",
    "    'data/optiver-realized-volatility-prediction/trade_train.parquet/*')\n",
    "train_data = pl.read_csv(\n",
    "    'data/optiver-realized-volatility-prediction/train.csv')\n",
    "\n",
    "time_length_list = [500,400,300,200,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2d7ff8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T19:38:32.638450Z",
     "iopub.status.busy": "2025-09-06T19:38:32.638126Z",
     "iopub.status.idle": "2025-09-06T19:40:22.611015Z",
     "shell.execute_reply": "2025-09-06T19:40:22.609922Z"
    },
    "papermill": {
     "duration": 109.986276,
     "end_time": "2025-09-06T19:40:22.612656",
     "exception": false,
     "start_time": "2025-09-06T19:38:32.626380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#list_book_test,list_trade_test,test_data = list_book_train,list_trade_train,train_data.clone().drop('target')\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df_train = \u001b[43mpreprocessor\u001b[49m(list_book_train, list_trade_train,train_data,time_length_list)\n\u001b[32m      4\u001b[39m df_test = preprocessor(list_book_test,list_trade_test,test_data,time_length_list,train=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'preprocessor' is not defined"
     ]
    }
   ],
   "source": [
    "#list_book_test,list_trade_test,test_data = list_book_train,list_trade_train,train_data.clone().drop('target')\n",
    "\n",
    "df_train = preprocessor(list_book_train, list_trade_train,train_data,time_length_list)\n",
    "df_test = preprocessor(list_book_test,list_trade_test,test_data,time_length_list,train=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d870e42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T19:40:22.645497Z",
     "iopub.status.busy": "2025-09-06T19:40:22.645157Z",
     "iopub.status.idle": "2025-09-06T19:40:22.652828Z",
     "shell.execute_reply": "2025-09-06T19:40:22.651974Z"
    },
    "papermill": {
     "duration": 0.025958,
     "end_time": "2025-09-06T19:40:22.654369",
     "exception": false,
     "start_time": "2025-09-06T19:40:22.628411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_time_stock(df):\n",
    "    vol_cols = ['log_return1_realized_vol', 'log_return2_realized_vol', 'log_return1_realized_vol_400', 'log_return2_realized_vol_400', \n",
    "                'log_return1_realized_vol_300', 'log_return2_realized_vol_300', 'log_return1_realized_vol_200', 'log_return2_realized_vol_200', \n",
    "                'trade_log_return_realized_vol', 'trade_log_return_realized_vol_400', 'trade_log_return_realized_vol_300', 'trade_log_return_realized_vol_200']\n",
    "\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
    "    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
    "    df_time_id = df_time_id.add_suffix('_' + 'time')\n",
    "    \n",
    "    # Merge with original dataframe\n",
    "    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n",
    "    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n",
    "    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19dfc92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T19:40:22.685958Z",
     "iopub.status.busy": "2025-09-06T19:40:22.685528Z",
     "iopub.status.idle": "2025-09-06T19:40:24.648909Z",
     "shell.execute_reply": "2025-09-06T19:40:24.647736Z"
    },
    "papermill": {
     "duration": 1.983391,
     "end_time": "2025-09-06T19:40:24.652720",
     "exception": false,
     "start_time": "2025-09-06T19:40:22.669329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train=get_time_stock(df_train)\n",
    "df_test=get_time_stock(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90824e2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T19:40:24.689136Z",
     "iopub.status.busy": "2025-09-06T19:40:24.688776Z",
     "iopub.status.idle": "2025-09-06T19:40:26.467509Z",
     "shell.execute_reply": "2025-09-06T19:40:26.466351Z"
    },
    "papermill": {
     "duration": 1.799228,
     "end_time": "2025-09-06T19:40:26.470717",
     "exception": false,
     "start_time": "2025-09-06T19:40:24.671489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>log_return1_realized_vol</th>\n",
       "      <th>log_return2_realized_vol</th>\n",
       "      <th>log_return1_wmp_realized_vol</th>\n",
       "      <th>log_return2_wmp_realized_vol</th>\n",
       "      <th>wap1_sum</th>\n",
       "      <th>wap1_std</th>\n",
       "      <th>wap2_sum</th>\n",
       "      <th>wap2_std</th>\n",
       "      <th>...</th>\n",
       "      <th>cluster_5_size_tau2</th>\n",
       "      <th>cluster_6_log_return1_realized_vol</th>\n",
       "      <th>cluster_6_total_volume_sum</th>\n",
       "      <th>cluster_6_size_sum</th>\n",
       "      <th>cluster_6_order_count_sum</th>\n",
       "      <th>cluster_6_price_spread_sum</th>\n",
       "      <th>cluster_6_bid_spread_sum</th>\n",
       "      <th>cluster_6_ask_spread_sum</th>\n",
       "      <th>cluster_6_volume_imbalance_sum</th>\n",
       "      <th>cluster_6_size_tau2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0-4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>3.001215</td>\n",
       "      <td>0.00017</td>\n",
       "      <td>3.00165</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0-32</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0-34</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 244 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  row_id  time_id  log_return1_realized_vol  log_return2_realized_vol  \\\n",
       "0    0-4        4                  0.000294                  0.000252   \n",
       "1   0-32       32                       NaN                       NaN   \n",
       "2   0-34       34                       NaN                       NaN   \n",
       "\n",
       "   log_return1_wmp_realized_vol  log_return2_wmp_realized_vol  wap1_sum  \\\n",
       "0                      0.000245                      0.000027  3.001215   \n",
       "1                           NaN                           NaN       NaN   \n",
       "2                           NaN                           NaN       NaN   \n",
       "\n",
       "   wap1_std  wap2_sum  wap2_std  ...  cluster_5_size_tau2  \\\n",
       "0   0.00017   3.00165  0.000153  ...                  NaN   \n",
       "1       NaN       NaN       NaN  ...                  NaN   \n",
       "2       NaN       NaN       NaN  ...                  NaN   \n",
       "\n",
       "   cluster_6_log_return1_realized_vol  cluster_6_total_volume_sum  \\\n",
       "0                                 NaN                         NaN   \n",
       "1                                 NaN                         NaN   \n",
       "2                                 NaN                         NaN   \n",
       "\n",
       "   cluster_6_size_sum  cluster_6_order_count_sum  cluster_6_price_spread_sum  \\\n",
       "0                 NaN                        NaN                         NaN   \n",
       "1                 NaN                        NaN                         NaN   \n",
       "2                 NaN                        NaN                         NaN   \n",
       "\n",
       "   cluster_6_bid_spread_sum  cluster_6_ask_spread_sum  \\\n",
       "0                       NaN                       NaN   \n",
       "1                       NaN                       NaN   \n",
       "2                       NaN                       NaN   \n",
       "\n",
       "   cluster_6_volume_imbalance_sum  cluster_6_size_tau2  \n",
       "0                             NaN                  NaN  \n",
       "1                             NaN                  NaN  \n",
       "2                             NaN                  NaN  \n",
       "\n",
       "[3 rows x 244 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['size_tau'] = np.sqrt( 1/ df_train['seconds_in_bucket_unique'] )\n",
    "df_test['size_tau'] = np.sqrt( 1/ df_test['seconds_in_bucket_unique'] )\n",
    "\n",
    "df_train['size_tau_400'] = np.sqrt( 1/ df_train['seconds_in_bucket_unique_400'] )\n",
    "df_test['size_tau_400'] = np.sqrt( 1/ df_test['seconds_in_bucket_unique_400'] )\n",
    "df_train['size_tau_300'] = np.sqrt( 1/ df_train['seconds_in_bucket_unique_300'] )\n",
    "df_test['size_tau_300'] = np.sqrt( 1/ df_test['seconds_in_bucket_unique_300'] )\n",
    "\n",
    "df_train['size_tau_200'] = np.sqrt( 1/ df_train['seconds_in_bucket_unique_200'] )\n",
    "df_test['size_tau_200'] = np.sqrt( 1/ df_test['seconds_in_bucket_unique_200'] )\n",
    "\n",
    "df_train['size_tau2'] = np.sqrt( 1/ df_train['order_count_sum'] )\n",
    "df_test['size_tau2'] = np.sqrt( 1/ df_test['order_count_sum'] )\n",
    "\n",
    "df_train['size_tau2_400'] = np.sqrt( 0.33/ df_train['order_count_sum'] )\n",
    "df_test['size_tau2_400'] = np.sqrt( 0.33/ df_test['order_count_sum'] )\n",
    "df_train['size_tau2_300'] = np.sqrt( 0.5/ df_train['order_count_sum'] )\n",
    "df_test['size_tau2_300'] = np.sqrt( 0.5/ df_test['order_count_sum'] )\n",
    "\n",
    "df_train['size_tau2_200'] = np.sqrt( 0.66/ df_train['order_count_sum'] )\n",
    "df_test['size_tau2_200'] = np.sqrt( 0.66/ df_test['order_count_sum'] )\n",
    "\n",
    "# delta tau\n",
    "df_train['size_tau2_d'] = df_train['size_tau2_400'] - df_train['size_tau2']\n",
    "df_test['size_tau2_d'] = df_test['size_tau2_400'] - df_test['size_tau2']\n",
    "\n",
    "\n",
    "train_cluster = generate_cluster_features(df_train, cluster_stocks).reset_index()\n",
    "train = pd.merge(df_train,train_cluster,how='left',on='time_id')\n",
    "test_cluster = generate_cluster_features(df_test, cluster_stocks).reset_index()\n",
    "df_test['time_id'] = df_test['row_id'].str.split(\"-\").str[1]\n",
    "df_test['time_id'] = df_test['time_id'].astype(int)\n",
    "test_cluster['time_id'] = test_cluster['time_id'].astype(int)\n",
    "\n",
    "test =  pd.merge(df_test,test_cluster,how='left',on='time_id')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aa8069",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T19:40:26.515323Z",
     "iopub.status.busy": "2025-09-06T19:40:26.514990Z",
     "iopub.status.idle": "2025-09-06T19:40:43.209816Z",
     "shell.execute_reply": "2025-09-06T19:40:43.207876Z"
    },
    "papermill": {
     "duration": 16.725722,
     "end_time": "2025-09-06T19:40:43.214949",
     "exception": false,
     "start_time": "2025-09-06T19:40:26.489227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nfolds = 5\n",
    "n_time = train_p.shape[0]\n",
    "distance_sum=[]\n",
    "distance_sum = [np.zeros(n_time-nfolds) for _ in range(nfolds)]\n",
    "selected_mat=[]\n",
    "train_p = train_p.fillna(train_p.mean())\n",
    "target_mat = train_p.values\n",
    "target_mat = MinMaxScaler(feature_range=(-1, 1)).fit_transform(target_mat)\n",
    "target_mat = np.c_[target_mat, range(n_time)]\n",
    "num_per_fold = int(n_time/nfolds)\n",
    "\n",
    "# Random select points to be the initial centers\n",
    "centers = np.random.choice(np.array(n_time), size=nfolds, replace=False)\n",
    "centers = np.sort(centers)[::-1]\n",
    "# The index of rows that have been considered\n",
    "ind_values = [[centers[i]] for i in range(nfolds)]\n",
    "\n",
    "for i in range(nfolds):\n",
    "    # The row that have been considered\n",
    "\n",
    "    selected_mat.append(target_mat[ind_values[i], :])\n",
    " \n",
    "    target_mat = np.delete(target_mat, obj=ind_values[i], axis=0)\n",
    "for i in range(num_per_fold):\n",
    "\n",
    "    for j in range(nfolds):\n",
    "        threshold = np.random.uniform(0, 1, 1)\n",
    "        # Match the size foe future matrix calculation\n",
    "       \n",
    "        selected_mat[j] = np.tile(selected_mat[j], (target_mat.shape[0], 1))\n",
    "        distance_sum[j] += np.sum((target_mat[:, :-1] -\n",
    "                               selected_mat[j][:, :-1])**2, axis=1)\n",
    "        prob_vec = distance_sum[j]/np.sum(distance_sum[j])\n",
    "        cum_prob = 0\n",
    "        line_idx = 0\n",
    "        for val in prob_vec:\n",
    "            cum_prob += val\n",
    "            if (cum_prob > threshold):  # the column was selected\n",
    "                break\n",
    "            line_idx += 1\n",
    "        cum_prob = 0\n",
    "\n",
    "        # Update parameters\n",
    "        \n",
    "        for n in range(nfolds):\n",
    "            distance_sum[n] = np.delete(distance_sum[n].copy(), line_idx)\n",
    "        centers[j] = line_idx\n",
    "        selected_mat[j] = target_mat[line_idx, :]\n",
    "        ind_values[j].append(target_mat[line_idx, -1])\n",
    "        target_mat = np.delete(target_mat, obj=line_idx, axis=0)\n",
    "  \n",
    "    if target_mat.shape[0]==0:\n",
    "        break\n",
    "    \n",
    "for ind in range(nfolds):\n",
    "    ind_values[ind] = train_p.index[[int(term) for term in ind_values[ind]]]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d065118",
   "metadata": {
    "papermill": {
     "duration": 0.020341,
     "end_time": "2025-09-06T19:40:43.265632",
     "exception": false,
     "start_time": "2025-09-06T19:40:43.245291",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2.Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3824c1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T19:40:43.306683Z",
     "iopub.status.busy": "2025-09-06T19:40:43.306169Z",
     "iopub.status.idle": "2025-09-06T19:40:58.026609Z",
     "shell.execute_reply": "2025-09-06T19:40:58.025564Z"
    },
    "papermill": {
     "duration": 14.743773,
     "end_time": "2025-09-06T19:40:58.028432",
     "exception": false,
     "start_time": "2025-09-06T19:40:43.284659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "import lightgbm as lgb\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d582f492",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T19:40:58.061945Z",
     "iopub.status.busy": "2025-09-06T19:40:58.060540Z",
     "iopub.status.idle": "2025-09-06T19:40:58.068110Z",
     "shell.execute_reply": "2025-09-06T19:40:58.066522Z"
    },
    "papermill": {
     "duration": 0.026174,
     "end_time": "2025-09-06T19:40:58.070494",
     "exception": false,
     "start_time": "2025-09-06T19:40:58.044320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rmspe(y_true, y_pred):\n",
    "    # Root mean squared percentage error\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "def RMSPELoss(y_pred, y_true):\n",
    "    return torch.sqrt(torch.mean( ((y_true - y_pred) / y_true) ** 2 )).clone()\n",
    "\n",
    "def rmspe_feval(y_pred, dataset):\n",
    "    y_true = dataset.get_label()\n",
    "\n",
    "    return 'RMSPE', round(rmspe(y_true = y_true, y_pred = y_pred),5), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597246e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T19:40:58.104527Z",
     "iopub.status.busy": "2025-09-06T19:40:58.104218Z",
     "iopub.status.idle": "2025-09-06T19:40:58.111244Z",
     "shell.execute_reply": "2025-09-06T19:40:58.110147Z"
    },
    "papermill": {
     "duration": 0.026258,
     "end_time": "2025-09-06T19:40:58.113039",
     "exception": false,
     "start_time": "2025-09-06T19:40:58.086781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"stock_id_target_mean = df_train.groupby('stock_id')['target'].mean() \\ndf_test['stock_id_target_enc'] = df_test['stock_id'].map(stock_id_target_mean) # test_set\\n\\ntmp = np.repeat(np.nan, df_train.shape[0])\\nkf = KFold(n_splits = 10, shuffle=True,random_state = 19911109)\\nfor idx_1, idx_2 in kf.split(df_train):\\n    target_mean = df_train.iloc[idx_1].groupby('stock_id')['target'].mean()\\n\\n    tmp[idx_2] = df_train['stock_id'].iloc[idx_2].map(target_mean)\\ndf_train['stock_id_target_enc'] = tmp\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''stock_id_target_mean = df_train.groupby('stock_id')['target'].mean() \n",
    "df_test['stock_id_target_enc'] = df_test['stock_id'].map(stock_id_target_mean) # test_set\n",
    "\n",
    "tmp = np.repeat(np.nan, df_train.shape[0])\n",
    "kf = KFold(n_splits = 10, shuffle=True,random_state = 19911109)\n",
    "for idx_1, idx_2 in kf.split(df_train):\n",
    "    target_mean = df_train.iloc[idx_1].groupby('stock_id')['target'].mean()\n",
    "\n",
    "    tmp[idx_2] = df_train['stock_id'].iloc[idx_2].map(target_mean)\n",
    "df_train['stock_id_target_enc'] = tmp'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15dce37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T19:40:58.148429Z",
     "iopub.status.busy": "2025-09-06T19:40:58.147388Z",
     "iopub.status.idle": "2025-09-06T20:04:20.967322Z",
     "shell.execute_reply": "2025-09-06T20:04:20.965936Z"
    },
    "papermill": {
     "duration": 1402.861092,
     "end_time": "2025-09-06T20:04:20.991124",
     "exception": false,
     "start_time": "2025-09-06T19:40:58.130032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012847 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 54115\n",
      "[LightGBM] [Info] Number of data points in the train set: 343145, number of used features: 242\n",
      "[LightGBM] [Info] Start training from score 0.001797\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's rmse: 0.000513589\ttraining's RMSPE: 0.23788\tvalid_1's rmse: 0.00053701\tvalid_1's RMSPE: 0.24774\n",
      "[200]\ttraining's rmse: 0.000462895\ttraining's RMSPE: 0.2144\tvalid_1's rmse: 0.000492626\tvalid_1's RMSPE: 0.22727\n",
      "[300]\ttraining's rmse: 0.000449824\ttraining's RMSPE: 0.20835\tvalid_1's rmse: 0.000480582\tvalid_1's RMSPE: 0.22171\n",
      "[400]\ttraining's rmse: 0.000441074\ttraining's RMSPE: 0.2043\tvalid_1's rmse: 0.000473008\tvalid_1's RMSPE: 0.21822\n",
      "[500]\ttraining's rmse: 0.000434233\ttraining's RMSPE: 0.20113\tvalid_1's rmse: 0.000467825\tvalid_1's RMSPE: 0.21583\n",
      "[600]\ttraining's rmse: 0.000428646\ttraining's RMSPE: 0.19854\tvalid_1's rmse: 0.000463431\tvalid_1's RMSPE: 0.2138\n",
      "[700]\ttraining's rmse: 0.000423842\ttraining's RMSPE: 0.19632\tvalid_1's rmse: 0.000459925\tvalid_1's RMSPE: 0.21218\n",
      "[800]\ttraining's rmse: 0.00041968\ttraining's RMSPE: 0.19439\tvalid_1's rmse: 0.000457358\tvalid_1's RMSPE: 0.211\n",
      "[900]\ttraining's rmse: 0.000415611\ttraining's RMSPE: 0.1925\tvalid_1's rmse: 0.000454357\tvalid_1's RMSPE: 0.20961\n",
      "[1000]\ttraining's rmse: 0.000411919\ttraining's RMSPE: 0.19079\tvalid_1's rmse: 0.0004517\tvalid_1's RMSPE: 0.20839\n",
      "[1100]\ttraining's rmse: 0.000408594\ttraining's RMSPE: 0.18925\tvalid_1's rmse: 0.000449788\tvalid_1's RMSPE: 0.2075\n",
      "[1200]\ttraining's rmse: 0.000405663\ttraining's RMSPE: 0.1879\tvalid_1's rmse: 0.00044853\tvalid_1's RMSPE: 0.20692\n",
      "[1300]\ttraining's rmse: 0.00040293\ttraining's RMSPE: 0.18663\tvalid_1's rmse: 0.000446699\tvalid_1's RMSPE: 0.20608\n",
      "[1400]\ttraining's rmse: 0.00040024\ttraining's RMSPE: 0.18538\tvalid_1's rmse: 0.000445039\tvalid_1's RMSPE: 0.20531\n",
      "[1500]\ttraining's rmse: 0.000397701\ttraining's RMSPE: 0.18421\tvalid_1's rmse: 0.000443061\tvalid_1's RMSPE: 0.2044\n",
      "[1600]\ttraining's rmse: 0.000395345\ttraining's RMSPE: 0.18312\tvalid_1's rmse: 0.000441609\tvalid_1's RMSPE: 0.20373\n",
      "[1700]\ttraining's rmse: 0.000393206\ttraining's RMSPE: 0.18213\tvalid_1's rmse: 0.000440709\tvalid_1's RMSPE: 0.20332\n",
      "[1800]\ttraining's rmse: 0.000391267\ttraining's RMSPE: 0.18123\tvalid_1's rmse: 0.000439936\tvalid_1's RMSPE: 0.20296\n",
      "[1900]\ttraining's rmse: 0.000389365\ttraining's RMSPE: 0.18035\tvalid_1's rmse: 0.000439099\tvalid_1's RMSPE: 0.20257\n",
      "[2000]\ttraining's rmse: 0.000387458\ttraining's RMSPE: 0.17946\tvalid_1's rmse: 0.000438322\tvalid_1's RMSPE: 0.20221\n",
      "[2100]\ttraining's rmse: 0.000385684\ttraining's RMSPE: 0.17864\tvalid_1's rmse: 0.000437417\tvalid_1's RMSPE: 0.2018\n",
      "[2200]\ttraining's rmse: 0.000383999\ttraining's RMSPE: 0.17786\tvalid_1's rmse: 0.000436812\tvalid_1's RMSPE: 0.20152\n",
      "[2300]\ttraining's rmse: 0.000382476\ttraining's RMSPE: 0.17716\tvalid_1's rmse: 0.000436209\tvalid_1's RMSPE: 0.20124\n",
      "[2400]\ttraining's rmse: 0.000380776\ttraining's RMSPE: 0.17637\tvalid_1's rmse: 0.000435414\tvalid_1's RMSPE: 0.20087\n",
      "[2500]\ttraining's rmse: 0.000379208\ttraining's RMSPE: 0.17564\tvalid_1's rmse: 0.000434838\tvalid_1's RMSPE: 0.20061\n",
      "[2600]\ttraining's rmse: 0.000377701\ttraining's RMSPE: 0.17494\tvalid_1's rmse: 0.000434322\tvalid_1's RMSPE: 0.20037\n",
      "[2700]\ttraining's rmse: 0.000376336\ttraining's RMSPE: 0.17431\tvalid_1's rmse: 0.000433817\tvalid_1's RMSPE: 0.20014\n",
      "[2800]\ttraining's rmse: 0.000375077\ttraining's RMSPE: 0.17373\tvalid_1's rmse: 0.000433399\tvalid_1's RMSPE: 0.19994\n",
      "[2900]\ttraining's rmse: 0.000373694\ttraining's RMSPE: 0.17309\tvalid_1's rmse: 0.000432873\tvalid_1's RMSPE: 0.1997\n",
      "[3000]\ttraining's rmse: 0.00037237\ttraining's RMSPE: 0.17247\tvalid_1's rmse: 0.000432271\tvalid_1's RMSPE: 0.19942\n",
      "[3100]\ttraining's rmse: 0.000371106\ttraining's RMSPE: 0.17189\tvalid_1's rmse: 0.000431966\tvalid_1's RMSPE: 0.19928\n",
      "[3200]\ttraining's rmse: 0.000369847\ttraining's RMSPE: 0.17131\tvalid_1's rmse: 0.000431544\tvalid_1's RMSPE: 0.19909\n",
      "[3300]\ttraining's rmse: 0.000368708\ttraining's RMSPE: 0.17078\tvalid_1's rmse: 0.000431389\tvalid_1's RMSPE: 0.19902\n",
      "[3400]\ttraining's rmse: 0.000367522\ttraining's RMSPE: 0.17023\tvalid_1's rmse: 0.000430953\tvalid_1's RMSPE: 0.19881\n",
      "[3500]\ttraining's rmse: 0.00036639\ttraining's RMSPE: 0.1697\tvalid_1's rmse: 0.000430638\tvalid_1's RMSPE: 0.19867\n",
      "[3600]\ttraining's rmse: 0.000365228\ttraining's RMSPE: 0.16917\tvalid_1's rmse: 0.000430482\tvalid_1's RMSPE: 0.1986\n",
      "Early stopping, best iteration is:\n",
      "[3557]\ttraining's rmse: 0.00036572\ttraining's RMSPE: 0.16939\tvalid_1's rmse: 0.000430187\tvalid_1's RMSPE: 0.19846\n",
      "Fold 2\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014989 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 54179\n",
      "[LightGBM] [Info] Number of data points in the train set: 343145, number of used features: 242\n",
      "[LightGBM] [Info] Start training from score 0.001802\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's rmse: 0.000512723\ttraining's RMSPE: 0.23719\tvalid_1's rmse: 0.00052299\tvalid_1's RMSPE: 0.24248\n",
      "[200]\ttraining's rmse: 0.000462228\ttraining's RMSPE: 0.21383\tvalid_1's rmse: 0.000473244\tvalid_1's RMSPE: 0.21941\n",
      "[300]\ttraining's rmse: 0.000449157\ttraining's RMSPE: 0.20778\tvalid_1's rmse: 0.000462295\tvalid_1's RMSPE: 0.21434\n",
      "[400]\ttraining's rmse: 0.000440628\ttraining's RMSPE: 0.20384\tvalid_1's rmse: 0.000455645\tvalid_1's RMSPE: 0.21125\n",
      "[500]\ttraining's rmse: 0.000433719\ttraining's RMSPE: 0.20064\tvalid_1's rmse: 0.000450639\tvalid_1's RMSPE: 0.20893\n",
      "[600]\ttraining's rmse: 0.000428056\ttraining's RMSPE: 0.19802\tvalid_1's rmse: 0.00044667\tvalid_1's RMSPE: 0.20709\n",
      "[700]\ttraining's rmse: 0.000423409\ttraining's RMSPE: 0.19587\tvalid_1's rmse: 0.000443887\tvalid_1's RMSPE: 0.2058\n",
      "[800]\ttraining's rmse: 0.000419107\ttraining's RMSPE: 0.19388\tvalid_1's rmse: 0.000441149\tvalid_1's RMSPE: 0.20453\n",
      "[900]\ttraining's rmse: 0.000414957\ttraining's RMSPE: 0.19196\tvalid_1's rmse: 0.000438305\tvalid_1's RMSPE: 0.20321\n",
      "[1000]\ttraining's rmse: 0.000411182\ttraining's RMSPE: 0.19022\tvalid_1's rmse: 0.000435956\tvalid_1's RMSPE: 0.20212\n",
      "[1100]\ttraining's rmse: 0.000408004\ttraining's RMSPE: 0.18875\tvalid_1's rmse: 0.000433936\tvalid_1's RMSPE: 0.20119\n",
      "[1200]\ttraining's rmse: 0.000404944\ttraining's RMSPE: 0.18733\tvalid_1's rmse: 0.000432981\tvalid_1's RMSPE: 0.20074\n",
      "[1300]\ttraining's rmse: 0.000402142\ttraining's RMSPE: 0.18603\tvalid_1's rmse: 0.000431455\tvalid_1's RMSPE: 0.20004\n",
      "[1400]\ttraining's rmse: 0.000399538\ttraining's RMSPE: 0.18483\tvalid_1's rmse: 0.000430553\tvalid_1's RMSPE: 0.19962\n",
      "[1500]\ttraining's rmse: 0.000396998\ttraining's RMSPE: 0.18365\tvalid_1's rmse: 0.00042931\tvalid_1's RMSPE: 0.19904\n",
      "[1600]\ttraining's rmse: 0.000394828\ttraining's RMSPE: 0.18265\tvalid_1's rmse: 0.000428241\tvalid_1's RMSPE: 0.19855\n",
      "[1700]\ttraining's rmse: 0.000392583\ttraining's RMSPE: 0.18161\tvalid_1's rmse: 0.000427327\tvalid_1's RMSPE: 0.19812\n",
      "[1800]\ttraining's rmse: 0.000390658\ttraining's RMSPE: 0.18072\tvalid_1's rmse: 0.000426577\tvalid_1's RMSPE: 0.19777\n",
      "[1900]\ttraining's rmse: 0.000388771\ttraining's RMSPE: 0.17985\tvalid_1's rmse: 0.000426097\tvalid_1's RMSPE: 0.19755\n",
      "[2000]\ttraining's rmse: 0.000386898\ttraining's RMSPE: 0.17898\tvalid_1's rmse: 0.000425755\tvalid_1's RMSPE: 0.19739\n",
      "[2100]\ttraining's rmse: 0.000385193\ttraining's RMSPE: 0.17819\tvalid_1's rmse: 0.000425377\tvalid_1's RMSPE: 0.19722\n",
      "[2200]\ttraining's rmse: 0.000383415\ttraining's RMSPE: 0.17737\tvalid_1's rmse: 0.000424759\tvalid_1's RMSPE: 0.19693\n",
      "[2300]\ttraining's rmse: 0.00038182\ttraining's RMSPE: 0.17663\tvalid_1's rmse: 0.000424421\tvalid_1's RMSPE: 0.19678\n",
      "[2400]\ttraining's rmse: 0.000380227\ttraining's RMSPE: 0.1759\tvalid_1's rmse: 0.000424049\tvalid_1's RMSPE: 0.1966\n",
      "[2500]\ttraining's rmse: 0.000378719\ttraining's RMSPE: 0.1752\tvalid_1's rmse: 0.000423616\tvalid_1's RMSPE: 0.1964\n",
      "Early stopping, best iteration is:\n",
      "[2474]\ttraining's rmse: 0.000379063\ttraining's RMSPE: 0.17536\tvalid_1's rmse: 0.000423573\tvalid_1's RMSPE: 0.19638\n",
      "Fold 3\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016514 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 54181\n",
      "[LightGBM] [Info] Number of data points in the train set: 343146, number of used features: 242\n",
      "[LightGBM] [Info] Start training from score 0.001804\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's rmse: 0.000513165\ttraining's RMSPE: 0.23712\tvalid_1's rmse: 0.000523838\tvalid_1's RMSPE: 0.24398\n",
      "[200]\ttraining's rmse: 0.000463056\ttraining's RMSPE: 0.21397\tvalid_1's rmse: 0.000473654\tvalid_1's RMSPE: 0.22061\n",
      "[300]\ttraining's rmse: 0.000449891\ttraining's RMSPE: 0.20788\tvalid_1's rmse: 0.000460974\tvalid_1's RMSPE: 0.2147\n",
      "[400]\ttraining's rmse: 0.000441227\ttraining's RMSPE: 0.20388\tvalid_1's rmse: 0.000453813\tvalid_1's RMSPE: 0.21137\n",
      "[500]\ttraining's rmse: 0.000434614\ttraining's RMSPE: 0.20082\tvalid_1's rmse: 0.000448723\tvalid_1's RMSPE: 0.20899\n",
      "[600]\ttraining's rmse: 0.000428852\ttraining's RMSPE: 0.19816\tvalid_1's rmse: 0.000444418\tvalid_1's RMSPE: 0.20699\n",
      "[700]\ttraining's rmse: 0.000423947\ttraining's RMSPE: 0.19589\tvalid_1's rmse: 0.000441375\tvalid_1's RMSPE: 0.20557\n",
      "[800]\ttraining's rmse: 0.000419646\ttraining's RMSPE: 0.19391\tvalid_1's rmse: 0.000438305\tvalid_1's RMSPE: 0.20414\n",
      "[900]\ttraining's rmse: 0.000415861\ttraining's RMSPE: 0.19216\tvalid_1's rmse: 0.000435944\tvalid_1's RMSPE: 0.20304\n",
      "[1000]\ttraining's rmse: 0.000412117\ttraining's RMSPE: 0.19043\tvalid_1's rmse: 0.000433616\tvalid_1's RMSPE: 0.20196\n",
      "[1100]\ttraining's rmse: 0.000408781\ttraining's RMSPE: 0.18889\tvalid_1's rmse: 0.000431577\tvalid_1's RMSPE: 0.20101\n",
      "[1200]\ttraining's rmse: 0.000405615\ttraining's RMSPE: 0.18742\tvalid_1's rmse: 0.000429598\tvalid_1's RMSPE: 0.20009\n",
      "[1300]\ttraining's rmse: 0.000402851\ttraining's RMSPE: 0.18615\tvalid_1's rmse: 0.000427877\tvalid_1's RMSPE: 0.19929\n",
      "[1400]\ttraining's rmse: 0.000400284\ttraining's RMSPE: 0.18496\tvalid_1's rmse: 0.000426307\tvalid_1's RMSPE: 0.19855\n",
      "[1500]\ttraining's rmse: 0.000397941\ttraining's RMSPE: 0.18388\tvalid_1's rmse: 0.000425067\tvalid_1's RMSPE: 0.19798\n",
      "[1600]\ttraining's rmse: 0.000395684\ttraining's RMSPE: 0.18284\tvalid_1's rmse: 0.000424012\tvalid_1's RMSPE: 0.19749\n",
      "[1700]\ttraining's rmse: 0.000393568\ttraining's RMSPE: 0.18186\tvalid_1's rmse: 0.000422972\tvalid_1's RMSPE: 0.197\n",
      "[1800]\ttraining's rmse: 0.000391528\ttraining's RMSPE: 0.18091\tvalid_1's rmse: 0.000421914\tvalid_1's RMSPE: 0.19651\n",
      "[1900]\ttraining's rmse: 0.000389524\ttraining's RMSPE: 0.17999\tvalid_1's rmse: 0.000420941\tvalid_1's RMSPE: 0.19606\n",
      "[2000]\ttraining's rmse: 0.00038772\ttraining's RMSPE: 0.17916\tvalid_1's rmse: 0.000420273\tvalid_1's RMSPE: 0.19574\n",
      "[2100]\ttraining's rmse: 0.000386026\ttraining's RMSPE: 0.17837\tvalid_1's rmse: 0.000419505\tvalid_1's RMSPE: 0.19539\n",
      "[2200]\ttraining's rmse: 0.000384264\ttraining's RMSPE: 0.17756\tvalid_1's rmse: 0.000418721\tvalid_1's RMSPE: 0.19502\n",
      "Early stopping, best iteration is:\n",
      "[2196]\ttraining's rmse: 0.000384315\ttraining's RMSPE: 0.17758\tvalid_1's rmse: 0.000418727\tvalid_1's RMSPE: 0.19502\n",
      "Fold 4\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016856 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 54178\n",
      "[LightGBM] [Info] Number of data points in the train set: 343146, number of used features: 242\n",
      "[LightGBM] [Info] Start training from score 0.001798\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's rmse: 0.000512135\ttraining's RMSPE: 0.23722\tvalid_1's rmse: 0.000519184\tvalid_1's RMSPE: 0.23948\n",
      "[200]\ttraining's rmse: 0.000461967\ttraining's RMSPE: 0.21398\tvalid_1's rmse: 0.000471469\tvalid_1's RMSPE: 0.21747\n",
      "[300]\ttraining's rmse: 0.000449154\ttraining's RMSPE: 0.20805\tvalid_1's rmse: 0.00046001\tvalid_1's RMSPE: 0.21219\n",
      "[400]\ttraining's rmse: 0.000440455\ttraining's RMSPE: 0.20402\tvalid_1's rmse: 0.000452632\tvalid_1's RMSPE: 0.20878\n",
      "[500]\ttraining's rmse: 0.000433816\ttraining's RMSPE: 0.20094\tvalid_1's rmse: 0.000447364\tvalid_1's RMSPE: 0.20635\n",
      "[600]\ttraining's rmse: 0.000427912\ttraining's RMSPE: 0.19821\tvalid_1's rmse: 0.000442779\tvalid_1's RMSPE: 0.20424\n",
      "[700]\ttraining's rmse: 0.000422994\ttraining's RMSPE: 0.19593\tvalid_1's rmse: 0.00043916\tvalid_1's RMSPE: 0.20257\n",
      "[800]\ttraining's rmse: 0.000418459\ttraining's RMSPE: 0.19383\tvalid_1's rmse: 0.000435755\tvalid_1's RMSPE: 0.201\n",
      "[900]\ttraining's rmse: 0.000414742\ttraining's RMSPE: 0.19211\tvalid_1's rmse: 0.000433234\tvalid_1's RMSPE: 0.19984\n",
      "[1000]\ttraining's rmse: 0.000411271\ttraining's RMSPE: 0.1905\tvalid_1's rmse: 0.000430899\tvalid_1's RMSPE: 0.19876\n",
      "[1100]\ttraining's rmse: 0.000407902\ttraining's RMSPE: 0.18894\tvalid_1's rmse: 0.0004286\tvalid_1's RMSPE: 0.1977\n",
      "[1200]\ttraining's rmse: 0.000404833\ttraining's RMSPE: 0.18752\tvalid_1's rmse: 0.000426732\tvalid_1's RMSPE: 0.19684\n",
      "[1300]\ttraining's rmse: 0.000402037\ttraining's RMSPE: 0.18622\tvalid_1's rmse: 0.000425029\tvalid_1's RMSPE: 0.19605\n",
      "[1400]\ttraining's rmse: 0.000399608\ttraining's RMSPE: 0.1851\tvalid_1's rmse: 0.000423793\tvalid_1's RMSPE: 0.19548\n",
      "[1500]\ttraining's rmse: 0.000397201\ttraining's RMSPE: 0.18398\tvalid_1's rmse: 0.000422432\tvalid_1's RMSPE: 0.19485\n",
      "[1600]\ttraining's rmse: 0.000394911\ttraining's RMSPE: 0.18292\tvalid_1's rmse: 0.0004213\tvalid_1's RMSPE: 0.19433\n",
      "[1700]\ttraining's rmse: 0.000392651\ttraining's RMSPE: 0.18188\tvalid_1's rmse: 0.000419991\tvalid_1's RMSPE: 0.19373\n",
      "[1800]\ttraining's rmse: 0.000390723\ttraining's RMSPE: 0.18098\tvalid_1's rmse: 0.000419214\tvalid_1's RMSPE: 0.19337\n",
      "[1900]\ttraining's rmse: 0.000388746\ttraining's RMSPE: 0.18007\tvalid_1's rmse: 0.000418123\tvalid_1's RMSPE: 0.19287\n",
      "[2000]\ttraining's rmse: 0.000386855\ttraining's RMSPE: 0.17919\tvalid_1's rmse: 0.000417249\tvalid_1's RMSPE: 0.19246\n",
      "[2100]\ttraining's rmse: 0.000385097\ttraining's RMSPE: 0.17838\tvalid_1's rmse: 0.000416538\tvalid_1's RMSPE: 0.19214\n",
      "[2200]\ttraining's rmse: 0.000383317\ttraining's RMSPE: 0.17755\tvalid_1's rmse: 0.000415662\tvalid_1's RMSPE: 0.19173\n",
      "[2300]\ttraining's rmse: 0.000381693\ttraining's RMSPE: 0.1768\tvalid_1's rmse: 0.000415243\tvalid_1's RMSPE: 0.19154\n",
      "[2400]\ttraining's rmse: 0.000380086\ttraining's RMSPE: 0.17606\tvalid_1's rmse: 0.000414599\tvalid_1's RMSPE: 0.19124\n",
      "[2500]\ttraining's rmse: 0.00037857\ttraining's RMSPE: 0.17535\tvalid_1's rmse: 0.000414056\tvalid_1's RMSPE: 0.19099\n",
      "[2600]\ttraining's rmse: 0.0003771\ttraining's RMSPE: 0.17467\tvalid_1's rmse: 0.000413405\tvalid_1's RMSPE: 0.19069\n",
      "[2700]\ttraining's rmse: 0.00037566\ttraining's RMSPE: 0.17401\tvalid_1's rmse: 0.000412999\tvalid_1's RMSPE: 0.1905\n",
      "[2800]\ttraining's rmse: 0.000374267\ttraining's RMSPE: 0.17336\tvalid_1's rmse: 0.000412479\tvalid_1's RMSPE: 0.19026\n",
      "[2900]\ttraining's rmse: 0.000372902\ttraining's RMSPE: 0.17273\tvalid_1's rmse: 0.000411943\tvalid_1's RMSPE: 0.19002\n",
      "[3000]\ttraining's rmse: 0.000371575\ttraining's RMSPE: 0.17211\tvalid_1's rmse: 0.000411467\tvalid_1's RMSPE: 0.1898\n",
      "[3100]\ttraining's rmse: 0.000370257\ttraining's RMSPE: 0.1715\tvalid_1's rmse: 0.000411054\tvalid_1's RMSPE: 0.18961\n",
      "[3200]\ttraining's rmse: 0.000368975\ttraining's RMSPE: 0.17091\tvalid_1's rmse: 0.000410783\tvalid_1's RMSPE: 0.18948\n",
      "[3300]\ttraining's rmse: 0.000367748\ttraining's RMSPE: 0.17034\tvalid_1's rmse: 0.000410591\tvalid_1's RMSPE: 0.18939\n",
      "Early stopping, best iteration is:\n",
      "[3279]\ttraining's rmse: 0.000367989\ttraining's RMSPE: 0.17045\tvalid_1's rmse: 0.00041053\tvalid_1's RMSPE: 0.18936\n",
      "Fold 5\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017972 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 54183\n",
      "[LightGBM] [Info] Number of data points in the train set: 343146, number of used features: 242\n",
      "[LightGBM] [Info] Start training from score 0.001800\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's rmse: 0.000513051\ttraining's RMSPE: 0.23754\tvalid_1's rmse: 0.000517479\tvalid_1's RMSPE: 0.2391\n",
      "[200]\ttraining's rmse: 0.000462309\ttraining's RMSPE: 0.21405\tvalid_1's rmse: 0.000470007\tvalid_1's RMSPE: 0.21717\n",
      "[300]\ttraining's rmse: 0.000449397\ttraining's RMSPE: 0.20807\tvalid_1's rmse: 0.000460252\tvalid_1's RMSPE: 0.21266\n",
      "[400]\ttraining's rmse: 0.000440793\ttraining's RMSPE: 0.20409\tvalid_1's rmse: 0.000454446\tvalid_1's RMSPE: 0.20998\n",
      "[500]\ttraining's rmse: 0.000434158\ttraining's RMSPE: 0.20102\tvalid_1's rmse: 0.000449971\tvalid_1's RMSPE: 0.20791\n",
      "[600]\ttraining's rmse: 0.000428517\ttraining's RMSPE: 0.1984\tvalid_1's rmse: 0.000446382\tvalid_1's RMSPE: 0.20625\n",
      "[700]\ttraining's rmse: 0.000423119\ttraining's RMSPE: 0.1959\tvalid_1's rmse: 0.000442722\tvalid_1's RMSPE: 0.20456\n",
      "[800]\ttraining's rmse: 0.00041881\ttraining's RMSPE: 0.19391\tvalid_1's rmse: 0.000440302\tvalid_1's RMSPE: 0.20344\n",
      "[900]\ttraining's rmse: 0.000414698\ttraining's RMSPE: 0.19201\tvalid_1's rmse: 0.00043771\tvalid_1's RMSPE: 0.20225\n",
      "[1000]\ttraining's rmse: 0.00041109\ttraining's RMSPE: 0.19034\tvalid_1's rmse: 0.000435586\tvalid_1's RMSPE: 0.20127\n",
      "[1100]\ttraining's rmse: 0.000407717\ttraining's RMSPE: 0.18877\tvalid_1's rmse: 0.000433984\tvalid_1's RMSPE: 0.20053\n",
      "[1200]\ttraining's rmse: 0.000404539\ttraining's RMSPE: 0.1873\tvalid_1's rmse: 0.000432163\tvalid_1's RMSPE: 0.19968\n",
      "[1300]\ttraining's rmse: 0.000401893\ttraining's RMSPE: 0.18608\tvalid_1's rmse: 0.000431246\tvalid_1's RMSPE: 0.19926\n",
      "[1400]\ttraining's rmse: 0.000399235\ttraining's RMSPE: 0.18485\tvalid_1's rmse: 0.000430027\tvalid_1's RMSPE: 0.1987\n",
      "[1500]\ttraining's rmse: 0.000396792\ttraining's RMSPE: 0.18372\tvalid_1's rmse: 0.000428936\tvalid_1's RMSPE: 0.19819\n",
      "[1600]\ttraining's rmse: 0.000394416\ttraining's RMSPE: 0.18262\tvalid_1's rmse: 0.000427772\tvalid_1's RMSPE: 0.19765\n",
      "[1700]\ttraining's rmse: 0.000392269\ttraining's RMSPE: 0.18162\tvalid_1's rmse: 0.000427053\tvalid_1's RMSPE: 0.19732\n",
      "[1800]\ttraining's rmse: 0.000390094\ttraining's RMSPE: 0.18061\tvalid_1's rmse: 0.000425954\tvalid_1's RMSPE: 0.19682\n",
      "[1900]\ttraining's rmse: 0.000388222\ttraining's RMSPE: 0.17975\tvalid_1's rmse: 0.000425321\tvalid_1's RMSPE: 0.19652\n",
      "[2000]\ttraining's rmse: 0.00038637\ttraining's RMSPE: 0.17889\tvalid_1's rmse: 0.000424779\tvalid_1's RMSPE: 0.19627\n",
      "[2100]\ttraining's rmse: 0.000384655\ttraining's RMSPE: 0.1781\tvalid_1's rmse: 0.000424218\tvalid_1's RMSPE: 0.19601\n",
      "[2200]\ttraining's rmse: 0.000382855\ttraining's RMSPE: 0.17726\tvalid_1's rmse: 0.00042364\tvalid_1's RMSPE: 0.19575\n",
      "Early stopping, best iteration is:\n",
      "[2199]\ttraining's rmse: 0.000382868\ttraining's RMSPE: 0.17727\tvalid_1's rmse: 0.000423633\tvalid_1's RMSPE: 0.19574\n",
      "Average RMSE: 0.19499\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"objective\": \"rmse\",\n",
    "    \"metric\": \"rmse\",\n",
    "   \n",
    "    \"boosting_type\": \"gbdt\",\n",
    "  #   'min_data_in_leaf':500,\n",
    "    'max_depth': -1,\n",
    "    'subsample_freq': 4,\n",
    " \n",
    "    'feature_fraction': 0.3,\n",
    "    'bagging_fraction': 0.75,\n",
    "    'learning_rate': 0.02,\n",
    "\n",
    "    'lambda_l1': 0.5,\n",
    "    'lambda_l2': 1.0,\n",
    "\n",
    "}\n",
    "def train_and_pred(train,test,params,n_splits = 5):\n",
    "    feature_importances = pd.DataFrame()\n",
    "    fold_scores=[]\n",
    "    \n",
    "    X = train.drop(['target'],axis=1)\n",
    "    y = train['target']\n",
    "    \n",
    "    kfold = KFold(n_splits , random_state = 42, shuffle = True)\n",
    "    test_pred= np.zeros(test.shape[0])\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X, y)):\n",
    "        print(f\"Fold {fold+1}\")\n",
    "        \n",
    "        X_train, y_train = X.loc[train_idx], y[train_idx]\n",
    "        X_val, y_val = X.loc[val_idx], y[val_idx]\n",
    "\n",
    "        weights_train = 1 / np.square(y_train.values)\n",
    "        weights_val = 1 / np.square(y_val.values)\n",
    "      \n",
    "        '''X_train_cl = generate_cluster_features(X_train, cluster_stocks)\n",
    "        X_val_cl = generate_cluster_features(X_val, cluster_stocks)\n",
    "        X_train =  pd.merge(X_train,X_train_cl,how='left',on='time_id')\n",
    "        X_val =  pd.merge(X_val,X_val_cl,how='left',on='time_id')\n",
    "        '''\n",
    "        X_train = X_train.drop(['row_id','time_id'],axis=1)\n",
    "        X_val = X_val.drop(['row_id','time_id'],axis=1)\n",
    "        features = X_train.columns.tolist()\n",
    "        lgb_train = lgb.Dataset(X_train, label=y_train, weight=weights_train,\n",
    "                                categorical_feature=['stock_id'])\n",
    "        lgb_val = lgb.Dataset(X_val, label=y_val, weight=weights_val,\n",
    "                            reference=lgb_train)\n",
    "        model = lgb.train(\n",
    "            params=params,\n",
    "            train_set=lgb_train,\n",
    "            num_boost_round=5000,\n",
    "            feval=rmspe_feval,\n",
    "            valid_sets=[lgb_train, lgb_val],\n",
    "            callbacks=[lgb.early_stopping(\n",
    "                stopping_rounds=50), lgb.log_evaluation(100)],\n",
    "\n",
    "        )  \n",
    "\n",
    "        y_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "   \n",
    "        test_pred += model.predict(test[features]) / 5\n",
    "        rmse = rmspe(y_true=y_val, y_pred=y_pred)\n",
    "        \n",
    "\n",
    "        fold_scores.append(round(rmse,5))\n",
    "\n",
    "        fold_importance = pd.DataFrame()\n",
    "        fold_importance[\"feature\"] = X_train.columns\n",
    "        fold_importance[\"importance\"] = model.feature_importance(\n",
    "            importance_type='split')\n",
    "        fold_importance[\"fold\"] = fold + 1\n",
    "\n",
    "        feature_importances = pd.concat(\n",
    "            [feature_importances, fold_importance], axis=0)\n",
    "\n",
    "    print(f\"Average RMSE: {np.mean(fold_scores):.5f}\")\n",
    "    return feature_importances, test_pred\n",
    "\n",
    "feature_importances,test_pred =  train_and_pred(train,test,params,n_splits = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d259c0",
   "metadata": {
    "papermill": {
     "duration": 0.024331,
     "end_time": "2025-09-06T20:04:21.038903",
     "exception": false,
     "start_time": "2025-09-06T20:04:21.014572",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1610afc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T20:04:21.088213Z",
     "iopub.status.busy": "2025-09-06T20:04:21.087437Z",
     "iopub.status.idle": "2025-09-06T20:04:21.093653Z",
     "shell.execute_reply": "2025-09-06T20:04:21.092656Z"
    },
    "papermill": {
     "duration": 0.03318,
     "end_time": "2025-09-06T20:04:21.095409",
     "exception": false,
     "start_time": "2025-09-06T20:04:21.062229",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mean_importance = feature_importances.groupby(\"feature\")[\"importance\"].mean().sort_values(ascending=False)\\n\\nsns.barplot(x=mean_importance[-20:],\\n    y=mean_importance.index[-20:])\\nplt.title(\\'Mean Feature Importance by Folds (least important)\\')\\nplt.show()'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''mean_importance = feature_importances.groupby(\"feature\")[\"importance\"].mean().sort_values(ascending=False)\n",
    "\n",
    "sns.barplot(x=mean_importance[-20:],\n",
    "    y=mean_importance.index[-20:])\n",
    "plt.title('Mean Feature Importance by Folds (least important)')\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cf97a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T20:04:21.145700Z",
     "iopub.status.busy": "2025-09-06T20:04:21.145367Z",
     "iopub.status.idle": "2025-09-06T20:04:21.151536Z",
     "shell.execute_reply": "2025-09-06T20:04:21.150536Z"
    },
    "papermill": {
     "duration": 0.03343,
     "end_time": "2025-09-06T20:04:21.153240",
     "exception": false,
     "start_time": "2025-09-06T20:04:21.119810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"sns.barplot(x=mean_importance[:20],\\n    y=mean_importance.index[:20])\\nplt.title('Mean Feature Importance by Folds (most important)')\\nplt.show()\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''sns.barplot(x=mean_importance[:20],\n",
    "    y=mean_importance.index[:20])\n",
    "plt.title('Mean Feature Importance by Folds (most important)')\n",
    "plt.show()'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0a4fb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T20:04:21.205742Z",
     "iopub.status.busy": "2025-09-06T20:04:21.204518Z",
     "iopub.status.idle": "2025-09-06T20:04:40.469893Z",
     "shell.execute_reply": "2025-09-06T20:04:40.468834Z"
    },
    "papermill": {
     "duration": 19.294234,
     "end_time": "2025-09-06T20:04:40.471742",
     "exception": false,
     "start_time": "2025-09-06T20:04:21.177508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "def rmse_keras(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square( (y_true - y_pred)/ y_true )))\n",
    "    \n",
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=20, verbose=0,\n",
    "    mode='min',restore_best_weights=True)\n",
    "\n",
    "plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.2, patience=7, verbose=0,\n",
    "    mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca580cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T20:04:40.525952Z",
     "iopub.status.busy": "2025-09-06T20:04:40.525048Z",
     "iopub.status.idle": "2025-09-06T20:04:40.530884Z",
     "shell.execute_reply": "2025-09-06T20:04:40.529912Z"
    },
    "papermill": {
     "duration": 0.034602,
     "end_time": "2025-09-06T20:04:40.532411",
     "exception": false,
     "start_time": "2025-09-06T20:04:40.497809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "colNames = df_train.columns.tolist()\n",
    "colNames.remove('row_id')\n",
    "colNames.remove('target')\n",
    "colNames.remove('time_id')\n",
    "colNames.remove('stock_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acca9c7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T20:04:40.584291Z",
     "iopub.status.busy": "2025-09-06T20:04:40.583954Z",
     "iopub.status.idle": "2025-09-06T20:05:03.393581Z",
     "shell.execute_reply": "2025-09-06T20:05:03.392515Z"
    },
    "papermill": {
     "duration": 22.838911,
     "end_time": "2025-09-06T20:05:03.395655",
     "exception": false,
     "start_time": "2025-09-06T20:04:40.556744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "qt_train = []\n",
    "\n",
    "train_nn=df_train[colNames].copy()\n",
    "test_nn=df_test[colNames].copy()\n",
    "\n",
    "for col in colNames:\n",
    "\n",
    "    qt = QuantileTransformer(random_state=42,n_quantiles=2000, output_distribution='normal')\n",
    "    train_nn[col] = qt.fit_transform(train_nn[[col]])\n",
    "    test_nn[col] = qt.transform(test_nn[[col]])    \n",
    "    qt_train.append(qt)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7d9f0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T20:05:03.448033Z",
     "iopub.status.busy": "2025-09-06T20:05:03.447081Z",
     "iopub.status.idle": "2025-09-06T20:05:04.818603Z",
     "shell.execute_reply": "2025-09-06T20:05:04.817574Z"
    },
    "papermill": {
     "duration": 1.399362,
     "end_time": "2025-09-06T20:05:04.820377",
     "exception": false,
     "start_time": "2025-09-06T20:05:03.421015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13/4193384786.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_nn[['stock_id','time_id','target']]=df_train[['stock_id','time_id','target']]\n",
      "/tmp/ipykernel_13/4193384786.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_nn[['stock_id','time_id','target']]=df_train[['stock_id','time_id','target']]\n",
      "/tmp/ipykernel_13/4193384786.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_nn[['stock_id','time_id','target']]=df_train[['stock_id','time_id','target']]\n",
      "/tmp/ipykernel_13/4193384786.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_nn[['stock_id','time_id']]=df_test[['stock_id','time_id']]\n",
      "/tmp/ipykernel_13/4193384786.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_nn[['stock_id','time_id']]=df_test[['stock_id','time_id']]\n"
     ]
    }
   ],
   "source": [
    "train_nn[['stock_id','time_id','target']]=df_train[['stock_id','time_id','target']]\n",
    "train_cluster = generate_cluster_features(df_train, cluster_stocks).reset_index()\n",
    "test_cluster = generate_cluster_features(df_test, cluster_stocks).reset_index()\n",
    "df_test['time_id'] = df_test['row_id'].str.split(\"-\").str[1]\n",
    "df_test['time_id'] = df_test['time_id'].astype(int)\n",
    "test_cluster['time_id'] = test_cluster['time_id'].astype(int)\n",
    "test_nn[['stock_id','time_id']]=df_test[['stock_id','time_id']]\n",
    "'''\n",
    "train_nn[['stock_id','time_id','target']]=df_train[['stock_id','time_id','target']]\n",
    "train_cluster = generate_cluster_features(df_train, cluster_stocks).reset_index()'''\n",
    "train_nn = pd.merge(train_nn,train_cluster,how='left',on='time_id')\n",
    "test_nn = pd.merge(test_nn,test_cluster,how='left',on='time_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a256db85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T20:05:04.871514Z",
     "iopub.status.busy": "2025-09-06T20:05:04.871158Z",
     "iopub.status.idle": "2025-09-06T20:05:04.899432Z",
     "shell.execute_reply": "2025-09-06T20:05:04.898468Z"
    },
    "papermill": {
     "duration": 0.055686,
     "end_time": "2025-09-06T20:05:04.901077",
     "exception": false,
     "start_time": "2025-09-06T20:05:04.845391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_id</th>\n",
       "      <th>cluster_0_log_return1_realized_vol</th>\n",
       "      <th>cluster_0_total_volume_sum</th>\n",
       "      <th>cluster_0_size_sum</th>\n",
       "      <th>cluster_0_order_count_sum</th>\n",
       "      <th>cluster_0_price_spread_sum</th>\n",
       "      <th>cluster_0_bid_spread_sum</th>\n",
       "      <th>cluster_0_ask_spread_sum</th>\n",
       "      <th>cluster_0_volume_imbalance_sum</th>\n",
       "      <th>cluster_0_size_tau2</th>\n",
       "      <th>...</th>\n",
       "      <th>cluster_5_size_tau2</th>\n",
       "      <th>cluster_6_log_return1_realized_vol</th>\n",
       "      <th>cluster_6_total_volume_sum</th>\n",
       "      <th>cluster_6_size_sum</th>\n",
       "      <th>cluster_6_order_count_sum</th>\n",
       "      <th>cluster_6_price_spread_sum</th>\n",
       "      <th>cluster_6_bid_spread_sum</th>\n",
       "      <th>cluster_6_ask_spread_sum</th>\n",
       "      <th>cluster_6_volume_imbalance_sum</th>\n",
       "      <th>cluster_6_size_tau2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.004872</td>\n",
       "      <td>243938.84</td>\n",
       "      <td>34805.44</td>\n",
       "      <td>606.36</td>\n",
       "      <td>0.264870</td>\n",
       "      <td>0.054052</td>\n",
       "      <td>-0.055414</td>\n",
       "      <td>99555.00</td>\n",
       "      <td>0.055045</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060859</td>\n",
       "      <td>0.004564</td>\n",
       "      <td>3.456186e+06</td>\n",
       "      <td>37392.944444</td>\n",
       "      <td>311.500000</td>\n",
       "      <td>0.266546</td>\n",
       "      <td>0.097849</td>\n",
       "      <td>-0.105899</td>\n",
       "      <td>8.243767e+05</td>\n",
       "      <td>0.070290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.003074</td>\n",
       "      <td>290943.32</td>\n",
       "      <td>11826.40</td>\n",
       "      <td>226.68</td>\n",
       "      <td>0.116857</td>\n",
       "      <td>0.036446</td>\n",
       "      <td>-0.036836</td>\n",
       "      <td>85056.28</td>\n",
       "      <td>0.080612</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086877</td>\n",
       "      <td>0.003627</td>\n",
       "      <td>5.159010e+06</td>\n",
       "      <td>12386.777778</td>\n",
       "      <td>157.388889</td>\n",
       "      <td>0.141960</td>\n",
       "      <td>0.075472</td>\n",
       "      <td>-0.075217</td>\n",
       "      <td>1.319986e+06</td>\n",
       "      <td>0.100094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>0.002754</td>\n",
       "      <td>393005.60</td>\n",
       "      <td>12940.56</td>\n",
       "      <td>190.80</td>\n",
       "      <td>0.102819</td>\n",
       "      <td>0.041222</td>\n",
       "      <td>-0.040275</td>\n",
       "      <td>109140.72</td>\n",
       "      <td>0.088701</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085441</td>\n",
       "      <td>0.003565</td>\n",
       "      <td>6.443588e+06</td>\n",
       "      <td>15469.722222</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>0.172422</td>\n",
       "      <td>0.100452</td>\n",
       "      <td>-0.097067</td>\n",
       "      <td>1.649668e+06</td>\n",
       "      <td>0.111269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>0.002709</td>\n",
       "      <td>283044.96</td>\n",
       "      <td>10313.44</td>\n",
       "      <td>191.08</td>\n",
       "      <td>0.129462</td>\n",
       "      <td>0.035683</td>\n",
       "      <td>-0.036416</td>\n",
       "      <td>88984.08</td>\n",
       "      <td>0.092025</td>\n",
       "      <td>...</td>\n",
       "      <td>0.093941</td>\n",
       "      <td>0.003611</td>\n",
       "      <td>3.376000e+06</td>\n",
       "      <td>17120.888889</td>\n",
       "      <td>146.666667</td>\n",
       "      <td>0.152984</td>\n",
       "      <td>0.081375</td>\n",
       "      <td>-0.083333</td>\n",
       "      <td>6.566752e+05</td>\n",
       "      <td>0.098094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62</td>\n",
       "      <td>0.002379</td>\n",
       "      <td>275719.00</td>\n",
       "      <td>9913.92</td>\n",
       "      <td>191.32</td>\n",
       "      <td>0.098860</td>\n",
       "      <td>0.034108</td>\n",
       "      <td>-0.032763</td>\n",
       "      <td>86559.32</td>\n",
       "      <td>0.090439</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090875</td>\n",
       "      <td>0.002842</td>\n",
       "      <td>4.570958e+06</td>\n",
       "      <td>14141.277778</td>\n",
       "      <td>131.111111</td>\n",
       "      <td>0.132486</td>\n",
       "      <td>0.072047</td>\n",
       "      <td>-0.073787</td>\n",
       "      <td>1.017866e+06</td>\n",
       "      <td>0.105215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3825</th>\n",
       "      <td>32751</td>\n",
       "      <td>0.003303</td>\n",
       "      <td>338324.60</td>\n",
       "      <td>15640.00</td>\n",
       "      <td>336.32</td>\n",
       "      <td>0.163817</td>\n",
       "      <td>0.042633</td>\n",
       "      <td>-0.043783</td>\n",
       "      <td>148267.72</td>\n",
       "      <td>0.065250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067603</td>\n",
       "      <td>0.003442</td>\n",
       "      <td>4.871410e+06</td>\n",
       "      <td>28828.833333</td>\n",
       "      <td>244.222222</td>\n",
       "      <td>0.175172</td>\n",
       "      <td>0.090258</td>\n",
       "      <td>-0.095934</td>\n",
       "      <td>7.220615e+05</td>\n",
       "      <td>0.084764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3826</th>\n",
       "      <td>32753</td>\n",
       "      <td>0.003080</td>\n",
       "      <td>346881.64</td>\n",
       "      <td>16640.52</td>\n",
       "      <td>351.96</td>\n",
       "      <td>0.098663</td>\n",
       "      <td>0.032366</td>\n",
       "      <td>-0.033313</td>\n",
       "      <td>103372.36</td>\n",
       "      <td>0.072102</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070870</td>\n",
       "      <td>0.003170</td>\n",
       "      <td>6.858452e+06</td>\n",
       "      <td>18563.111111</td>\n",
       "      <td>229.555556</td>\n",
       "      <td>0.141583</td>\n",
       "      <td>0.082203</td>\n",
       "      <td>-0.079893</td>\n",
       "      <td>1.640696e+06</td>\n",
       "      <td>0.080604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3827</th>\n",
       "      <td>32758</td>\n",
       "      <td>0.002817</td>\n",
       "      <td>309286.04</td>\n",
       "      <td>11447.68</td>\n",
       "      <td>185.52</td>\n",
       "      <td>0.138975</td>\n",
       "      <td>0.038987</td>\n",
       "      <td>-0.039310</td>\n",
       "      <td>88230.84</td>\n",
       "      <td>0.102183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099296</td>\n",
       "      <td>0.003202</td>\n",
       "      <td>6.210604e+06</td>\n",
       "      <td>21978.388889</td>\n",
       "      <td>182.444444</td>\n",
       "      <td>0.168502</td>\n",
       "      <td>0.091938</td>\n",
       "      <td>-0.094530</td>\n",
       "      <td>1.979517e+06</td>\n",
       "      <td>0.112833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3828</th>\n",
       "      <td>32763</td>\n",
       "      <td>0.003409</td>\n",
       "      <td>589748.32</td>\n",
       "      <td>35193.24</td>\n",
       "      <td>462.52</td>\n",
       "      <td>0.136002</td>\n",
       "      <td>0.059267</td>\n",
       "      <td>-0.060566</td>\n",
       "      <td>158674.56</td>\n",
       "      <td>0.054565</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055242</td>\n",
       "      <td>0.004474</td>\n",
       "      <td>1.015840e+07</td>\n",
       "      <td>57377.166667</td>\n",
       "      <td>337.611111</td>\n",
       "      <td>0.213039</td>\n",
       "      <td>0.119058</td>\n",
       "      <td>-0.122507</td>\n",
       "      <td>3.010704e+06</td>\n",
       "      <td>0.061386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3829</th>\n",
       "      <td>32767</td>\n",
       "      <td>0.003527</td>\n",
       "      <td>511149.52</td>\n",
       "      <td>13720.60</td>\n",
       "      <td>204.40</td>\n",
       "      <td>0.096679</td>\n",
       "      <td>0.040731</td>\n",
       "      <td>-0.039608</td>\n",
       "      <td>121643.84</td>\n",
       "      <td>0.082558</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083975</td>\n",
       "      <td>0.003309</td>\n",
       "      <td>8.239631e+06</td>\n",
       "      <td>31303.833333</td>\n",
       "      <td>152.222222</td>\n",
       "      <td>0.148841</td>\n",
       "      <td>0.091238</td>\n",
       "      <td>-0.097222</td>\n",
       "      <td>8.819613e+05</td>\n",
       "      <td>0.097424</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3830 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      time_id  cluster_0_log_return1_realized_vol  cluster_0_total_volume_sum  \\\n",
       "0           5                            0.004872                   243938.84   \n",
       "1          11                            0.003074                   290943.32   \n",
       "2          16                            0.002754                   393005.60   \n",
       "3          31                            0.002709                   283044.96   \n",
       "4          62                            0.002379                   275719.00   \n",
       "...       ...                                 ...                         ...   \n",
       "3825    32751                            0.003303                   338324.60   \n",
       "3826    32753                            0.003080                   346881.64   \n",
       "3827    32758                            0.002817                   309286.04   \n",
       "3828    32763                            0.003409                   589748.32   \n",
       "3829    32767                            0.003527                   511149.52   \n",
       "\n",
       "      cluster_0_size_sum  cluster_0_order_count_sum  \\\n",
       "0               34805.44                     606.36   \n",
       "1               11826.40                     226.68   \n",
       "2               12940.56                     190.80   \n",
       "3               10313.44                     191.08   \n",
       "4                9913.92                     191.32   \n",
       "...                  ...                        ...   \n",
       "3825            15640.00                     336.32   \n",
       "3826            16640.52                     351.96   \n",
       "3827            11447.68                     185.52   \n",
       "3828            35193.24                     462.52   \n",
       "3829            13720.60                     204.40   \n",
       "\n",
       "      cluster_0_price_spread_sum  cluster_0_bid_spread_sum  \\\n",
       "0                       0.264870                  0.054052   \n",
       "1                       0.116857                  0.036446   \n",
       "2                       0.102819                  0.041222   \n",
       "3                       0.129462                  0.035683   \n",
       "4                       0.098860                  0.034108   \n",
       "...                          ...                       ...   \n",
       "3825                    0.163817                  0.042633   \n",
       "3826                    0.098663                  0.032366   \n",
       "3827                    0.138975                  0.038987   \n",
       "3828                    0.136002                  0.059267   \n",
       "3829                    0.096679                  0.040731   \n",
       "\n",
       "      cluster_0_ask_spread_sum  cluster_0_volume_imbalance_sum  \\\n",
       "0                    -0.055414                        99555.00   \n",
       "1                    -0.036836                        85056.28   \n",
       "2                    -0.040275                       109140.72   \n",
       "3                    -0.036416                        88984.08   \n",
       "4                    -0.032763                        86559.32   \n",
       "...                        ...                             ...   \n",
       "3825                 -0.043783                       148267.72   \n",
       "3826                 -0.033313                       103372.36   \n",
       "3827                 -0.039310                        88230.84   \n",
       "3828                 -0.060566                       158674.56   \n",
       "3829                 -0.039608                       121643.84   \n",
       "\n",
       "      cluster_0_size_tau2  ...  cluster_5_size_tau2  \\\n",
       "0                0.055045  ...             0.060859   \n",
       "1                0.080612  ...             0.086877   \n",
       "2                0.088701  ...             0.085441   \n",
       "3                0.092025  ...             0.093941   \n",
       "4                0.090439  ...             0.090875   \n",
       "...                   ...  ...                  ...   \n",
       "3825             0.065250  ...             0.067603   \n",
       "3826             0.072102  ...             0.070870   \n",
       "3827             0.102183  ...             0.099296   \n",
       "3828             0.054565  ...             0.055242   \n",
       "3829             0.082558  ...             0.083975   \n",
       "\n",
       "      cluster_6_log_return1_realized_vol  cluster_6_total_volume_sum  \\\n",
       "0                               0.004564                3.456186e+06   \n",
       "1                               0.003627                5.159010e+06   \n",
       "2                               0.003565                6.443588e+06   \n",
       "3                               0.003611                3.376000e+06   \n",
       "4                               0.002842                4.570958e+06   \n",
       "...                                  ...                         ...   \n",
       "3825                            0.003442                4.871410e+06   \n",
       "3826                            0.003170                6.858452e+06   \n",
       "3827                            0.003202                6.210604e+06   \n",
       "3828                            0.004474                1.015840e+07   \n",
       "3829                            0.003309                8.239631e+06   \n",
       "\n",
       "      cluster_6_size_sum  cluster_6_order_count_sum  \\\n",
       "0           37392.944444                 311.500000   \n",
       "1           12386.777778                 157.388889   \n",
       "2           15469.722222                 146.000000   \n",
       "3           17120.888889                 146.666667   \n",
       "4           14141.277778                 131.111111   \n",
       "...                  ...                        ...   \n",
       "3825        28828.833333                 244.222222   \n",
       "3826        18563.111111                 229.555556   \n",
       "3827        21978.388889                 182.444444   \n",
       "3828        57377.166667                 337.611111   \n",
       "3829        31303.833333                 152.222222   \n",
       "\n",
       "      cluster_6_price_spread_sum  cluster_6_bid_spread_sum  \\\n",
       "0                       0.266546                  0.097849   \n",
       "1                       0.141960                  0.075472   \n",
       "2                       0.172422                  0.100452   \n",
       "3                       0.152984                  0.081375   \n",
       "4                       0.132486                  0.072047   \n",
       "...                          ...                       ...   \n",
       "3825                    0.175172                  0.090258   \n",
       "3826                    0.141583                  0.082203   \n",
       "3827                    0.168502                  0.091938   \n",
       "3828                    0.213039                  0.119058   \n",
       "3829                    0.148841                  0.091238   \n",
       "\n",
       "      cluster_6_ask_spread_sum  cluster_6_volume_imbalance_sum  \\\n",
       "0                    -0.105899                    8.243767e+05   \n",
       "1                    -0.075217                    1.319986e+06   \n",
       "2                    -0.097067                    1.649668e+06   \n",
       "3                    -0.083333                    6.566752e+05   \n",
       "4                    -0.073787                    1.017866e+06   \n",
       "...                        ...                             ...   \n",
       "3825                 -0.095934                    7.220615e+05   \n",
       "3826                 -0.079893                    1.640696e+06   \n",
       "3827                 -0.094530                    1.979517e+06   \n",
       "3828                 -0.122507                    3.010704e+06   \n",
       "3829                 -0.097222                    8.819613e+05   \n",
       "\n",
       "      cluster_6_size_tau2  \n",
       "0                0.070290  \n",
       "1                0.100094  \n",
       "2                0.111269  \n",
       "3                0.098094  \n",
       "4                0.105215  \n",
       "...                   ...  \n",
       "3825             0.084764  \n",
       "3826             0.080604  \n",
       "3827             0.112833  \n",
       "3828             0.061386  \n",
       "3829             0.097424  \n",
       "\n",
       "[3830 rows x 64 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d7acc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T20:05:04.952659Z",
     "iopub.status.busy": "2025-09-06T20:05:04.952305Z",
     "iopub.status.idle": "2025-09-06T20:05:04.967898Z",
     "shell.execute_reply": "2025-09-06T20:05:04.966900Z"
    },
    "papermill": {
     "duration": 0.043457,
     "end_time": "2025-09-06T20:05:04.969815",
     "exception": false,
     "start_time": "2025-09-06T20:05:04.926358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def swish(x, beta = 1):\n",
    "    return (x * K.sigmoid(beta * x))\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "\n",
    "get_custom_objects().update({'swish': Activation(swish)})\n",
    "\n",
    "hidden_units = (128,64,32)\n",
    "stock_embedding_size = 24\n",
    "cat_data = train_nn['stock_id']\n",
    "\n",
    "def base_model():\n",
    "    \n",
    "    # Each instance will consist of two inputs: a single user id, and a single movie id\n",
    "    stock_id_input = keras.Input(shape=(1,), name='stock_id')\n",
    "    num_input = keras.Input(shape=(242,), name='num_data')\n",
    "\n",
    "\n",
    "    #embedding, flatenning and concatenating\n",
    "    stock_embedded = keras.layers.Embedding(max(cat_data)+1, stock_embedding_size, \n",
    "                                           input_length=1, name='stock_embedding')(stock_id_input)\n",
    "    stock_flattened = keras.layers.Flatten()(stock_embedded)\n",
    "    out = keras.layers.Concatenate()([stock_flattened, num_input])\n",
    "    \n",
    "    # Add one or more hidden layers\n",
    "    for n_hidden in hidden_units:\n",
    "\n",
    "        out = keras.layers.Dense(n_hidden, activation='swish')(out)\n",
    "        \n",
    "\n",
    "    #out = keras.layers.Concatenate()([out, num_input])\n",
    "\n",
    "    # A single output: our predicted rating\n",
    "    out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n",
    "    \n",
    "    model = keras.Model(\n",
    "    inputs = [stock_id_input, num_input],\n",
    "    outputs = out,\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1abff9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T20:05:05.022937Z",
     "iopub.status.busy": "2025-09-06T20:05:05.022511Z",
     "iopub.status.idle": "2025-09-06T20:05:05.100020Z",
     "shell.execute_reply": "2025-09-06T20:05:05.099019Z"
    },
    "papermill": {
     "duration": 0.105605,
     "end_time": "2025-09-06T20:05:05.101628",
     "exception": false,
     "start_time": "2025-09-06T20:05:04.996023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_return1_realized_vol</th>\n",
       "      <th>log_return2_realized_vol</th>\n",
       "      <th>log_return1_wmp_realized_vol</th>\n",
       "      <th>log_return2_wmp_realized_vol</th>\n",
       "      <th>wap1_sum</th>\n",
       "      <th>wap1_std</th>\n",
       "      <th>wap2_sum</th>\n",
       "      <th>wap2_std</th>\n",
       "      <th>wmp1_sum</th>\n",
       "      <th>wmp1_std</th>\n",
       "      <th>...</th>\n",
       "      <th>cluster_5_size_tau2</th>\n",
       "      <th>cluster_6_log_return1_realized_vol</th>\n",
       "      <th>cluster_6_total_volume_sum</th>\n",
       "      <th>cluster_6_size_sum</th>\n",
       "      <th>cluster_6_order_count_sum</th>\n",
       "      <th>cluster_6_price_spread_sum</th>\n",
       "      <th>cluster_6_bid_spread_sum</th>\n",
       "      <th>cluster_6_ask_spread_sum</th>\n",
       "      <th>cluster_6_volume_imbalance_sum</th>\n",
       "      <th>cluster_6_size_tau2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.003362</td>\n",
       "      <td>0.354438</td>\n",
       "      <td>0.247420</td>\n",
       "      <td>0.187837</td>\n",
       "      <td>-0.472177</td>\n",
       "      <td>-0.211403</td>\n",
       "      <td>-0.472481</td>\n",
       "      <td>-0.119330</td>\n",
       "      <td>-0.472139</td>\n",
       "      <td>-0.311462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060859</td>\n",
       "      <td>0.004564</td>\n",
       "      <td>3.456186e+06</td>\n",
       "      <td>37392.944444</td>\n",
       "      <td>311.500000</td>\n",
       "      <td>0.266546</td>\n",
       "      <td>0.097849</td>\n",
       "      <td>-0.105899</td>\n",
       "      <td>8.243767e+05</td>\n",
       "      <td>0.070290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.201664</td>\n",
       "      <td>-0.297873</td>\n",
       "      <td>-0.304567</td>\n",
       "      <td>-0.530011</td>\n",
       "      <td>-1.341469</td>\n",
       "      <td>-1.683361</td>\n",
       "      <td>-1.341531</td>\n",
       "      <td>-1.764650</td>\n",
       "      <td>-1.341741</td>\n",
       "      <td>-1.469609</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086877</td>\n",
       "      <td>0.003627</td>\n",
       "      <td>5.159010e+06</td>\n",
       "      <td>12386.777778</td>\n",
       "      <td>157.388889</td>\n",
       "      <td>0.141960</td>\n",
       "      <td>0.075472</td>\n",
       "      <td>-0.075217</td>\n",
       "      <td>1.319986e+06</td>\n",
       "      <td>0.100094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.964602</td>\n",
       "      <td>-0.187208</td>\n",
       "      <td>-0.949768</td>\n",
       "      <td>-0.236531</td>\n",
       "      <td>-1.486974</td>\n",
       "      <td>0.092040</td>\n",
       "      <td>-1.486544</td>\n",
       "      <td>0.029785</td>\n",
       "      <td>-1.486964</td>\n",
       "      <td>-0.243400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085441</td>\n",
       "      <td>0.003565</td>\n",
       "      <td>6.443588e+06</td>\n",
       "      <td>15469.722222</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>0.172422</td>\n",
       "      <td>0.100452</td>\n",
       "      <td>-0.097067</td>\n",
       "      <td>1.649668e+06</td>\n",
       "      <td>0.111269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.286467</td>\n",
       "      <td>-0.499593</td>\n",
       "      <td>-0.705922</td>\n",
       "      <td>-0.475682</td>\n",
       "      <td>-2.485702</td>\n",
       "      <td>-0.097618</td>\n",
       "      <td>-2.485836</td>\n",
       "      <td>-0.361858</td>\n",
       "      <td>-2.485239</td>\n",
       "      <td>-0.380944</td>\n",
       "      <td>...</td>\n",
       "      <td>0.093941</td>\n",
       "      <td>0.003611</td>\n",
       "      <td>3.376000e+06</td>\n",
       "      <td>17120.888889</td>\n",
       "      <td>146.666667</td>\n",
       "      <td>0.152984</td>\n",
       "      <td>0.081375</td>\n",
       "      <td>-0.083333</td>\n",
       "      <td>6.566752e+05</td>\n",
       "      <td>0.098094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.523530</td>\n",
       "      <td>-0.537201</td>\n",
       "      <td>-1.122709</td>\n",
       "      <td>-0.692663</td>\n",
       "      <td>-1.630097</td>\n",
       "      <td>-1.714946</td>\n",
       "      <td>-1.629972</td>\n",
       "      <td>-1.534265</td>\n",
       "      <td>-1.629997</td>\n",
       "      <td>-1.991143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090875</td>\n",
       "      <td>0.002842</td>\n",
       "      <td>4.570958e+06</td>\n",
       "      <td>14141.277778</td>\n",
       "      <td>131.111111</td>\n",
       "      <td>0.132486</td>\n",
       "      <td>0.072047</td>\n",
       "      <td>-0.073787</td>\n",
       "      <td>1.017866e+06</td>\n",
       "      <td>0.105215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428927</th>\n",
       "      <td>-0.231039</td>\n",
       "      <td>0.096767</td>\n",
       "      <td>0.223575</td>\n",
       "      <td>0.322715</td>\n",
       "      <td>-0.428408</td>\n",
       "      <td>-0.720561</td>\n",
       "      <td>-0.428452</td>\n",
       "      <td>-0.458513</td>\n",
       "      <td>-0.429117</td>\n",
       "      <td>-0.404787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067603</td>\n",
       "      <td>0.003442</td>\n",
       "      <td>4.871410e+06</td>\n",
       "      <td>28828.833333</td>\n",
       "      <td>244.222222</td>\n",
       "      <td>0.175172</td>\n",
       "      <td>0.090258</td>\n",
       "      <td>-0.095934</td>\n",
       "      <td>7.220615e+05</td>\n",
       "      <td>0.084764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428928</th>\n",
       "      <td>-0.030576</td>\n",
       "      <td>-0.046142</td>\n",
       "      <td>0.011094</td>\n",
       "      <td>-0.112112</td>\n",
       "      <td>-1.105433</td>\n",
       "      <td>0.581081</td>\n",
       "      <td>-1.105135</td>\n",
       "      <td>0.576590</td>\n",
       "      <td>-1.106006</td>\n",
       "      <td>0.663268</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070870</td>\n",
       "      <td>0.003170</td>\n",
       "      <td>6.858452e+06</td>\n",
       "      <td>18563.111111</td>\n",
       "      <td>229.555556</td>\n",
       "      <td>0.141583</td>\n",
       "      <td>0.082203</td>\n",
       "      <td>-0.079893</td>\n",
       "      <td>1.640696e+06</td>\n",
       "      <td>0.080604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428929</th>\n",
       "      <td>0.314005</td>\n",
       "      <td>0.573072</td>\n",
       "      <td>0.513883</td>\n",
       "      <td>0.413298</td>\n",
       "      <td>-0.817845</td>\n",
       "      <td>-0.788789</td>\n",
       "      <td>-0.818202</td>\n",
       "      <td>-0.493011</td>\n",
       "      <td>-0.818887</td>\n",
       "      <td>-0.599950</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099296</td>\n",
       "      <td>0.003202</td>\n",
       "      <td>6.210604e+06</td>\n",
       "      <td>21978.388889</td>\n",
       "      <td>182.444444</td>\n",
       "      <td>0.168502</td>\n",
       "      <td>0.091938</td>\n",
       "      <td>-0.094530</td>\n",
       "      <td>1.979517e+06</td>\n",
       "      <td>0.112833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428930</th>\n",
       "      <td>-0.104948</td>\n",
       "      <td>0.078398</td>\n",
       "      <td>-0.381851</td>\n",
       "      <td>-0.078000</td>\n",
       "      <td>0.064013</td>\n",
       "      <td>-0.824915</td>\n",
       "      <td>0.063978</td>\n",
       "      <td>-0.749835</td>\n",
       "      <td>0.064233</td>\n",
       "      <td>-0.901647</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055242</td>\n",
       "      <td>0.004474</td>\n",
       "      <td>1.015840e+07</td>\n",
       "      <td>57377.166667</td>\n",
       "      <td>337.611111</td>\n",
       "      <td>0.213039</td>\n",
       "      <td>0.119058</td>\n",
       "      <td>-0.122507</td>\n",
       "      <td>3.010704e+06</td>\n",
       "      <td>0.061386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428931</th>\n",
       "      <td>-1.160234</td>\n",
       "      <td>-0.939174</td>\n",
       "      <td>-1.340337</td>\n",
       "      <td>-0.711016</td>\n",
       "      <td>-1.160266</td>\n",
       "      <td>-1.096286</td>\n",
       "      <td>-1.159978</td>\n",
       "      <td>-0.896150</td>\n",
       "      <td>-1.159665</td>\n",
       "      <td>-1.260971</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083975</td>\n",
       "      <td>0.003309</td>\n",
       "      <td>8.239631e+06</td>\n",
       "      <td>31303.833333</td>\n",
       "      <td>152.222222</td>\n",
       "      <td>0.148841</td>\n",
       "      <td>0.091238</td>\n",
       "      <td>-0.097222</td>\n",
       "      <td>8.819613e+05</td>\n",
       "      <td>0.097424</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428932 rows × 244 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        log_return1_realized_vol  log_return2_realized_vol  \\\n",
       "0                      -0.003362                  0.354438   \n",
       "1                      -0.201664                 -0.297873   \n",
       "2                      -0.964602                 -0.187208   \n",
       "3                      -0.286467                 -0.499593   \n",
       "4                      -0.523530                 -0.537201   \n",
       "...                          ...                       ...   \n",
       "428927                 -0.231039                  0.096767   \n",
       "428928                 -0.030576                 -0.046142   \n",
       "428929                  0.314005                  0.573072   \n",
       "428930                 -0.104948                  0.078398   \n",
       "428931                 -1.160234                 -0.939174   \n",
       "\n",
       "        log_return1_wmp_realized_vol  log_return2_wmp_realized_vol  wap1_sum  \\\n",
       "0                           0.247420                      0.187837 -0.472177   \n",
       "1                          -0.304567                     -0.530011 -1.341469   \n",
       "2                          -0.949768                     -0.236531 -1.486974   \n",
       "3                          -0.705922                     -0.475682 -2.485702   \n",
       "4                          -1.122709                     -0.692663 -1.630097   \n",
       "...                              ...                           ...       ...   \n",
       "428927                      0.223575                      0.322715 -0.428408   \n",
       "428928                      0.011094                     -0.112112 -1.105433   \n",
       "428929                      0.513883                      0.413298 -0.817845   \n",
       "428930                     -0.381851                     -0.078000  0.064013   \n",
       "428931                     -1.340337                     -0.711016 -1.160266   \n",
       "\n",
       "        wap1_std  wap2_sum  wap2_std  wmp1_sum  wmp1_std  ...  \\\n",
       "0      -0.211403 -0.472481 -0.119330 -0.472139 -0.311462  ...   \n",
       "1      -1.683361 -1.341531 -1.764650 -1.341741 -1.469609  ...   \n",
       "2       0.092040 -1.486544  0.029785 -1.486964 -0.243400  ...   \n",
       "3      -0.097618 -2.485836 -0.361858 -2.485239 -0.380944  ...   \n",
       "4      -1.714946 -1.629972 -1.534265 -1.629997 -1.991143  ...   \n",
       "...          ...       ...       ...       ...       ...  ...   \n",
       "428927 -0.720561 -0.428452 -0.458513 -0.429117 -0.404787  ...   \n",
       "428928  0.581081 -1.105135  0.576590 -1.106006  0.663268  ...   \n",
       "428929 -0.788789 -0.818202 -0.493011 -0.818887 -0.599950  ...   \n",
       "428930 -0.824915  0.063978 -0.749835  0.064233 -0.901647  ...   \n",
       "428931 -1.096286 -1.159978 -0.896150 -1.159665 -1.260971  ...   \n",
       "\n",
       "        cluster_5_size_tau2  cluster_6_log_return1_realized_vol  \\\n",
       "0                  0.060859                            0.004564   \n",
       "1                  0.086877                            0.003627   \n",
       "2                  0.085441                            0.003565   \n",
       "3                  0.093941                            0.003611   \n",
       "4                  0.090875                            0.002842   \n",
       "...                     ...                                 ...   \n",
       "428927             0.067603                            0.003442   \n",
       "428928             0.070870                            0.003170   \n",
       "428929             0.099296                            0.003202   \n",
       "428930             0.055242                            0.004474   \n",
       "428931             0.083975                            0.003309   \n",
       "\n",
       "        cluster_6_total_volume_sum  cluster_6_size_sum  \\\n",
       "0                     3.456186e+06        37392.944444   \n",
       "1                     5.159010e+06        12386.777778   \n",
       "2                     6.443588e+06        15469.722222   \n",
       "3                     3.376000e+06        17120.888889   \n",
       "4                     4.570958e+06        14141.277778   \n",
       "...                            ...                 ...   \n",
       "428927                4.871410e+06        28828.833333   \n",
       "428928                6.858452e+06        18563.111111   \n",
       "428929                6.210604e+06        21978.388889   \n",
       "428930                1.015840e+07        57377.166667   \n",
       "428931                8.239631e+06        31303.833333   \n",
       "\n",
       "        cluster_6_order_count_sum  cluster_6_price_spread_sum  \\\n",
       "0                      311.500000                    0.266546   \n",
       "1                      157.388889                    0.141960   \n",
       "2                      146.000000                    0.172422   \n",
       "3                      146.666667                    0.152984   \n",
       "4                      131.111111                    0.132486   \n",
       "...                           ...                         ...   \n",
       "428927                 244.222222                    0.175172   \n",
       "428928                 229.555556                    0.141583   \n",
       "428929                 182.444444                    0.168502   \n",
       "428930                 337.611111                    0.213039   \n",
       "428931                 152.222222                    0.148841   \n",
       "\n",
       "        cluster_6_bid_spread_sum  cluster_6_ask_spread_sum  \\\n",
       "0                       0.097849                 -0.105899   \n",
       "1                       0.075472                 -0.075217   \n",
       "2                       0.100452                 -0.097067   \n",
       "3                       0.081375                 -0.083333   \n",
       "4                       0.072047                 -0.073787   \n",
       "...                          ...                       ...   \n",
       "428927                  0.090258                 -0.095934   \n",
       "428928                  0.082203                 -0.079893   \n",
       "428929                  0.091938                 -0.094530   \n",
       "428930                  0.119058                 -0.122507   \n",
       "428931                  0.091238                 -0.097222   \n",
       "\n",
       "        cluster_6_volume_imbalance_sum  cluster_6_size_tau2  \n",
       "0                         8.243767e+05             0.070290  \n",
       "1                         1.319986e+06             0.100094  \n",
       "2                         1.649668e+06             0.111269  \n",
       "3                         6.566752e+05             0.098094  \n",
       "4                         1.017866e+06             0.105215  \n",
       "...                                ...                  ...  \n",
       "428927                    7.220615e+05             0.084764  \n",
       "428928                    1.640696e+06             0.080604  \n",
       "428929                    1.979517e+06             0.112833  \n",
       "428930                    3.010704e+06             0.061386  \n",
       "428931                    8.819613e+05             0.097424  \n",
       "\n",
       "[428932 rows x 244 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c7404f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T20:05:05.155129Z",
     "iopub.status.busy": "2025-09-06T20:05:05.154819Z",
     "iopub.status.idle": "2025-09-06T20:05:05.178932Z",
     "shell.execute_reply": "2025-09-06T20:05:05.177880Z"
    },
    "papermill": {
     "duration": 0.053765,
     "end_time": "2025-09-06T20:05:05.180512",
     "exception": false,
     "start_time": "2025-09-06T20:05:05.126747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n",
      "  has_large_values = (abs_vals > 1e6).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n",
      "  has_large_values = (abs_vals > 1e6).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>log_return1_realized_vol</th>\n",
       "      <th>log_return2_realized_vol</th>\n",
       "      <th>log_return1_wmp_realized_vol</th>\n",
       "      <th>log_return2_wmp_realized_vol</th>\n",
       "      <th>wap1_sum</th>\n",
       "      <th>wap1_std</th>\n",
       "      <th>wap2_sum</th>\n",
       "      <th>wap2_std</th>\n",
       "      <th>...</th>\n",
       "      <th>trade_log_return_realized_vol_200_min_time</th>\n",
       "      <th>size_tau</th>\n",
       "      <th>size_tau_400</th>\n",
       "      <th>size_tau_300</th>\n",
       "      <th>size_tau_200</th>\n",
       "      <th>size_tau2</th>\n",
       "      <th>size_tau2_400</th>\n",
       "      <th>size_tau2_300</th>\n",
       "      <th>size_tau2_200</th>\n",
       "      <th>size_tau2_d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0-4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>3.001215</td>\n",
       "      <td>0.00017</td>\n",
       "      <td>3.00165</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.301511</td>\n",
       "      <td>0.173205</td>\n",
       "      <td>0.213201</td>\n",
       "      <td>0.244949</td>\n",
       "      <td>-0.128306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0-32</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0-34</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 181 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  row_id  time_id  log_return1_realized_vol  log_return2_realized_vol  \\\n",
       "0    0-4        4                  0.000294                  0.000252   \n",
       "1   0-32       32                       NaN                       NaN   \n",
       "2   0-34       34                       NaN                       NaN   \n",
       "\n",
       "   log_return1_wmp_realized_vol  log_return2_wmp_realized_vol  wap1_sum  \\\n",
       "0                      0.000245                      0.000027  3.001215   \n",
       "1                           NaN                           NaN       NaN   \n",
       "2                           NaN                           NaN       NaN   \n",
       "\n",
       "   wap1_std  wap2_sum  wap2_std  ...  \\\n",
       "0   0.00017   3.00165  0.000153  ...   \n",
       "1       NaN       NaN       NaN  ...   \n",
       "2       NaN       NaN       NaN  ...   \n",
       "\n",
       "   trade_log_return_realized_vol_200_min_time  size_tau  size_tau_400  \\\n",
       "0                                         NaN   0.57735           NaN   \n",
       "1                                         NaN       NaN           NaN   \n",
       "2                                         NaN       NaN           NaN   \n",
       "\n",
       "   size_tau_300  size_tau_200  size_tau2  size_tau2_400  size_tau2_300  \\\n",
       "0           NaN           NaN   0.301511       0.173205       0.213201   \n",
       "1           NaN           NaN        NaN            NaN            NaN   \n",
       "2           NaN           NaN        NaN            NaN            NaN   \n",
       "\n",
       "   size_tau2_200  size_tau2_d  \n",
       "0       0.244949    -0.128306  \n",
       "1            NaN          NaN  \n",
       "2            NaN          NaN  \n",
       "\n",
       "[3 rows x 181 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab89c2fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T20:05:05.233663Z",
     "iopub.status.busy": "2025-09-06T20:05:05.233324Z",
     "iopub.status.idle": "2025-09-06T20:05:05.255417Z",
     "shell.execute_reply": "2025-09-06T20:05:05.254207Z"
    },
    "papermill": {
     "duration": 0.050285,
     "end_time": "2025-09-06T20:05:05.256986",
     "exception": false,
     "start_time": "2025-09-06T20:05:05.206701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n",
      "  has_large_values = (abs_vals > 1e6).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n",
      "  has_large_values = (abs_vals > 1e6).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_return1_realized_vol</th>\n",
       "      <th>log_return2_realized_vol</th>\n",
       "      <th>log_return1_wmp_realized_vol</th>\n",
       "      <th>log_return2_wmp_realized_vol</th>\n",
       "      <th>wap1_sum</th>\n",
       "      <th>wap1_std</th>\n",
       "      <th>wap2_sum</th>\n",
       "      <th>wap2_std</th>\n",
       "      <th>wmp1_sum</th>\n",
       "      <th>wmp1_std</th>\n",
       "      <th>...</th>\n",
       "      <th>cluster_5_size_tau2</th>\n",
       "      <th>cluster_6_log_return1_realized_vol</th>\n",
       "      <th>cluster_6_total_volume_sum</th>\n",
       "      <th>cluster_6_size_sum</th>\n",
       "      <th>cluster_6_order_count_sum</th>\n",
       "      <th>cluster_6_price_spread_sum</th>\n",
       "      <th>cluster_6_bid_spread_sum</th>\n",
       "      <th>cluster_6_ask_spread_sum</th>\n",
       "      <th>cluster_6_volume_imbalance_sum</th>\n",
       "      <th>cluster_6_size_tau2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.968637</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-2.38769</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-2.682197</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-2.6684</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 243 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   log_return1_realized_vol  log_return2_realized_vol  \\\n",
       "0                 -3.968637                 -5.199338   \n",
       "1                       NaN                       NaN   \n",
       "2                       NaN                       NaN   \n",
       "\n",
       "   log_return1_wmp_realized_vol  log_return2_wmp_realized_vol  wap1_sum  \\\n",
       "0                     -5.199338                     -5.199338 -5.199338   \n",
       "1                           NaN                           NaN       NaN   \n",
       "2                           NaN                           NaN       NaN   \n",
       "\n",
       "   wap1_std  wap2_sum  wap2_std  wmp1_sum  wmp1_std  ...  cluster_5_size_tau2  \\\n",
       "0  -2.38769 -5.199338 -2.682197 -5.199338   -2.6684  ...                  NaN   \n",
       "1       NaN       NaN       NaN       NaN       NaN  ...                  NaN   \n",
       "2       NaN       NaN       NaN       NaN       NaN  ...                  NaN   \n",
       "\n",
       "   cluster_6_log_return1_realized_vol  cluster_6_total_volume_sum  \\\n",
       "0                                 NaN                         NaN   \n",
       "1                                 NaN                         NaN   \n",
       "2                                 NaN                         NaN   \n",
       "\n",
       "   cluster_6_size_sum  cluster_6_order_count_sum  cluster_6_price_spread_sum  \\\n",
       "0                 NaN                        NaN                         NaN   \n",
       "1                 NaN                        NaN                         NaN   \n",
       "2                 NaN                        NaN                         NaN   \n",
       "\n",
       "   cluster_6_bid_spread_sum  cluster_6_ask_spread_sum  \\\n",
       "0                       NaN                       NaN   \n",
       "1                       NaN                       NaN   \n",
       "2                       NaN                       NaN   \n",
       "\n",
       "   cluster_6_volume_imbalance_sum  cluster_6_size_tau2  \n",
       "0                             NaN                  NaN  \n",
       "1                             NaN                  NaN  \n",
       "2                             NaN                  NaN  \n",
       "\n",
       "[3 rows x 243 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0431153b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T20:05:05.311955Z",
     "iopub.status.busy": "2025-09-06T20:05:05.311613Z",
     "iopub.status.idle": "2025-09-06T20:31:12.422188Z",
     "shell.execute_reply": "2025-09-06T20:31:12.420568Z"
    },
    "papermill": {
     "duration": 1567.140971,
     "end_time": "2025-09-06T20:31:12.424192",
     "exception": false,
     "start_time": "2025-09-06T20:05:05.283221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13/3046981331.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_nn[target_name] = 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "2025-09-06 20:05:08.680232: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 25ms/step - loss: 30.4658 - val_loss: 2.2712 - learning_rate: 0.0060\n",
      "Epoch 2/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 2.1111 - val_loss: 1.9404 - learning_rate: 0.0060\n",
      "Epoch 3/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 1.2343 - val_loss: 1.4763 - learning_rate: 0.0060\n",
      "Epoch 4/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 1.1445 - val_loss: 1.3474 - learning_rate: 0.0060\n",
      "Epoch 5/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 1.1359 - val_loss: 1.4693 - learning_rate: 0.0060\n",
      "Epoch 6/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - loss: 1.1484 - val_loss: 1.3756 - learning_rate: 0.0060\n",
      "Epoch 7/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 1.1585 - val_loss: 1.5864 - learning_rate: 0.0060\n",
      "Epoch 8/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 1.1600 - val_loss: 1.3591 - learning_rate: 0.0060\n",
      "Epoch 9/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 1.2165 - val_loss: 1.3160 - learning_rate: 0.0060\n",
      "Epoch 10/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 1.2129 - val_loss: 1.4490 - learning_rate: 0.0060\n",
      "Epoch 11/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 1.1190 - val_loss: 1.4326 - learning_rate: 0.0060\n",
      "Epoch 12/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 1.1876 - val_loss: 1.3451 - learning_rate: 0.0060\n",
      "Epoch 13/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 1.1607 - val_loss: 1.1725 - learning_rate: 0.0060\n",
      "Epoch 14/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 1.9797 - val_loss: 1.5034 - learning_rate: 0.0060\n",
      "Epoch 15/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 1.5862 - val_loss: 3.3208 - learning_rate: 0.0060\n",
      "Epoch 16/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 2.5414 - val_loss: 3.7993 - learning_rate: 0.0060\n",
      "Epoch 17/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 2.0880 - val_loss: 1.6733 - learning_rate: 0.0060\n",
      "Epoch 18/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 1.8420 - val_loss: 1.5269 - learning_rate: 0.0060\n",
      "Epoch 19/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 1.1470 - val_loss: 1.3258 - learning_rate: 0.0060\n",
      "Epoch 20/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 1.6932 - val_loss: 1.4644 - learning_rate: 0.0060\n",
      "Epoch 21/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.4398 - val_loss: 0.2368 - learning_rate: 0.0012\n",
      "Epoch 22/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2462 - val_loss: 0.2333 - learning_rate: 0.0012\n",
      "Epoch 23/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2345 - val_loss: 0.2258 - learning_rate: 0.0012\n",
      "Epoch 24/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2317 - val_loss: 0.2243 - learning_rate: 0.0012\n",
      "Epoch 25/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2321 - val_loss: 0.2357 - learning_rate: 0.0012\n",
      "Epoch 26/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2348 - val_loss: 0.2220 - learning_rate: 0.0012\n",
      "Epoch 27/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2405 - val_loss: 0.2793 - learning_rate: 0.0012\n",
      "Epoch 28/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2386 - val_loss: 0.2984 - learning_rate: 0.0012\n",
      "Epoch 29/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2476 - val_loss: 0.2386 - learning_rate: 0.0012\n",
      "Epoch 30/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 0.2435 - val_loss: 0.2197 - learning_rate: 0.0012\n",
      "Epoch 31/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.2437 - val_loss: 0.2237 - learning_rate: 0.0012\n",
      "Epoch 32/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2462 - val_loss: 0.2216 - learning_rate: 0.0012\n",
      "Epoch 33/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2431 - val_loss: 0.2187 - learning_rate: 0.0012\n",
      "Epoch 34/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2483 - val_loss: 0.2291 - learning_rate: 0.0012\n",
      "Epoch 35/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.3019 - val_loss: 0.2198 - learning_rate: 0.0012\n",
      "Epoch 36/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2451 - val_loss: 0.2190 - learning_rate: 0.0012\n",
      "Epoch 37/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2508 - val_loss: 0.2356 - learning_rate: 0.0012\n",
      "Epoch 38/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.3511 - val_loss: 0.4092 - learning_rate: 0.0012\n",
      "Epoch 39/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.3967 - val_loss: 0.4023 - learning_rate: 0.0012\n",
      "Epoch 40/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.3027 - val_loss: 0.2941 - learning_rate: 0.0012\n",
      "Epoch 41/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2224 - val_loss: 0.2186 - learning_rate: 2.4000e-04\n",
      "Epoch 42/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2137 - val_loss: 0.2183 - learning_rate: 2.4000e-04\n",
      "Epoch 43/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2135 - val_loss: 0.2167 - learning_rate: 2.4000e-04\n",
      "Epoch 44/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2130 - val_loss: 0.2160 - learning_rate: 2.4000e-04\n",
      "Epoch 45/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2127 - val_loss: 0.2159 - learning_rate: 2.4000e-04\n",
      "Epoch 46/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2126 - val_loss: 0.2176 - learning_rate: 2.4000e-04\n",
      "Epoch 47/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2125 - val_loss: 0.2224 - learning_rate: 2.4000e-04\n",
      "Epoch 48/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2126 - val_loss: 0.2192 - learning_rate: 2.4000e-04\n",
      "Epoch 49/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2122 - val_loss: 0.2223 - learning_rate: 2.4000e-04\n",
      "Epoch 50/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2126 - val_loss: 0.2223 - learning_rate: 2.4000e-04\n",
      "Epoch 51/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2133 - val_loss: 0.2170 - learning_rate: 2.4000e-04\n",
      "Epoch 52/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2131 - val_loss: 0.2162 - learning_rate: 2.4000e-04\n",
      "Epoch 53/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2103 - val_loss: 0.2153 - learning_rate: 4.8000e-05\n",
      "Epoch 54/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2102 - val_loss: 0.2153 - learning_rate: 4.8000e-05\n",
      "Epoch 55/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2101 - val_loss: 0.2152 - learning_rate: 4.8000e-05\n",
      "Epoch 56/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2101 - val_loss: 0.2154 - learning_rate: 4.8000e-05\n",
      "Epoch 57/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2101 - val_loss: 0.2156 - learning_rate: 4.8000e-05\n",
      "Epoch 58/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2101 - val_loss: 0.2158 - learning_rate: 4.8000e-05\n",
      "Epoch 59/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2101 - val_loss: 0.2166 - learning_rate: 4.8000e-05\n",
      "Epoch 60/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2101 - val_loss: 0.2173 - learning_rate: 4.8000e-05\n",
      "Epoch 61/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2102 - val_loss: 0.2172 - learning_rate: 4.8000e-05\n",
      "Epoch 62/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.2101 - val_loss: 0.2168 - learning_rate: 4.8000e-05\n",
      "Epoch 63/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.2097 - val_loss: 0.2148 - learning_rate: 9.6000e-06\n",
      "Epoch 64/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2095 - val_loss: 0.2148 - learning_rate: 9.6000e-06\n",
      "Epoch 65/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2095 - val_loss: 0.2148 - learning_rate: 9.6000e-06\n",
      "Epoch 66/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2094 - val_loss: 0.2148 - learning_rate: 9.6000e-06\n",
      "Epoch 67/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2094 - val_loss: 0.2148 - learning_rate: 9.6000e-06\n",
      "Epoch 68/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2095 - val_loss: 0.2149 - learning_rate: 9.6000e-06\n",
      "Epoch 69/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2095 - val_loss: 0.2150 - learning_rate: 9.6000e-06\n",
      "Epoch 70/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2095 - val_loss: 0.2150 - learning_rate: 9.6000e-06\n",
      "Epoch 71/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2093 - val_loss: 0.2145 - learning_rate: 1.9200e-06\n",
      "Epoch 72/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2092 - val_loss: 0.2145 - learning_rate: 1.9200e-06\n",
      "Epoch 73/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2092 - val_loss: 0.2145 - learning_rate: 1.9200e-06\n",
      "Epoch 74/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2092 - val_loss: 0.2145 - learning_rate: 1.9200e-06\n",
      "Epoch 75/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2092 - val_loss: 0.2145 - learning_rate: 1.9200e-06\n",
      "Epoch 76/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2092 - val_loss: 0.2146 - learning_rate: 1.9200e-06\n",
      "Epoch 77/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2092 - val_loss: 0.2146 - learning_rate: 1.9200e-06\n",
      "Epoch 78/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2092 - val_loss: 0.2146 - learning_rate: 1.9200e-06\n",
      "Epoch 79/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2092 - val_loss: 0.2146 - learning_rate: 3.8400e-07\n",
      "Epoch 80/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2092 - val_loss: 0.2146 - learning_rate: 3.8400e-07\n",
      "Epoch 81/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2092 - val_loss: 0.2146 - learning_rate: 3.8400e-07\n",
      "Epoch 82/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2092 - val_loss: 0.2146 - learning_rate: 3.8400e-07\n",
      "Epoch 83/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2092 - val_loss: 0.2146 - learning_rate: 3.8400e-07\n",
      "Epoch 84/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.2092 - val_loss: 0.2146 - learning_rate: 3.8400e-07\n",
      "Epoch 85/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2092 - val_loss: 0.2146 - learning_rate: 3.8400e-07\n",
      "Epoch 86/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2092 - val_loss: 0.2145 - learning_rate: 7.6800e-08\n",
      "Epoch 87/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2092 - val_loss: 0.2145 - learning_rate: 7.6800e-08\n",
      "Epoch 88/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2092 - val_loss: 0.2145 - learning_rate: 7.6800e-08\n",
      "Epoch 89/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2092 - val_loss: 0.2145 - learning_rate: 7.6800e-08\n",
      "Epoch 90/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.2092 - val_loss: 0.2145 - learning_rate: 7.6800e-08\n",
      "Epoch 91/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2092 - val_loss: 0.2145 - learning_rate: 7.6800e-08\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step\n",
      "Fold 1 NN: 0.21453\n",
      "[0.00216913 0.00229866 0.00564153]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "[0.00179851 0.00298297 0.00298297]\n",
      "CV 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - loss: 42.3679 - val_loss: 5.5565 - learning_rate: 0.0060\n",
      "Epoch 2/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 3.9396 - val_loss: 2.1993 - learning_rate: 0.0060\n",
      "Epoch 3/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 2.1759 - val_loss: 3.0735 - learning_rate: 0.0060\n",
      "Epoch 4/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 2.5514 - val_loss: 2.0783 - learning_rate: 0.0060\n",
      "Epoch 5/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 1.9069 - val_loss: 2.6381 - learning_rate: 0.0060\n",
      "Epoch 6/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 1.7907 - val_loss: 1.9936 - learning_rate: 0.0060\n",
      "Epoch 7/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 1.5822 - val_loss: 1.7161 - learning_rate: 0.0060\n",
      "Epoch 8/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 1.3918 - val_loss: 2.0555 - learning_rate: 0.0060\n",
      "Epoch 9/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 1.2826 - val_loss: 1.4358 - learning_rate: 0.0060\n",
      "Epoch 10/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 1.0637 - val_loss: 1.3443 - learning_rate: 0.0060\n",
      "Epoch 11/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.9827 - val_loss: 1.1941 - learning_rate: 0.0060\n",
      "Epoch 12/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.8942 - val_loss: 1.1048 - learning_rate: 0.0060\n",
      "Epoch 13/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.8369 - val_loss: 1.0994 - learning_rate: 0.0060\n",
      "Epoch 14/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.8119 - val_loss: 0.9651 - learning_rate: 0.0060\n",
      "Epoch 15/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.7665 - val_loss: 0.9316 - learning_rate: 0.0060\n",
      "Epoch 16/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.7015 - val_loss: 0.8798 - learning_rate: 0.0060\n",
      "Epoch 17/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.6716 - val_loss: 0.8337 - learning_rate: 0.0060\n",
      "Epoch 18/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.6637 - val_loss: 0.8782 - learning_rate: 0.0060\n",
      "Epoch 19/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.5834 - val_loss: 0.8354 - learning_rate: 0.0060\n",
      "Epoch 20/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.5682 - val_loss: 0.8122 - learning_rate: 0.0060\n",
      "Epoch 21/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.5926 - val_loss: 0.7572 - learning_rate: 0.0060\n",
      "Epoch 22/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.4180 - val_loss: 0.2275 - learning_rate: 0.0060\n",
      "Epoch 23/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2254 - val_loss: 0.2319 - learning_rate: 0.0060\n",
      "Epoch 24/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.2258 - val_loss: 0.2259 - learning_rate: 0.0060\n",
      "Epoch 25/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2261 - val_loss: 0.2315 - learning_rate: 0.0060\n",
      "Epoch 26/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2255 - val_loss: 0.2257 - learning_rate: 0.0060\n",
      "Epoch 27/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2291 - val_loss: 0.2347 - learning_rate: 0.0060\n",
      "Epoch 28/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2340 - val_loss: 0.2489 - learning_rate: 0.0060\n",
      "Epoch 29/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2382 - val_loss: 0.2235 - learning_rate: 0.0060\n",
      "Epoch 30/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.2400 - val_loss: 0.2734 - learning_rate: 0.0060\n",
      "Epoch 31/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2420 - val_loss: 0.2351 - learning_rate: 0.0060\n",
      "Epoch 32/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2434 - val_loss: 0.2502 - learning_rate: 0.0060\n",
      "Epoch 33/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2431 - val_loss: 0.2645 - learning_rate: 0.0060\n",
      "Epoch 34/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2439 - val_loss: 0.2652 - learning_rate: 0.0060\n",
      "Epoch 35/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2458 - val_loss: 0.2233 - learning_rate: 0.0060\n",
      "Epoch 36/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2399 - val_loss: 0.2229 - learning_rate: 0.0060\n",
      "Epoch 37/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2372 - val_loss: 0.2330 - learning_rate: 0.0060\n",
      "Epoch 38/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2383 - val_loss: 0.2385 - learning_rate: 0.0060\n",
      "Epoch 39/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.2370 - val_loss: 0.2424 - learning_rate: 0.0060\n",
      "Epoch 40/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2358 - val_loss: 0.2231 - learning_rate: 0.0060\n",
      "Epoch 41/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.2305 - val_loss: 0.2648 - learning_rate: 0.0060\n",
      "Epoch 42/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2417 - val_loss: 0.2274 - learning_rate: 0.0060\n",
      "Epoch 43/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2323 - val_loss: 0.2261 - learning_rate: 0.0060\n",
      "Epoch 44/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2104 - val_loss: 0.2203 - learning_rate: 0.0012\n",
      "Epoch 45/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2094 - val_loss: 0.2179 - learning_rate: 0.0012\n",
      "Epoch 46/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2083 - val_loss: 0.2175 - learning_rate: 0.0012\n",
      "Epoch 47/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2081 - val_loss: 0.2168 - learning_rate: 0.0012\n",
      "Epoch 48/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2079 - val_loss: 0.2165 - learning_rate: 0.0012\n",
      "Epoch 49/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2080 - val_loss: 0.2164 - learning_rate: 0.0012\n",
      "Epoch 50/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2079 - val_loss: 0.2163 - learning_rate: 0.0012\n",
      "Epoch 51/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2076 - val_loss: 0.2159 - learning_rate: 0.0012\n",
      "Epoch 52/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2072 - val_loss: 0.2164 - learning_rate: 0.0012\n",
      "Epoch 53/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2073 - val_loss: 0.2162 - learning_rate: 0.0012\n",
      "Epoch 54/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.2077 - val_loss: 0.2166 - learning_rate: 0.0012\n",
      "Epoch 55/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2078 - val_loss: 0.2157 - learning_rate: 0.0012\n",
      "Epoch 56/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2076 - val_loss: 0.2165 - learning_rate: 0.0012\n",
      "Epoch 57/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2083 - val_loss: 0.2169 - learning_rate: 0.0012\n",
      "Epoch 58/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2083 - val_loss: 0.2222 - learning_rate: 0.0012\n",
      "Epoch 59/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2087 - val_loss: 0.2216 - learning_rate: 0.0012\n",
      "Epoch 60/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2083 - val_loss: 0.2220 - learning_rate: 0.0012\n",
      "Epoch 61/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2079 - val_loss: 0.2226 - learning_rate: 0.0012\n",
      "Epoch 62/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2077 - val_loss: 0.2237 - learning_rate: 0.0012\n",
      "Epoch 63/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2051 - val_loss: 0.2150 - learning_rate: 2.4000e-04\n",
      "Epoch 64/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.2042 - val_loss: 0.2158 - learning_rate: 2.4000e-04\n",
      "Epoch 65/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2041 - val_loss: 0.2160 - learning_rate: 2.4000e-04\n",
      "Epoch 66/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.2040 - val_loss: 0.2156 - learning_rate: 2.4000e-04\n",
      "Epoch 67/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2039 - val_loss: 0.2155 - learning_rate: 2.4000e-04\n",
      "Epoch 68/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2040 - val_loss: 0.2156 - learning_rate: 2.4000e-04\n",
      "Epoch 69/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2040 - val_loss: 0.2157 - learning_rate: 2.4000e-04\n",
      "Epoch 70/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2039 - val_loss: 0.2158 - learning_rate: 2.4000e-04\n",
      "Epoch 71/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2033 - val_loss: 0.2145 - learning_rate: 4.8000e-05\n",
      "Epoch 72/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2029 - val_loss: 0.2144 - learning_rate: 4.8000e-05\n",
      "Epoch 73/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2029 - val_loss: 0.2144 - learning_rate: 4.8000e-05\n",
      "Epoch 74/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2028 - val_loss: 0.2144 - learning_rate: 4.8000e-05\n",
      "Epoch 75/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.2028 - val_loss: 0.2145 - learning_rate: 4.8000e-05\n",
      "Epoch 76/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2028 - val_loss: 0.2145 - learning_rate: 4.8000e-05\n",
      "Epoch 77/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2028 - val_loss: 0.2145 - learning_rate: 4.8000e-05\n",
      "Epoch 78/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2028 - val_loss: 0.2145 - learning_rate: 4.8000e-05\n",
      "Epoch 79/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2026 - val_loss: 0.2146 - learning_rate: 9.6000e-06\n",
      "Epoch 80/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2025 - val_loss: 0.2146 - learning_rate: 9.6000e-06\n",
      "Epoch 81/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2025 - val_loss: 0.2147 - learning_rate: 9.6000e-06\n",
      "Epoch 82/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2025 - val_loss: 0.2147 - learning_rate: 9.6000e-06\n",
      "Epoch 83/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2025 - val_loss: 0.2147 - learning_rate: 9.6000e-06\n",
      "Epoch 84/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2025 - val_loss: 0.2147 - learning_rate: 9.6000e-06\n",
      "Epoch 85/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2025 - val_loss: 0.2147 - learning_rate: 9.6000e-06\n",
      "Epoch 86/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.2025 - val_loss: 0.2148 - learning_rate: 1.9200e-06\n",
      "Epoch 87/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2024 - val_loss: 0.2148 - learning_rate: 1.9200e-06\n",
      "Epoch 88/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2024 - val_loss: 0.2148 - learning_rate: 1.9200e-06\n",
      "Epoch 89/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2024 - val_loss: 0.2148 - learning_rate: 1.9200e-06\n",
      "Epoch 90/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2024 - val_loss: 0.2148 - learning_rate: 1.9200e-06\n",
      "Epoch 91/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2024 - val_loss: 0.2148 - learning_rate: 1.9200e-06\n",
      "Epoch 92/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2024 - val_loss: 0.2148 - learning_rate: 1.9200e-06\n",
      "Epoch 93/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2024 - val_loss: 0.2147 - learning_rate: 3.8400e-07\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step\n",
      "Fold 2 NN: 0.21443\n",
      "[0.00138753 0.00241914 0.00202408]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "[0.00618555 0.00820823 0.00820823]\n",
      "CV 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - loss: 44.3677 - val_loss: 6.1219 - learning_rate: 0.0060\n",
      "Epoch 2/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 4.9485 - val_loss: 2.8735 - learning_rate: 0.0060\n",
      "Epoch 3/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 2.8591 - val_loss: 4.4978 - learning_rate: 0.0060\n",
      "Epoch 4/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 3.1190 - val_loss: 2.2499 - learning_rate: 0.0060\n",
      "Epoch 5/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 3.3467 - val_loss: 1.1054 - learning_rate: 0.0060\n",
      "Epoch 6/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 1.3169 - val_loss: 1.2619 - learning_rate: 0.0060\n",
      "Epoch 7/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 1.3977 - val_loss: 2.7598 - learning_rate: 0.0060\n",
      "Epoch 8/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 1.4285 - val_loss: 0.6907 - learning_rate: 0.0060\n",
      "Epoch 9/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 1.4393 - val_loss: 0.7125 - learning_rate: 0.0060\n",
      "Epoch 10/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.5552 - val_loss: 0.6183 - learning_rate: 0.0060\n",
      "Epoch 11/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 1.0517 - val_loss: 1.4054 - learning_rate: 0.0060\n",
      "Epoch 12/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 1.0956 - val_loss: 2.0510 - learning_rate: 0.0060\n",
      "Epoch 13/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 1.7595 - val_loss: 2.1658 - learning_rate: 0.0060\n",
      "Epoch 14/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 1.6995 - val_loss: 2.0255 - learning_rate: 0.0060\n",
      "Epoch 15/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 1.6601 - val_loss: 1.9681 - learning_rate: 0.0060\n",
      "Epoch 16/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 1.5782 - val_loss: 2.0459 - learning_rate: 0.0060\n",
      "Epoch 17/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 1.5513 - val_loss: 1.3833 - learning_rate: 0.0060\n",
      "Epoch 18/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.3978 - val_loss: 0.2318 - learning_rate: 0.0012\n",
      "Epoch 19/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2257 - val_loss: 0.2331 - learning_rate: 0.0012\n",
      "Epoch 20/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.2257 - val_loss: 0.2438 - learning_rate: 0.0012\n",
      "Epoch 21/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2254 - val_loss: 0.2604 - learning_rate: 0.0012\n",
      "Epoch 22/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2283 - val_loss: 0.2350 - learning_rate: 0.0012\n",
      "Epoch 23/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2219 - val_loss: 0.2261 - learning_rate: 0.0012\n",
      "Epoch 24/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2214 - val_loss: 0.2306 - learning_rate: 0.0012\n",
      "Epoch 25/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2238 - val_loss: 0.2262 - learning_rate: 0.0012\n",
      "Epoch 26/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2264 - val_loss: 0.2265 - learning_rate: 0.0012\n",
      "Epoch 27/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2210 - val_loss: 0.2248 - learning_rate: 0.0012\n",
      "Epoch 28/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2210 - val_loss: 0.2378 - learning_rate: 0.0012\n",
      "Epoch 29/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2234 - val_loss: 0.2493 - learning_rate: 0.0012\n",
      "Epoch 30/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2248 - val_loss: 0.2721 - learning_rate: 0.0012\n",
      "Epoch 31/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2332 - val_loss: 0.2266 - learning_rate: 0.0012\n",
      "Epoch 32/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.2269 - val_loss: 0.2628 - learning_rate: 0.0012\n",
      "Epoch 33/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2318 - val_loss: 0.2209 - learning_rate: 0.0012\n",
      "Epoch 34/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2247 - val_loss: 0.2245 - learning_rate: 0.0012\n",
      "Epoch 35/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2253 - val_loss: 0.2348 - learning_rate: 0.0012\n",
      "Epoch 36/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2269 - val_loss: 0.2267 - learning_rate: 0.0012\n",
      "Epoch 37/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2207 - val_loss: 0.2358 - learning_rate: 0.0012\n",
      "Epoch 38/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2316 - val_loss: 0.2423 - learning_rate: 0.0012\n",
      "Epoch 39/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2288 - val_loss: 0.2340 - learning_rate: 0.0012\n",
      "Epoch 40/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2258 - val_loss: 0.2370 - learning_rate: 0.0012\n",
      "Epoch 41/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2121 - val_loss: 0.2191 - learning_rate: 2.4000e-04\n",
      "Epoch 42/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2091 - val_loss: 0.2195 - learning_rate: 2.4000e-04\n",
      "Epoch 43/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.2090 - val_loss: 0.2204 - learning_rate: 2.4000e-04\n",
      "Epoch 44/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2091 - val_loss: 0.2193 - learning_rate: 2.4000e-04\n",
      "Epoch 45/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2089 - val_loss: 0.2187 - learning_rate: 2.4000e-04\n",
      "Epoch 46/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2087 - val_loss: 0.2191 - learning_rate: 2.4000e-04\n",
      "Epoch 47/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2087 - val_loss: 0.2185 - learning_rate: 2.4000e-04\n",
      "Epoch 48/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2086 - val_loss: 0.2191 - learning_rate: 2.4000e-04\n",
      "Epoch 49/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2088 - val_loss: 0.2180 - learning_rate: 2.4000e-04\n",
      "Epoch 50/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2086 - val_loss: 0.2192 - learning_rate: 2.4000e-04\n",
      "Epoch 51/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2090 - val_loss: 0.2218 - learning_rate: 2.4000e-04\n",
      "Epoch 52/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2093 - val_loss: 0.2227 - learning_rate: 2.4000e-04\n",
      "Epoch 53/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2096 - val_loss: 0.2240 - learning_rate: 2.4000e-04\n",
      "Epoch 54/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.2099 - val_loss: 0.2237 - learning_rate: 2.4000e-04\n",
      "Epoch 55/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2099 - val_loss: 0.2229 - learning_rate: 2.4000e-04\n",
      "Epoch 56/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2098 - val_loss: 0.2220 - learning_rate: 2.4000e-04\n",
      "Epoch 57/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.2068 - val_loss: 0.2183 - learning_rate: 4.8000e-05\n",
      "Epoch 58/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2064 - val_loss: 0.2179 - learning_rate: 4.8000e-05\n",
      "Epoch 59/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2062 - val_loss: 0.2180 - learning_rate: 4.8000e-05\n",
      "Epoch 60/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2061 - val_loss: 0.2182 - learning_rate: 4.8000e-05\n",
      "Epoch 61/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2060 - val_loss: 0.2185 - learning_rate: 4.8000e-05\n",
      "Epoch 62/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2060 - val_loss: 0.2187 - learning_rate: 4.8000e-05\n",
      "Epoch 63/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2060 - val_loss: 0.2184 - learning_rate: 4.8000e-05\n",
      "Epoch 64/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2057 - val_loss: 0.2172 - learning_rate: 9.6000e-06\n",
      "Epoch 65/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2056 - val_loss: 0.2172 - learning_rate: 9.6000e-06\n",
      "Epoch 66/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.2056 - val_loss: 0.2171 - learning_rate: 9.6000e-06\n",
      "Epoch 67/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2055 - val_loss: 0.2171 - learning_rate: 9.6000e-06\n",
      "Epoch 68/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2055 - val_loss: 0.2171 - learning_rate: 9.6000e-06\n",
      "Epoch 69/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2055 - val_loss: 0.2170 - learning_rate: 9.6000e-06\n",
      "Epoch 70/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2055 - val_loss: 0.2170 - learning_rate: 9.6000e-06\n",
      "Epoch 71/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.2055 - val_loss: 0.2170 - learning_rate: 9.6000e-06\n",
      "Epoch 72/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2055 - val_loss: 0.2170 - learning_rate: 9.6000e-06\n",
      "Epoch 73/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2055 - val_loss: 0.2171 - learning_rate: 9.6000e-06\n",
      "Epoch 74/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.2055 - val_loss: 0.2171 - learning_rate: 9.6000e-06\n",
      "Epoch 75/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2055 - val_loss: 0.2171 - learning_rate: 9.6000e-06\n",
      "Epoch 76/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2054 - val_loss: 0.2171 - learning_rate: 1.9200e-06\n",
      "Epoch 77/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.2053 - val_loss: 0.2171 - learning_rate: 1.9200e-06\n",
      "Epoch 78/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2053 - val_loss: 0.2171 - learning_rate: 1.9200e-06\n",
      "Epoch 79/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2053 - val_loss: 0.2170 - learning_rate: 1.9200e-06\n",
      "Epoch 80/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2053 - val_loss: 0.2170 - learning_rate: 1.9200e-06\n",
      "Epoch 81/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2053 - val_loss: 0.2170 - learning_rate: 1.9200e-06\n",
      "Epoch 82/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2053 - val_loss: 0.2170 - learning_rate: 1.9200e-06\n",
      "Epoch 83/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2053 - val_loss: 0.2170 - learning_rate: 3.8400e-07\n",
      "Epoch 84/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2053 - val_loss: 0.2170 - learning_rate: 3.8400e-07\n",
      "Epoch 85/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2053 - val_loss: 0.2170 - learning_rate: 3.8400e-07\n",
      "Epoch 86/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2053 - val_loss: 0.2170 - learning_rate: 3.8400e-07\n",
      "Epoch 87/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2053 - val_loss: 0.2170 - learning_rate: 3.8400e-07\n",
      "Epoch 88/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.2053 - val_loss: 0.2170 - learning_rate: 3.8400e-07\n",
      "Epoch 89/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2052 - val_loss: 0.2170 - learning_rate: 3.8400e-07\n",
      "Epoch 90/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2052 - val_loss: 0.2170 - learning_rate: 7.6800e-08\n",
      "Epoch 91/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2052 - val_loss: 0.2170 - learning_rate: 7.6800e-08\n",
      "Epoch 92/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2052 - val_loss: 0.2170 - learning_rate: 7.6800e-08\n",
      "Epoch 93/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2052 - val_loss: 0.2170 - learning_rate: 7.6800e-08\n",
      "Epoch 94/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2052 - val_loss: 0.2170 - learning_rate: 7.6800e-08\n",
      "Epoch 95/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2052 - val_loss: 0.2170 - learning_rate: 7.6800e-08\n",
      "Epoch 96/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 0.2052 - val_loss: 0.2170 - learning_rate: 7.6800e-08\n",
      "Epoch 97/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2052 - val_loss: 0.2170 - learning_rate: 7.6800e-08\n",
      "Epoch 98/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2052 - val_loss: 0.2170 - learning_rate: 1.5360e-08\n",
      "Epoch 99/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.2052 - val_loss: 0.2170 - learning_rate: 1.5360e-08\n",
      "Epoch 100/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2052 - val_loss: 0.2170 - learning_rate: 1.5360e-08\n",
      "Epoch 101/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2052 - val_loss: 0.2170 - learning_rate: 1.5360e-08\n",
      "Epoch 102/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2052 - val_loss: 0.2170 - learning_rate: 1.5360e-08\n",
      "Epoch 103/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2052 - val_loss: 0.2170 - learning_rate: 1.5360e-08\n",
      "Epoch 104/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2052 - val_loss: 0.2170 - learning_rate: 1.5360e-08\n",
      "Epoch 105/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2052 - val_loss: 0.2170 - learning_rate: 3.0720e-09\n",
      "Epoch 106/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2052 - val_loss: 0.2170 - learning_rate: 3.0720e-09\n",
      "Epoch 107/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2052 - val_loss: 0.2170 - learning_rate: 3.0720e-09\n",
      "Epoch 108/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2052 - val_loss: 0.2170 - learning_rate: 3.0720e-09\n",
      "Epoch 109/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2052 - val_loss: 0.2170 - learning_rate: 3.0720e-09\n",
      "Epoch 110/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.2052 - val_loss: 0.2170 - learning_rate: 3.0720e-09\n",
      "Epoch 111/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2052 - val_loss: 0.2170 - learning_rate: 3.0720e-09\n",
      "Epoch 112/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.2052 - val_loss: 0.2170 - learning_rate: 6.1440e-10\n",
      "Epoch 113/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2052 - val_loss: 0.2170 - learning_rate: 6.1440e-10\n",
      "Epoch 114/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2052 - val_loss: 0.2170 - learning_rate: 6.1440e-10\n",
      "Epoch 115/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2052 - val_loss: 0.2170 - learning_rate: 6.1440e-10\n",
      "Epoch 116/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2052 - val_loss: 0.2170 - learning_rate: 6.1440e-10\n",
      "Epoch 117/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.2052 - val_loss: 0.2170 - learning_rate: 6.1440e-10\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step\n",
      "Fold 3 NN: 0.21696\n",
      "[0.00377631 0.00761372 0.00199929]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "[0.00922979 0.01346354 0.01346354]\n",
      "CV 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - loss: 48.4889 - val_loss: 0.9369 - learning_rate: 0.0060\n",
      "Epoch 2/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 2.9034 - val_loss: 1.4596 - learning_rate: 0.0060\n",
      "Epoch 3/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 1.7363 - val_loss: 0.9700 - learning_rate: 0.0060\n",
      "Epoch 4/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.6936 - val_loss: 0.9909 - learning_rate: 0.0060\n",
      "Epoch 5/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 1.0201 - val_loss: 0.3872 - learning_rate: 0.0060\n",
      "Epoch 6/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.7706 - val_loss: 0.6713 - learning_rate: 0.0060\n",
      "Epoch 7/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.7510 - val_loss: 0.9042 - learning_rate: 0.0060\n",
      "Epoch 8/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.7232 - val_loss: 0.7094 - learning_rate: 0.0060\n",
      "Epoch 9/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.6782 - val_loss: 0.6550 - learning_rate: 0.0060\n",
      "Epoch 10/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.6469 - val_loss: 0.6030 - learning_rate: 0.0060\n",
      "Epoch 11/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.6227 - val_loss: 0.5133 - learning_rate: 0.0060\n",
      "Epoch 12/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.5901 - val_loss: 0.4665 - learning_rate: 0.0060\n",
      "Epoch 13/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2546 - val_loss: 0.2309 - learning_rate: 0.0012\n",
      "Epoch 14/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2196 - val_loss: 0.2261 - learning_rate: 0.0012\n",
      "Epoch 15/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2180 - val_loss: 0.2256 - learning_rate: 0.0012\n",
      "Epoch 16/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.2172 - val_loss: 0.2238 - learning_rate: 0.0012\n",
      "Epoch 17/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2162 - val_loss: 0.2221 - learning_rate: 0.0012\n",
      "Epoch 18/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2157 - val_loss: 0.2236 - learning_rate: 0.0012\n",
      "Epoch 19/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2164 - val_loss: 0.2227 - learning_rate: 0.0012\n",
      "Epoch 20/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2164 - val_loss: 0.2225 - learning_rate: 0.0012\n",
      "Epoch 21/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2147 - val_loss: 0.2206 - learning_rate: 0.0012\n",
      "Epoch 22/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.2142 - val_loss: 0.2196 - learning_rate: 0.0012\n",
      "Epoch 23/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2141 - val_loss: 0.2195 - learning_rate: 0.0012\n",
      "Epoch 24/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2137 - val_loss: 0.2197 - learning_rate: 0.0012\n",
      "Epoch 25/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2144 - val_loss: 0.2198 - learning_rate: 0.0012\n",
      "Epoch 26/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2167 - val_loss: 0.2274 - learning_rate: 0.0012\n",
      "Epoch 27/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2193 - val_loss: 0.2269 - learning_rate: 0.0012\n",
      "Epoch 28/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2172 - val_loss: 0.2289 - learning_rate: 0.0012\n",
      "Epoch 29/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2233 - val_loss: 0.2291 - learning_rate: 0.0012\n",
      "Epoch 30/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2119 - val_loss: 0.2177 - learning_rate: 2.4000e-04\n",
      "Epoch 31/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2092 - val_loss: 0.2176 - learning_rate: 2.4000e-04\n",
      "Epoch 32/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2090 - val_loss: 0.2182 - learning_rate: 2.4000e-04\n",
      "Epoch 33/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2087 - val_loss: 0.2180 - learning_rate: 2.4000e-04\n",
      "Epoch 34/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.2085 - val_loss: 0.2177 - learning_rate: 2.4000e-04\n",
      "Epoch 35/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.2083 - val_loss: 0.2175 - learning_rate: 2.4000e-04\n",
      "Epoch 36/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2083 - val_loss: 0.2179 - learning_rate: 2.4000e-04\n",
      "Epoch 37/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2083 - val_loss: 0.2179 - learning_rate: 2.4000e-04\n",
      "Epoch 38/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2082 - val_loss: 0.2178 - learning_rate: 2.4000e-04\n",
      "Epoch 39/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2082 - val_loss: 0.2173 - learning_rate: 2.4000e-04\n",
      "Epoch 40/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2082 - val_loss: 0.2184 - learning_rate: 2.4000e-04\n",
      "Epoch 41/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2086 - val_loss: 0.2182 - learning_rate: 2.4000e-04\n",
      "Epoch 42/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2086 - val_loss: 0.2179 - learning_rate: 2.4000e-04\n",
      "Epoch 43/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2086 - val_loss: 0.2181 - learning_rate: 2.4000e-04\n",
      "Epoch 44/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2088 - val_loss: 0.2181 - learning_rate: 2.4000e-04\n",
      "Epoch 45/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.2088 - val_loss: 0.2180 - learning_rate: 2.4000e-04\n",
      "Epoch 46/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 0.2087 - val_loss: 0.2178 - learning_rate: 2.4000e-04\n",
      "Epoch 47/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2067 - val_loss: 0.2157 - learning_rate: 4.8000e-05\n",
      "Epoch 48/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2061 - val_loss: 0.2157 - learning_rate: 4.8000e-05\n",
      "Epoch 49/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2061 - val_loss: 0.2157 - learning_rate: 4.8000e-05\n",
      "Epoch 50/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2060 - val_loss: 0.2159 - learning_rate: 4.8000e-05\n",
      "Epoch 51/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2060 - val_loss: 0.2161 - learning_rate: 4.8000e-05\n",
      "Epoch 52/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2060 - val_loss: 0.2162 - learning_rate: 4.8000e-05\n",
      "Epoch 53/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2060 - val_loss: 0.2161 - learning_rate: 4.8000e-05\n",
      "Epoch 54/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2060 - val_loss: 0.2161 - learning_rate: 4.8000e-05\n",
      "Epoch 55/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2055 - val_loss: 0.2153 - learning_rate: 9.6000e-06\n",
      "Epoch 56/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.2054 - val_loss: 0.2153 - learning_rate: 9.6000e-06\n",
      "Epoch 57/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2054 - val_loss: 0.2153 - learning_rate: 9.6000e-06\n",
      "Epoch 58/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2054 - val_loss: 0.2152 - learning_rate: 9.6000e-06\n",
      "Epoch 59/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2054 - val_loss: 0.2152 - learning_rate: 9.6000e-06\n",
      "Epoch 60/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2054 - val_loss: 0.2152 - learning_rate: 9.6000e-06\n",
      "Epoch 61/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 0.2054 - val_loss: 0.2152 - learning_rate: 9.6000e-06\n",
      "Epoch 62/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2053 - val_loss: 0.2152 - learning_rate: 9.6000e-06\n",
      "Epoch 63/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.2052 - val_loss: 0.2151 - learning_rate: 1.9200e-06\n",
      "Epoch 64/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2052 - val_loss: 0.2151 - learning_rate: 1.9200e-06\n",
      "Epoch 65/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.2052 - val_loss: 0.2151 - learning_rate: 1.9200e-06\n",
      "Epoch 66/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.2052 - val_loss: 0.2151 - learning_rate: 1.9200e-06\n",
      "Epoch 67/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.2052 - val_loss: 0.2151 - learning_rate: 1.9200e-06\n",
      "Epoch 68/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2052 - val_loss: 0.2151 - learning_rate: 1.9200e-06\n",
      "Epoch 69/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2052 - val_loss: 0.2151 - learning_rate: 1.9200e-06\n",
      "Epoch 70/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2052 - val_loss: 0.2151 - learning_rate: 1.9200e-06\n",
      "Epoch 71/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2051 - val_loss: 0.2151 - learning_rate: 3.8400e-07\n",
      "Epoch 72/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2051 - val_loss: 0.2151 - learning_rate: 3.8400e-07\n",
      "Epoch 73/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2051 - val_loss: 0.2151 - learning_rate: 3.8400e-07\n",
      "Epoch 74/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2051 - val_loss: 0.2151 - learning_rate: 3.8400e-07\n",
      "Epoch 75/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2051 - val_loss: 0.2151 - learning_rate: 3.8400e-07\n",
      "Epoch 76/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2051 - val_loss: 0.2151 - learning_rate: 3.8400e-07\n",
      "Epoch 77/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2051 - val_loss: 0.2151 - learning_rate: 3.8400e-07\n",
      "Epoch 78/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.2051 - val_loss: 0.2151 - learning_rate: 7.6800e-08\n",
      "Epoch 79/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.2051 - val_loss: 0.2152 - learning_rate: 7.6800e-08\n",
      "Epoch 80/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2051 - val_loss: 0.2152 - learning_rate: 7.6800e-08\n",
      "Epoch 81/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2051 - val_loss: 0.2152 - learning_rate: 7.6800e-08\n",
      "Epoch 82/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2051 - val_loss: 0.2152 - learning_rate: 7.6800e-08\n",
      "Epoch 83/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2051 - val_loss: 0.2152 - learning_rate: 7.6800e-08\n",
      "Epoch 84/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2051 - val_loss: 0.2152 - learning_rate: 7.6800e-08\n",
      "Epoch 85/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2051 - val_loss: 0.2152 - learning_rate: 1.5360e-08\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step\n",
      "Fold 4 NN: 0.21511\n",
      "[0.00164155 0.00520507 0.00373397]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "[0.0106996  0.01561073 0.01561073]\n",
      "CV 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - loss: 39.0775 - val_loss: 1.7095 - learning_rate: 0.0060\n",
      "Epoch 2/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 2.0659 - val_loss: 1.1788 - learning_rate: 0.0060\n",
      "Epoch 3/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 1.1232 - val_loss: 0.7200 - learning_rate: 0.0060\n",
      "Epoch 4/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.6412 - val_loss: 0.5133 - learning_rate: 0.0060\n",
      "Epoch 5/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.5818 - val_loss: 0.5736 - learning_rate: 0.0060\n",
      "Epoch 6/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.5708 - val_loss: 0.5022 - learning_rate: 0.0060\n",
      "Epoch 7/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.5600 - val_loss: 0.5093 - learning_rate: 0.0060\n",
      "Epoch 8/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.5625 - val_loss: 0.5933 - learning_rate: 0.0060\n",
      "Epoch 9/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.5536 - val_loss: 0.4754 - learning_rate: 0.0060\n",
      "Epoch 10/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.5554 - val_loss: 0.4817 - learning_rate: 0.0060\n",
      "Epoch 11/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.5653 - val_loss: 0.5499 - learning_rate: 0.0060\n",
      "Epoch 12/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.5612 - val_loss: 0.4455 - learning_rate: 0.0060\n",
      "Epoch 13/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.4797 - val_loss: 0.4272 - learning_rate: 0.0060\n",
      "Epoch 14/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.4714 - val_loss: 0.5130 - learning_rate: 0.0060\n",
      "Epoch 15/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.4683 - val_loss: 0.4446 - learning_rate: 0.0060\n",
      "Epoch 16/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.4870 - val_loss: 0.5068 - learning_rate: 0.0060\n",
      "Epoch 17/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.5540 - val_loss: 0.2253 - learning_rate: 0.0060\n",
      "Epoch 18/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2738 - val_loss: 0.5379 - learning_rate: 0.0060\n",
      "Epoch 19/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.7582 - val_loss: 0.3637 - learning_rate: 0.0060\n",
      "Epoch 20/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.3953 - val_loss: 0.3495 - learning_rate: 0.0060\n",
      "Epoch 21/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.3816 - val_loss: 0.3385 - learning_rate: 0.0060\n",
      "Epoch 22/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.3758 - val_loss: 0.3389 - learning_rate: 0.0060\n",
      "Epoch 23/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.3560 - val_loss: 0.2279 - learning_rate: 0.0060\n",
      "Epoch 24/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.2371 - val_loss: 0.2213 - learning_rate: 0.0060\n",
      "Epoch 25/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2655 - val_loss: 2.5521 - learning_rate: 0.0060\n",
      "Epoch 26/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 1.0264 - val_loss: 0.2252 - learning_rate: 0.0060\n",
      "Epoch 27/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2421 - val_loss: 0.2994 - learning_rate: 0.0060\n",
      "Epoch 28/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2777 - val_loss: 0.2983 - learning_rate: 0.0060\n",
      "Epoch 29/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.3215 - val_loss: 0.2213 - learning_rate: 0.0060\n",
      "Epoch 30/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2284 - val_loss: 0.2234 - learning_rate: 0.0060\n",
      "Epoch 31/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2290 - val_loss: 0.2182 - learning_rate: 0.0060\n",
      "Epoch 32/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2271 - val_loss: 0.2182 - learning_rate: 0.0060\n",
      "Epoch 33/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.2267 - val_loss: 0.2277 - learning_rate: 0.0060\n",
      "Epoch 34/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.2327 - val_loss: 0.2194 - learning_rate: 0.0060\n",
      "Epoch 35/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.2402 - val_loss: 0.2170 - learning_rate: 0.0060\n",
      "Epoch 36/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.2359 - val_loss: 0.2166 - learning_rate: 0.0060\n",
      "Epoch 37/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2299 - val_loss: 0.2239 - learning_rate: 0.0060\n",
      "Epoch 38/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2383 - val_loss: 0.2349 - learning_rate: 0.0060\n",
      "Epoch 39/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2499 - val_loss: 0.2250 - learning_rate: 0.0060\n",
      "Epoch 40/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2311 - val_loss: 0.2182 - learning_rate: 0.0060\n",
      "Epoch 41/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2313 - val_loss: 0.2171 - learning_rate: 0.0060\n",
      "Epoch 42/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2320 - val_loss: 0.2526 - learning_rate: 0.0060\n",
      "Epoch 43/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2439 - val_loss: 0.2163 - learning_rate: 0.0060\n",
      "Epoch 44/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2376 - val_loss: 0.2161 - learning_rate: 0.0060\n",
      "Epoch 45/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2306 - val_loss: 0.2154 - learning_rate: 0.0060\n",
      "Epoch 46/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.2330 - val_loss: 0.2205 - learning_rate: 0.0060\n",
      "Epoch 47/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.2291 - val_loss: 0.2153 - learning_rate: 0.0060\n",
      "Epoch 48/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2384 - val_loss: 0.2161 - learning_rate: 0.0060\n",
      "Epoch 49/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2458 - val_loss: 0.2340 - learning_rate: 0.0060\n",
      "Epoch 50/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.3428 - val_loss: 0.2260 - learning_rate: 0.0060\n",
      "Epoch 51/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2346 - val_loss: 0.2246 - learning_rate: 0.0060\n",
      "Epoch 52/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2349 - val_loss: 0.2238 - learning_rate: 0.0060\n",
      "Epoch 53/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2303 - val_loss: 0.2181 - learning_rate: 0.0060\n",
      "Epoch 54/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2281 - val_loss: 0.2159 - learning_rate: 0.0060\n",
      "Epoch 55/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2089 - val_loss: 0.2129 - learning_rate: 0.0012\n",
      "Epoch 56/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2079 - val_loss: 0.2127 - learning_rate: 0.0012\n",
      "Epoch 57/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2078 - val_loss: 0.2132 - learning_rate: 0.0012\n",
      "Epoch 58/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2077 - val_loss: 0.2144 - learning_rate: 0.0012\n",
      "Epoch 59/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.2078 - val_loss: 0.2142 - learning_rate: 0.0012\n",
      "Epoch 60/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2078 - val_loss: 0.2130 - learning_rate: 0.0012\n",
      "Epoch 61/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2074 - val_loss: 0.2129 - learning_rate: 0.0012\n",
      "Epoch 62/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2073 - val_loss: 0.2136 - learning_rate: 0.0012\n",
      "Epoch 63/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.2072 - val_loss: 0.2154 - learning_rate: 0.0012\n",
      "Epoch 64/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2063 - val_loss: 0.2122 - learning_rate: 2.4000e-04\n",
      "Epoch 65/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2056 - val_loss: 0.2122 - learning_rate: 2.4000e-04\n",
      "Epoch 66/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2055 - val_loss: 0.2120 - learning_rate: 2.4000e-04\n",
      "Epoch 67/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2055 - val_loss: 0.2119 - learning_rate: 2.4000e-04\n",
      "Epoch 68/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2054 - val_loss: 0.2119 - learning_rate: 2.4000e-04\n",
      "Epoch 69/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2054 - val_loss: 0.2119 - learning_rate: 2.4000e-04\n",
      "Epoch 70/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.2054 - val_loss: 0.2120 - learning_rate: 2.4000e-04\n",
      "Epoch 71/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.2053 - val_loss: 0.2122 - learning_rate: 2.4000e-04\n",
      "Epoch 72/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2053 - val_loss: 0.2120 - learning_rate: 2.4000e-04\n",
      "Epoch 73/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2052 - val_loss: 0.2118 - learning_rate: 2.4000e-04\n",
      "Epoch 74/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2052 - val_loss: 0.2120 - learning_rate: 2.4000e-04\n",
      "Epoch 75/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2049 - val_loss: 0.2120 - learning_rate: 4.8000e-05\n",
      "Epoch 76/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2048 - val_loss: 0.2120 - learning_rate: 4.8000e-05\n",
      "Epoch 77/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2047 - val_loss: 0.2120 - learning_rate: 4.8000e-05\n",
      "Epoch 78/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2047 - val_loss: 0.2120 - learning_rate: 4.8000e-05\n",
      "Epoch 79/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2047 - val_loss: 0.2120 - learning_rate: 4.8000e-05\n",
      "Epoch 80/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2047 - val_loss: 0.2120 - learning_rate: 4.8000e-05\n",
      "Epoch 81/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2047 - val_loss: 0.2120 - learning_rate: 4.8000e-05\n",
      "Epoch 82/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.2046 - val_loss: 0.2118 - learning_rate: 9.6000e-06\n",
      "Epoch 83/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2045 - val_loss: 0.2118 - learning_rate: 9.6000e-06\n",
      "Epoch 84/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2045 - val_loss: 0.2118 - learning_rate: 9.6000e-06\n",
      "Epoch 85/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2045 - val_loss: 0.2118 - learning_rate: 9.6000e-06\n",
      "Epoch 86/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2044 - val_loss: 0.2118 - learning_rate: 9.6000e-06\n",
      "Epoch 87/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2044 - val_loss: 0.2118 - learning_rate: 9.6000e-06\n",
      "Epoch 88/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2044 - val_loss: 0.2118 - learning_rate: 9.6000e-06\n",
      "Epoch 89/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2044 - val_loss: 0.2118 - learning_rate: 9.6000e-06\n",
      "Epoch 90/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2044 - val_loss: 0.2117 - learning_rate: 1.9200e-06\n",
      "Epoch 91/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2044 - val_loss: 0.2117 - learning_rate: 1.9200e-06\n",
      "Epoch 92/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2044 - val_loss: 0.2117 - learning_rate: 1.9200e-06\n",
      "Epoch 93/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - loss: 0.2044 - val_loss: 0.2117 - learning_rate: 1.9200e-06\n",
      "Epoch 94/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2044 - val_loss: 0.2117 - learning_rate: 1.9200e-06\n",
      "Epoch 95/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2044 - val_loss: 0.2117 - learning_rate: 1.9200e-06\n",
      "Epoch 96/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2044 - val_loss: 0.2117 - learning_rate: 1.9200e-06\n",
      "Epoch 97/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2044 - val_loss: 0.2117 - learning_rate: 1.9200e-06\n",
      "Epoch 98/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2044 - val_loss: 0.2117 - learning_rate: 1.9200e-06\n",
      "Epoch 99/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2044 - val_loss: 0.2117 - learning_rate: 1.9200e-06\n",
      "Epoch 100/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2044 - val_loss: 0.2117 - learning_rate: 1.9200e-06\n",
      "Epoch 101/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2044 - val_loss: 0.2117 - learning_rate: 1.9200e-06\n",
      "Epoch 102/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2044 - val_loss: 0.2117 - learning_rate: 1.9200e-06\n",
      "Epoch 103/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2044 - val_loss: 0.2117 - learning_rate: 3.8400e-07\n",
      "Epoch 104/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2044 - val_loss: 0.2117 - learning_rate: 3.8400e-07\n",
      "Epoch 105/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.2043 - val_loss: 0.2117 - learning_rate: 3.8400e-07\n",
      "Epoch 106/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2043 - val_loss: 0.2117 - learning_rate: 3.8400e-07\n",
      "Epoch 107/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2043 - val_loss: 0.2117 - learning_rate: 3.8400e-07\n",
      "Epoch 108/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2043 - val_loss: 0.2117 - learning_rate: 3.8400e-07\n",
      "Epoch 109/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2043 - val_loss: 0.2117 - learning_rate: 3.8400e-07\n",
      "Epoch 110/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.2043 - val_loss: 0.2117 - learning_rate: 7.6800e-08\n",
      "Epoch 111/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.2043 - val_loss: 0.2117 - learning_rate: 7.6800e-08\n",
      "Epoch 112/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2043 - val_loss: 0.2117 - learning_rate: 7.6800e-08\n",
      "Epoch 113/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2043 - val_loss: 0.2117 - learning_rate: 7.6800e-08\n",
      "Epoch 114/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2043 - val_loss: 0.2117 - learning_rate: 7.6800e-08\n",
      "Epoch 115/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2043 - val_loss: 0.2117 - learning_rate: 7.6800e-08\n",
      "Epoch 116/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.2043 - val_loss: 0.2117 - learning_rate: 7.6800e-08\n",
      "Epoch 117/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.2043 - val_loss: 0.2117 - learning_rate: 1.5360e-08\n",
      "Epoch 118/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2043 - val_loss: 0.2117 - learning_rate: 1.5360e-08\n",
      "Epoch 119/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2043 - val_loss: 0.2117 - learning_rate: 1.5360e-08\n",
      "Epoch 120/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2043 - val_loss: 0.2117 - learning_rate: 1.5360e-08\n",
      "Epoch 121/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2043 - val_loss: 0.2117 - learning_rate: 1.5360e-08\n",
      "Epoch 122/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2043 - val_loss: 0.2117 - learning_rate: 1.5360e-08\n",
      "Epoch 123/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2043 - val_loss: 0.2117 - learning_rate: 1.5360e-08\n",
      "Epoch 124/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2043 - val_loss: 0.2117 - learning_rate: 3.0720e-09\n",
      "Epoch 125/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2043 - val_loss: 0.2117 - learning_rate: 3.0720e-09\n",
      "Epoch 126/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.2043 - val_loss: 0.2117 - learning_rate: 3.0720e-09\n",
      "Epoch 127/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.2043 - val_loss: 0.2117 - learning_rate: 3.0720e-09\n",
      "Epoch 128/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.2043 - val_loss: 0.2117 - learning_rate: 3.0720e-09\n",
      "Epoch 129/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.2043 - val_loss: 0.2117 - learning_rate: 3.0720e-09\n",
      "Epoch 130/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2043 - val_loss: 0.2117 - learning_rate: 3.0720e-09\n",
      "Epoch 131/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2043 - val_loss: 0.2117 - learning_rate: 6.1440e-10\n",
      "Epoch 132/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2043 - val_loss: 0.2117 - learning_rate: 6.1440e-10\n",
      "Epoch 133/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2043 - val_loss: 0.2117 - learning_rate: 6.1440e-10\n",
      "Epoch 134/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2043 - val_loss: 0.2117 - learning_rate: 6.1440e-10\n",
      "Epoch 135/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2043 - val_loss: 0.2117 - learning_rate: 6.1440e-10\n",
      "Epoch 136/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.2043 - val_loss: 0.2117 - learning_rate: 6.1440e-10\n",
      "Epoch 137/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.2043 - val_loss: 0.2117 - learning_rate: 6.1440e-10\n",
      "Epoch 138/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.2043 - val_loss: 0.2117 - learning_rate: 1.2288e-10\n",
      "Epoch 139/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.2043 - val_loss: 0.2117 - learning_rate: 1.2288e-10\n",
      "Epoch 140/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.2043 - val_loss: 0.2117 - learning_rate: 1.2288e-10\n",
      "Epoch 141/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2043 - val_loss: 0.2117 - learning_rate: 1.2288e-10\n",
      "Epoch 142/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2043 - val_loss: 0.2117 - learning_rate: 1.2288e-10\n",
      "Epoch 143/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2043 - val_loss: 0.2117 - learning_rate: 1.2288e-10\n",
      "Epoch 144/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.2043 - val_loss: 0.2117 - learning_rate: 1.2288e-10\n",
      "Epoch 145/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2043 - val_loss: 0.2117 - learning_rate: 2.4576e-11\n",
      "Epoch 146/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2043 - val_loss: 0.2117 - learning_rate: 2.4576e-11\n",
      "Epoch 147/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2043 - val_loss: 0.2117 - learning_rate: 2.4576e-11\n",
      "Epoch 148/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2043 - val_loss: 0.2117 - learning_rate: 2.4576e-11\n",
      "Epoch 149/1000\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.2043 - val_loss: 0.2117 - learning_rate: 2.4576e-11\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step\n",
      "Fold 5 NN: 0.21169\n",
      "[0.00212858 0.00244429 0.00632477]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "[0.01377152 0.01803437 0.01803437]\n"
     ]
    }
   ],
   "source": [
    "target_name='target'\n",
    "scores_folds = {}\n",
    "model_name = 'NN'\n",
    "pred_name = 'pred_{}'.format(model_name)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kfolds=5\n",
    "kf = KFold(n_splits=kfolds, shuffle=True, random_state=42)\n",
    "scores_folds[model_name] = []\n",
    "counter = 1\n",
    "colNames_0 = df_train.columns.tolist()\n",
    "colNames_0.remove('row_id')\n",
    "try:\n",
    "    colNames_0.remove('pred_NN')\n",
    "except:\n",
    "    pass\n",
    "#df_train['stock_id'] = df_train['row_id'].str.split(\"-\").str[0]\n",
    "\n",
    "\n",
    "\n",
    "df_test['stock_id']=df_test['row_id'].str.split(\"-\").str[0]\n",
    "df_test['stock_id'] = df_test['stock_id'].astype(int)\n",
    "test_nn[['stock_id','time_id']]=df_test[['stock_id','time_id']]\n",
    "#test_nn = pd.merge(test_nn,test_cluster,how='left',on='time_id')\n",
    "test_nn[colNames] = test_nn[colNames].fillna(train_nn[colNames].mean())\n",
    "\n",
    "\n",
    "train_nn = train_nn.fillna(0)\n",
    "test_nn = test_nn.fillna(0)\n",
    "test_predictions_nn = np.zeros(test_nn.shape[0])\n",
    "\n",
    "test_nn[target_name] = 0\n",
    "train_nn[pred_name] = 0\n",
    "\n",
    "for n_count in range(kfolds):\n",
    "    print('CV {}/{}'.format(counter, kfolds))\n",
    "    \n",
    "    indexes = np.arange(nfolds).astype(int)    \n",
    "    indexes = np.delete(indexes,obj=n_count, axis=0) \n",
    "    \n",
    "    indexes = np.r_[ind_values[indexes[0]],ind_values[indexes[1]],ind_values[indexes[2]],ind_values[indexes[3]]]\n",
    "    #train_nn[['stock_id','time_id','target']]=df_train[['stock_id','time_id','target']]\n",
    "    \n",
    "    X_train = train_nn.loc[train_nn.time_id.isin(indexes), colNames_0]\n",
    "    df_train_1 = df_train.loc[df_train.time_id.isin(indexes),colNames_0]\n",
    "    train_cluster = generate_cluster_features(df_train_1, cluster_stocks).reset_index()\n",
    "    train_cluster = train_cluster.fillna(0)\n",
    "\n",
    "    X_train = pd.merge(X_train,train_cluster,how='left',on='time_id')\n",
    "    \n",
    "\n",
    "    X_test = train_nn.loc[train_nn.time_id.isin(ind_values[n_count]), colNames_0]\n",
    "    df_train_0 = df_train.loc[df_train.time_id.isin(ind_values[n_count]),colNames_0]\n",
    "    test_cluster = generate_cluster_features(df_train_0, cluster_stocks).reset_index()\n",
    "    test_cluster = test_cluster.fillna(0)\n",
    "    X_test = pd.merge(X_test,test_cluster,how='left',on='time_id')\n",
    "    #print(2,train_cluster)\n",
    "    #print(colNames_0)\n",
    "    y_train = train_nn.loc[train_nn.time_id.isin(indexes), target_name]\n",
    "    y_test = train_nn.loc[train_nn.time_id.isin(ind_values[n_count]), target_name]\n",
    "    \n",
    "    #print(train_cluster)\n",
    "    #print(generate_cluster_features(df_train, cluster_stocks).reset_index().head())\n",
    "    #print(test_nn.head())\n",
    "    \n",
    "    #############################################################################################\n",
    "    # NN\n",
    "    #############################################################################################\n",
    "    \n",
    "    model = base_model()\n",
    "    \n",
    "    model.compile(\n",
    "        keras.optimizers.Adam(learning_rate=0.006),\n",
    "        loss=rmse_keras\n",
    "    )\n",
    "    colNames_new = train_nn.columns.tolist()\n",
    " \n",
    "    try:\n",
    "        colNames_new.remove('row_id')\n",
    "    except:\n",
    "        pass\n",
    "    colNames_new.remove('target')\n",
    "    colNames_new.remove('time_id')\n",
    "    '''try:\n",
    "        colNames_new.remove('stock_id')\n",
    "    except:\n",
    "        pass'''\n",
    "    try:\n",
    "        colNames_new.remove('pred_NN')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    X_train = X_train.loc[:, colNames_new]\n",
    "    X_test = X_test.loc[:, colNames_new]\n",
    "    \n",
    "    num_data = X_train[colNames_new]\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))         \n",
    "    num_data = scaler.fit_transform(num_data.values)    \n",
    "    \n",
    "    cat_data = X_train['stock_id']    \n",
    "    target =  y_train\n",
    "    \n",
    "    num_data_test = X_test[colNames_new]\n",
    "    num_data_test = scaler.transform(num_data_test.values)\n",
    "    cat_data_test = X_test['stock_id']\n",
    "\n",
    "    tt =scaler.transform(test_nn[colNames_new].values)\n",
    "\n",
    "    model.fit([cat_data, num_data], \n",
    "              target,               \n",
    "              batch_size=2048,\n",
    "              epochs=1000,\n",
    "              validation_data=([cat_data_test, num_data_test], y_test),\n",
    "              callbacks=[es, plateau],\n",
    "              validation_batch_size=len(y_test),\n",
    "              shuffle=True,\n",
    "             verbose = 1)\n",
    "\n",
    "    preds = model.predict([cat_data_test, num_data_test]).reshape(1,-1)[0]\n",
    "    \n",
    "    score = round(rmspe(y_true = y_test, y_pred = preds),5)\n",
    "    print('Fold {} {}: {}'.format(counter, model_name, score))\n",
    "    scores_folds[model_name].append(score)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #test_nn[target_name] += model.predict([test_nn['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)\n",
    "    print(preds[:3])\n",
    "    test_predictions_nn += model.predict([test_nn['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)/kfolds\n",
    "    print(test_predictions_nn[:3])\n",
    "    #test[target_name] += model.predict([test['stock_id'], test[features_to_consider]]).reshape(1,-1)[0].clip(0,1e10)\n",
    "       \n",
    "    counter += 1\n",
    "    colNames_new.append('stock_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee4b682",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T20:31:15.207926Z",
     "iopub.status.busy": "2025-09-06T20:31:15.207524Z",
     "iopub.status.idle": "2025-09-06T20:31:15.250678Z",
     "shell.execute_reply": "2025-09-06T20:31:15.249552Z"
    },
    "papermill": {
     "duration": 1.44711,
     "end_time": "2025-09-06T20:31:15.252526",
     "exception": false,
     "start_time": "2025-09-06T20:31:13.805416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSPE NN: 1.0 - Folds: [0.21453, 0.21443, 0.21696, 0.21511, 0.21169]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13/3919126777.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_nn[\"row_id\"] = test_nn[\"stock_id\"].astype(str) + \"-\" + test_nn[\"time_id\"].astype(str)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_return1_realized_vol</th>\n",
       "      <th>log_return2_realized_vol</th>\n",
       "      <th>log_return1_wmp_realized_vol</th>\n",
       "      <th>log_return2_wmp_realized_vol</th>\n",
       "      <th>wap1_sum</th>\n",
       "      <th>wap1_std</th>\n",
       "      <th>wap2_sum</th>\n",
       "      <th>wap2_std</th>\n",
       "      <th>wmp1_sum</th>\n",
       "      <th>wmp1_std</th>\n",
       "      <th>...</th>\n",
       "      <th>cluster_6_total_volume_sum</th>\n",
       "      <th>cluster_6_size_sum</th>\n",
       "      <th>cluster_6_order_count_sum</th>\n",
       "      <th>cluster_6_price_spread_sum</th>\n",
       "      <th>cluster_6_bid_spread_sum</th>\n",
       "      <th>cluster_6_ask_spread_sum</th>\n",
       "      <th>cluster_6_volume_imbalance_sum</th>\n",
       "      <th>cluster_6_size_tau2</th>\n",
       "      <th>target</th>\n",
       "      <th>row_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.968637</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-2.387690</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-2.682197</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-2.668400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008072</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001254</td>\n",
       "      <td>0.001453</td>\n",
       "      <td>0.001412</td>\n",
       "      <td>0.004062</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>0.004728</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>0.003865</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>0.004748</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010231</td>\n",
       "      <td>0-32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001254</td>\n",
       "      <td>0.001453</td>\n",
       "      <td>0.001412</td>\n",
       "      <td>0.004062</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>0.004728</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>0.003865</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>0.004748</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010231</td>\n",
       "      <td>0-34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 245 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   log_return1_realized_vol  log_return2_realized_vol  \\\n",
       "0                 -3.968637                 -5.199338   \n",
       "1                  0.001254                  0.001453   \n",
       "2                  0.001254                  0.001453   \n",
       "\n",
       "   log_return1_wmp_realized_vol  log_return2_wmp_realized_vol  wap1_sum  \\\n",
       "0                     -5.199338                     -5.199338 -5.199338   \n",
       "1                      0.001412                      0.004062  0.000398   \n",
       "2                      0.001412                      0.004062  0.000398   \n",
       "\n",
       "   wap1_std  wap2_sum  wap2_std  wmp1_sum  wmp1_std  ...  \\\n",
       "0 -2.387690 -5.199338 -2.682197 -5.199338 -2.668400  ...   \n",
       "1  0.004728  0.000390  0.003865  0.000405  0.004748  ...   \n",
       "2  0.004728  0.000390  0.003865  0.000405  0.004748  ...   \n",
       "\n",
       "   cluster_6_total_volume_sum  cluster_6_size_sum  cluster_6_order_count_sum  \\\n",
       "0                         0.0                 0.0                        0.0   \n",
       "1                         0.0                 0.0                        0.0   \n",
       "2                         0.0                 0.0                        0.0   \n",
       "\n",
       "   cluster_6_price_spread_sum  cluster_6_bid_spread_sum  \\\n",
       "0                         0.0                       0.0   \n",
       "1                         0.0                       0.0   \n",
       "2                         0.0                       0.0   \n",
       "\n",
       "   cluster_6_ask_spread_sum  cluster_6_volume_imbalance_sum  \\\n",
       "0                       0.0                             0.0   \n",
       "1                       0.0                             0.0   \n",
       "2                       0.0                             0.0   \n",
       "\n",
       "   cluster_6_size_tau2    target  row_id  \n",
       "0                  0.0  0.008072     0-4  \n",
       "1                  0.0  0.010231    0-32  \n",
       "2                  0.0  0.010231    0-34  \n",
       "\n",
       "[3 rows x 245 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_nn[\"row_id\"] = test_nn[\"stock_id\"].astype(str) + \"-\" + test_nn[\"time_id\"].astype(str) \n",
    "test_nn[target_name] = (test_predictions_nn+test_pred)/2\n",
    "\n",
    "score = round(rmspe(y_true = train_nn[target_name].values, y_pred = train_nn[pred_name].values),5)\n",
    "print('RMSPE {}: {} - Folds: {}'.format(model_name, score, scores_folds[model_name]))\n",
    "\n",
    "display(test_nn)\n",
    "test_nn[['row_id', target_name]].to_csv('submission.csv',index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208dbf27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T20:31:18.031133Z",
     "iopub.status.busy": "2025-09-06T20:31:18.030794Z",
     "iopub.status.idle": "2025-09-06T20:31:18.203929Z",
     "shell.execute_reply": "2025-09-06T20:31:18.202931Z"
    },
    "papermill": {
     "duration": 1.579033,
     "end_time": "2025-09-06T20:31:18.205448",
     "exception": false,
     "start_time": "2025-09-06T20:31:16.626415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "      <th>time_id</th>\n",
       "      <th>log_return1_realized_vol</th>\n",
       "      <th>log_return2_realized_vol</th>\n",
       "      <th>log_return1_wmp_realized_vol</th>\n",
       "      <th>log_return2_wmp_realized_vol</th>\n",
       "      <th>wap1_sum</th>\n",
       "      <th>wap1_std</th>\n",
       "      <th>wap2_sum</th>\n",
       "      <th>...</th>\n",
       "      <th>trade_log_return_realized_vol_200_min_time</th>\n",
       "      <th>size_tau</th>\n",
       "      <th>size_tau_400</th>\n",
       "      <th>size_tau_300</th>\n",
       "      <th>size_tau_200</th>\n",
       "      <th>size_tau2</th>\n",
       "      <th>size_tau2_400</th>\n",
       "      <th>size_tau2_300</th>\n",
       "      <th>size_tau2_200</th>\n",
       "      <th>size_tau2_d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0-5</td>\n",
       "      <td>0.004136</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004499</td>\n",
       "      <td>0.006999</td>\n",
       "      <td>0.005466</td>\n",
       "      <td>0.006119</td>\n",
       "      <td>303.125061</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>303.105539</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>0.158114</td>\n",
       "      <td>0.192450</td>\n",
       "      <td>0.218218</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.095346</td>\n",
       "      <td>0.054772</td>\n",
       "      <td>0.067420</td>\n",
       "      <td>0.077460</td>\n",
       "      <td>-0.040574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0-11</td>\n",
       "      <td>0.001445</td>\n",
       "      <td>11</td>\n",
       "      <td>0.003966</td>\n",
       "      <td>0.004628</td>\n",
       "      <td>0.003873</td>\n",
       "      <td>0.003877</td>\n",
       "      <td>200.047768</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>200.041171</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.182574</td>\n",
       "      <td>0.213201</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.301511</td>\n",
       "      <td>0.132453</td>\n",
       "      <td>0.076089</td>\n",
       "      <td>0.093659</td>\n",
       "      <td>0.107606</td>\n",
       "      <td>-0.056365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0-16</td>\n",
       "      <td>0.002168</td>\n",
       "      <td>16</td>\n",
       "      <td>0.002451</td>\n",
       "      <td>0.004960</td>\n",
       "      <td>0.002658</td>\n",
       "      <td>0.004693</td>\n",
       "      <td>187.913849</td>\n",
       "      <td>0.000864</td>\n",
       "      <td>187.939824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.235702</td>\n",
       "      <td>0.288675</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.121268</td>\n",
       "      <td>0.069663</td>\n",
       "      <td>0.085749</td>\n",
       "      <td>0.098518</td>\n",
       "      <td>-0.051605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0-31</td>\n",
       "      <td>0.002195</td>\n",
       "      <td>31</td>\n",
       "      <td>0.003742</td>\n",
       "      <td>0.004090</td>\n",
       "      <td>0.003071</td>\n",
       "      <td>0.004029</td>\n",
       "      <td>119.859781</td>\n",
       "      <td>0.000757</td>\n",
       "      <td>119.835941</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258199</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>0.130189</td>\n",
       "      <td>0.074788</td>\n",
       "      <td>0.092057</td>\n",
       "      <td>0.105766</td>\n",
       "      <td>-0.055401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0-62</td>\n",
       "      <td>0.001747</td>\n",
       "      <td>62</td>\n",
       "      <td>0.003210</td>\n",
       "      <td>0.003988</td>\n",
       "      <td>0.002414</td>\n",
       "      <td>0.003480</td>\n",
       "      <td>175.932865</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>175.934256</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.213201</td>\n",
       "      <td>0.267261</td>\n",
       "      <td>0.301511</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.060892</td>\n",
       "      <td>0.074953</td>\n",
       "      <td>0.086115</td>\n",
       "      <td>-0.045108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428927</th>\n",
       "      <td>126-32751</td>\n",
       "      <td>0.003461</td>\n",
       "      <td>32751</td>\n",
       "      <td>0.003885</td>\n",
       "      <td>0.005912</td>\n",
       "      <td>0.005386</td>\n",
       "      <td>0.006712</td>\n",
       "      <td>309.870466</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>309.871372</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.164399</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.235702</td>\n",
       "      <td>0.277350</td>\n",
       "      <td>0.098533</td>\n",
       "      <td>0.056603</td>\n",
       "      <td>0.069673</td>\n",
       "      <td>0.080049</td>\n",
       "      <td>-0.041930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428928</th>\n",
       "      <td>126-32753</td>\n",
       "      <td>0.003113</td>\n",
       "      <td>32753</td>\n",
       "      <td>0.004424</td>\n",
       "      <td>0.005423</td>\n",
       "      <td>0.004695</td>\n",
       "      <td>0.005078</td>\n",
       "      <td>223.552143</td>\n",
       "      <td>0.001264</td>\n",
       "      <td>223.580314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.152499</td>\n",
       "      <td>0.188982</td>\n",
       "      <td>0.223607</td>\n",
       "      <td>0.277350</td>\n",
       "      <td>0.082479</td>\n",
       "      <td>0.047380</td>\n",
       "      <td>0.058321</td>\n",
       "      <td>0.067006</td>\n",
       "      <td>-0.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428929</th>\n",
       "      <td>126-32758</td>\n",
       "      <td>0.004070</td>\n",
       "      <td>32758</td>\n",
       "      <td>0.005529</td>\n",
       "      <td>0.008121</td>\n",
       "      <td>0.006570</td>\n",
       "      <td>0.007160</td>\n",
       "      <td>256.277050</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>256.255056</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.169031</td>\n",
       "      <td>0.192450</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.267261</td>\n",
       "      <td>0.101015</td>\n",
       "      <td>0.058029</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.082065</td>\n",
       "      <td>-0.042986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428930</th>\n",
       "      <td>126-32763</td>\n",
       "      <td>0.003357</td>\n",
       "      <td>32763</td>\n",
       "      <td>0.004216</td>\n",
       "      <td>0.005838</td>\n",
       "      <td>0.003708</td>\n",
       "      <td>0.005197</td>\n",
       "      <td>399.721736</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>399.714332</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>0.111803</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>0.152499</td>\n",
       "      <td>0.192450</td>\n",
       "      <td>0.065372</td>\n",
       "      <td>0.037553</td>\n",
       "      <td>0.046225</td>\n",
       "      <td>0.053109</td>\n",
       "      <td>-0.027819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428931</th>\n",
       "      <td>126-32767</td>\n",
       "      <td>0.002090</td>\n",
       "      <td>32767</td>\n",
       "      <td>0.002184</td>\n",
       "      <td>0.003139</td>\n",
       "      <td>0.002156</td>\n",
       "      <td>0.003438</td>\n",
       "      <td>217.058919</td>\n",
       "      <td>0.000384</td>\n",
       "      <td>217.079726</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.242536</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.096225</td>\n",
       "      <td>0.055277</td>\n",
       "      <td>0.068041</td>\n",
       "      <td>0.078174</td>\n",
       "      <td>-0.040948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428932 rows × 182 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           row_id    target  time_id  log_return1_realized_vol  \\\n",
       "0             0-5  0.004136        5                  0.004499   \n",
       "1            0-11  0.001445       11                  0.003966   \n",
       "2            0-16  0.002168       16                  0.002451   \n",
       "3            0-31  0.002195       31                  0.003742   \n",
       "4            0-62  0.001747       62                  0.003210   \n",
       "...           ...       ...      ...                       ...   \n",
       "428927  126-32751  0.003461    32751                  0.003885   \n",
       "428928  126-32753  0.003113    32753                  0.004424   \n",
       "428929  126-32758  0.004070    32758                  0.005529   \n",
       "428930  126-32763  0.003357    32763                  0.004216   \n",
       "428931  126-32767  0.002090    32767                  0.002184   \n",
       "\n",
       "        log_return2_realized_vol  log_return1_wmp_realized_vol  \\\n",
       "0                       0.006999                      0.005466   \n",
       "1                       0.004628                      0.003873   \n",
       "2                       0.004960                      0.002658   \n",
       "3                       0.004090                      0.003071   \n",
       "4                       0.003988                      0.002414   \n",
       "...                          ...                           ...   \n",
       "428927                  0.005912                      0.005386   \n",
       "428928                  0.005423                      0.004695   \n",
       "428929                  0.008121                      0.006570   \n",
       "428930                  0.005838                      0.003708   \n",
       "428931                  0.003139                      0.002156   \n",
       "\n",
       "        log_return2_wmp_realized_vol    wap1_sum  wap1_std    wap2_sum  ...  \\\n",
       "0                           0.006119  303.125061  0.000693  303.105539  ...   \n",
       "1                           0.003877  200.047768  0.000262  200.041171  ...   \n",
       "2                           0.004693  187.913849  0.000864  187.939824  ...   \n",
       "3                           0.004029  119.859781  0.000757  119.835941  ...   \n",
       "4                           0.003480  175.932865  0.000258  175.934256  ...   \n",
       "...                              ...         ...       ...         ...  ...   \n",
       "428927                      0.006712  309.870466  0.000486  309.871372  ...   \n",
       "428928                      0.005078  223.552143  0.001264  223.580314  ...   \n",
       "428929                      0.007160  256.277050  0.000466  256.255056  ...   \n",
       "428930                      0.005197  399.721736  0.000456  399.714332  ...   \n",
       "428931                      0.003438  217.058919  0.000384  217.079726  ...   \n",
       "\n",
       "        trade_log_return_realized_vol_200_min_time  size_tau  size_tau_400  \\\n",
       "0                                         0.000589  0.158114      0.192450   \n",
       "1                                         0.000000  0.182574      0.213201   \n",
       "2                                         0.000252  0.200000      0.235702   \n",
       "3                                         0.000000  0.258199      0.316228   \n",
       "4                                         0.000242  0.213201      0.267261   \n",
       "...                                            ...       ...           ...   \n",
       "428927                                    0.000437  0.164399      0.204124   \n",
       "428928                                    0.000163  0.152499      0.188982   \n",
       "428929                                    0.000000  0.169031      0.192450   \n",
       "428930                                    0.000576  0.111803      0.136083   \n",
       "428931                                    0.000000  0.166667      0.204124   \n",
       "\n",
       "        size_tau_300  size_tau_200  size_tau2  size_tau2_400  size_tau2_300  \\\n",
       "0           0.218218      0.250000   0.095346       0.054772       0.067420   \n",
       "1           0.250000      0.301511   0.132453       0.076089       0.093659   \n",
       "2           0.288675      0.316228   0.121268       0.069663       0.085749   \n",
       "3           0.333333      0.577350   0.130189       0.074788       0.092057   \n",
       "4           0.301511      0.408248   0.106000       0.060892       0.074953   \n",
       "...              ...           ...        ...            ...            ...   \n",
       "428927      0.235702      0.277350   0.098533       0.056603       0.069673   \n",
       "428928      0.223607      0.277350   0.082479       0.047380       0.058321   \n",
       "428929      0.204124      0.267261   0.101015       0.058029       0.071429   \n",
       "428930      0.152499      0.192450   0.065372       0.037553       0.046225   \n",
       "428931      0.242536      0.316228   0.096225       0.055277       0.068041   \n",
       "\n",
       "        size_tau2_200  size_tau2_d  \n",
       "0            0.077460    -0.040574  \n",
       "1            0.107606    -0.056365  \n",
       "2            0.098518    -0.051605  \n",
       "3            0.105766    -0.055401  \n",
       "4            0.086115    -0.045108  \n",
       "...               ...          ...  \n",
       "428927       0.080049    -0.041930  \n",
       "428928       0.067006    -0.035098  \n",
       "428929       0.082065    -0.042986  \n",
       "428930       0.053109    -0.027819  \n",
       "428931       0.078174    -0.040948  \n",
       "\n",
       "[428932 rows x 182 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f17166",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T20:31:21.018217Z",
     "iopub.status.busy": "2025-09-06T20:31:21.017888Z",
     "iopub.status.idle": "2025-09-06T20:31:21.023928Z",
     "shell.execute_reply": "2025-09-06T20:31:21.023130Z"
    },
    "papermill": {
     "duration": 1.408321,
     "end_time": "2025-09-06T20:31:21.025386",
     "exception": false,
     "start_time": "2025-09-06T20:31:19.617065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.214544"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(list(scores_folds.values())[0])/5"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 2344753,
     "sourceId": 27233,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3189.233975,
   "end_time": "2025-09-06T20:31:25.869635",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-06T19:38:16.635660",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
